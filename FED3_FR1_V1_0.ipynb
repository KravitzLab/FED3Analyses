{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/FED3Analyses/blob/Stats-Table-update/FED3_FR1_V1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEm7saj7IRD2"
      },
      "source": [
        "## FED3 FR1 data analysis\n",
        "<br>\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqe_1a1j1bQaqhOxq0VvukPqfolLRUqOdl-g&s\" width=\"200\" />\n",
        "\n",
        "Updated: 12-16-25\n",
        "<br>\n",
        "Authors: Chantelle Murrell and Sebastian Alves  \n",
        "Version 1.1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k-YGEmW-KCNK"
      },
      "outputs": [],
      "source": [
        "# @title Install libraries and import them {\"run\":\"auto\"}\n",
        "\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Packages to ensure are installed (add others here if you like)\n",
        "packages = {\n",
        "    \"fed3\": \"git+https://github.com/earnestt1234/fed3.git\",\n",
        "    \"fed3bandit\": \"fed3bandit\",\n",
        "    \"pingouin\": \"pingouin\",\n",
        "    \"ipydatagrid\": \"ipydatagrid\",\n",
        "    \"openpyxl\": \"openpyxl\",\n",
        "}\n",
        "\n",
        "for name, source in packages.items():\n",
        "    if importlib.util.find_spec(name) is None:\n",
        "        print(f\"Installing {name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", source])\n",
        "\n",
        "# ----------------------------\n",
        "# Imports\n",
        "# ----------------------------\n",
        "# Standard library\n",
        "import copy\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tempfile\n",
        "import threading\n",
        "import time\n",
        "import warnings\n",
        "import zipfile\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "from os.path import basename, splitext\n",
        "\n",
        "# Third-party\n",
        "from ipydatagrid import DataGrid, TextRenderer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "import fed3\n",
        "import fed3.plot as fplot\n",
        "import fed3bandit as f3b\n",
        "from scipy.stats import f_oneway\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.gridspec as gridspec\n",
        "from google.colab import files\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration\n",
        "# ----------------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.rcParams.update({'font.size': 12, 'figure.autolayout': True})\n",
        "plt.rcParams['figure.figsize'] = [6, 4]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "print(\"Packages installed and imports ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TONj_5IvKdNs",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Upload files\n",
        "\n",
        "\n",
        "# Reset caches to avoid duplicates if you re-run this cell\n",
        "feds, loaded_files, session_types = [], [], []\n",
        "\n",
        "def extract_session_type(csv_path, fallback=\"Unknown\"):\n",
        "    \"\"\"Read 'Session_Type ' or variants; return first non-empty value.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, sep=None, engine='python', dtype=str)\n",
        "        df.columns = [c.strip() for c in df.columns]\n",
        "        lower = {c.casefold(): c for c in df.columns}\n",
        "        for cand in [\"session_type\", \"session type\", \"sessiontype\", \"session\"]:\n",
        "            if cand in lower:\n",
        "                col = lower[cand]\n",
        "                vals = df[col].dropna().astype(str).str.strip()\n",
        "                vals = vals[vals.ne(\"\")]\n",
        "                if not vals.empty:\n",
        "                    return vals.iloc[0]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return fallback\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for name, data in uploaded.items():\n",
        "    if name.lower().endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(io.BytesIO(data)) as zf:\n",
        "            for zi in zf.infolist():\n",
        "                if not zi.filename.lower().endswith(\".csv\"):\n",
        "                    continue\n",
        "                file_data = zf.read(zi)\n",
        "                if len(file_data) <= 1024:\n",
        "                    continue\n",
        "                with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".csv\", delete=False) as tmp:\n",
        "                    tmp.write(file_data); tmp_path = tmp.name\n",
        "                try:\n",
        "                    session_type = extract_session_type(tmp_path)\n",
        "                    df = fed3.load(tmp_path)\n",
        "                    df.name = os.path.basename(zi.filename)\n",
        "                    df.attrs = {\"Session_type\": session_type}\n",
        "                    feds.append(df)\n",
        "                    loaded_files.append(os.path.basename(zi.filename))\n",
        "                    session_types.append(session_type)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {zi.filename}: {e}\")\n",
        "                finally:\n",
        "                    os.remove(tmp_path)\n",
        "    elif name.lower().endswith(\".csv\"):\n",
        "        if len(data) <= 1024:\n",
        "            continue\n",
        "        with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".csv\", delete=False) as tmp:\n",
        "            tmp.write(data); tmp_path = tmp.name\n",
        "        try:\n",
        "            session_type = extract_session_type(tmp_path)\n",
        "            df = fed3.load(tmp_path)\n",
        "            df.name = os.path.basename(name)\n",
        "            df.attrs = {\"Session_type\": session_type}\n",
        "            feds.append(df)\n",
        "            loaded_files.append(os.path.basename(name))\n",
        "            session_types.append(session_type)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {name}: {e}\")\n",
        "        finally:\n",
        "            os.remove(tmp_path)\n",
        "\n",
        "print(f\"Loaded {len(loaded_files)} files. Session types captured for all.\")\n",
        "# Optional quick plot\n",
        "if feds:\n",
        "    try:\n",
        "        fed3.as_aligned(feds, alignment=\"datetime\", inplace=True)\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        fplot.line(feds, y='pellets'); plt.legend().remove(); plt.tight_layout(); plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Plotting skipped: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K311oswozte",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Build Key\n",
        "\n",
        "\n",
        "import os, glob, io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ipydatagrid import DataGrid, TextRenderer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from google.colab import files as colab_files\n",
        "from google.colab import output as colab_output\n",
        "\n",
        "# Require that the file-upload cell has already populated these:\n",
        "assert 'loaded_files' in globals() and 'session_types' in globals(), \\\n",
        "    \"Run the 'Upload FED3 files' cell first.\"\n",
        "\n",
        "colab_output.enable_custom_widget_manager()\n",
        "\n",
        "# ---------------------------\n",
        "# Base: bare-bones Key_Df from loaded data\n",
        "# ---------------------------\n",
        "def _make_base_key_df():\n",
        "    return pd.DataFrame({\"filename\": loaded_files, \"Session_type\": session_types})\n",
        "\n",
        "def _file_base(s):\n",
        "    return os.path.splitext(os.path.basename(str(s)))[0].strip()\n",
        "\n",
        "def _norm_base_lower(s):\n",
        "    return _file_base(s).lower()\n",
        "\n",
        "# ---------------------------\n",
        "# Key scanner: detect Mouse_ID or filename columns\n",
        "# ---------------------------\n",
        "def _scan_key_columns(df):\n",
        "    \"\"\"\n",
        "    Returns dict:\n",
        "      {\n",
        "        'has_mouse': bool,\n",
        "        'has_filename': bool,\n",
        "        'filename_col': 'filename'|'File'|None,\n",
        "        'msg': str\n",
        "      }\n",
        "    Accepts keys that have either Mouse_ID or a filename column (filename/File).\n",
        "    \"\"\"\n",
        "    info = {'has_mouse': False, 'has_filename': False, 'filename_col': None, 'msg': ''}\n",
        "    try:\n",
        "        cols = [str(c).strip() for c in df.columns]\n",
        "        has_mouse = 'Mouse_ID' in cols\n",
        "        fname_col = 'filename' if 'filename' in cols else ('File' if 'File' in cols else None)\n",
        "        info.update({\n",
        "            'has_mouse': has_mouse,\n",
        "            'has_filename': fname_col is not None,\n",
        "            'filename_col': fname_col\n",
        "        })\n",
        "        if has_mouse:\n",
        "            info['msg'] = \"'Mouse_ID' found.\"\n",
        "        elif fname_col:\n",
        "            info['msg'] = f\"'{fname_col}' found; will match on filename.\"\n",
        "        else:\n",
        "            info['msg'] = \"Neither 'Mouse_ID' nor 'filename'/'File' found in provided key.\"\n",
        "    except Exception as e:\n",
        "        info['msg'] = f\"Error while checking key: {e}\"\n",
        "    return info\n",
        "\n",
        "# ---------------------------\n",
        "# Read uploaded key (CSV/XLSX), accept Mouse_ID or filename\n",
        "# ---------------------------\n",
        "def _read_key_from_upload(name, content_bytes):\n",
        "    \"\"\"Return (df_or_None, message). Reads CSV/XLSX bytes from Colab upload.\"\"\"\n",
        "    ext = name.lower().rsplit('.', 1)[-1] if '.' in name else ''\n",
        "    try:\n",
        "        bio = io.BytesIO(content_bytes)\n",
        "        if ext == 'xlsx':\n",
        "            xls = pd.ExcelFile(bio, engine='openpyxl')\n",
        "            frames = [pd.read_excel(xls, sheet_name=s) for s in xls.sheet_names]\n",
        "            key_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        elif ext == 'csv':\n",
        "            key_df = pd.read_csv(bio, sep=None, engine='python')\n",
        "        else:\n",
        "            return None, f\"Unsupported key type .{ext}\"\n",
        "\n",
        "        key_df = key_df.copy()\n",
        "        key_df.columns = [str(c).strip() for c in key_df.columns]\n",
        "        scan = _scan_key_columns(key_df)\n",
        "        if not (scan['has_mouse'] or scan['has_filename']):\n",
        "            return None, scan['msg']\n",
        "\n",
        "        # Normalize types/columns we might use later\n",
        "        if scan['has_mouse']:\n",
        "            globals()['KEY_MATCH_MODE'] = 'mouse_id'\n",
        "            key_df['Mouse_ID'] = key_df['Mouse_ID'].astype(str).str.strip()\n",
        "\n",
        "        if scan['has_filename']:\n",
        "            globals()['KEY_MATCH_MODE'] = 'filename'\n",
        "            fcol = scan['filename_col']\n",
        "            key_df[fcol] = key_df[fcol].astype(str).str.strip()\n",
        "            key_df['_key_file_base_lower'] = key_df[fcol].map(_norm_base_lower)\n",
        "        else:\n",
        "            globals()['KEY_MATCH_MODE'] = None\n",
        "\n",
        "        # Persist a deterministic copy on disk for reproducibility\n",
        "        fixed_path = f\"_uploaded_key.{ext}\"\n",
        "        with open(fixed_path, \"wb\") as f:\n",
        "            f.write(content_bytes)\n",
        "        globals()['uploaded_key_path'] = fixed_path\n",
        "\n",
        "        return key_df, f\"Key loaded from upload ({name}) and saved to {fixed_path}. {scan['msg']}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error reading uploaded key: {e}\"\n",
        "\n",
        "# ---------------------------\n",
        "# Matching filename <-> Mouse_ID\n",
        "# ---------------------------\n",
        "def _match_mouse_id_to_filenames(filenames, key_df):\n",
        "    \"\"\"Return DataFrame: filename, Mouse_ID, match_status based on Mouse_ID substring in filename.\"\"\"\n",
        "    base_names_lower = [_norm_base_lower(f) for f in filenames]\n",
        "    mouse_ids = (\n",
        "        key_df['Mouse_ID']\n",
        "        .dropna().astype(str).map(str.strip)\n",
        "        .replace({'': np.nan}).dropna().unique().tolist()\n",
        "    )\n",
        "    rows = []\n",
        "    for fname, base in zip(filenames, base_names_lower):\n",
        "        hits = [mid for mid in mouse_ids if str(mid).lower() in base]\n",
        "        if len(hits) == 1:\n",
        "            rows.append({\"filename\": fname, \"Mouse_ID\": hits[0], \"match_status\": \"Matched (Mouse_ID in filename)\"})\n",
        "        elif len(hits) > 1:\n",
        "            longest = max(len(str(h)) for h in hits)\n",
        "            best = [h for h in hits if len(str(h)) == longest]\n",
        "            if len(best) == 1:\n",
        "                rows.append({\"filename\": fname, \"Mouse_ID\": best[0], \"match_status\": \"Matched (longest Mouse_ID token)\"})\n",
        "            else:\n",
        "                rows.append({\"filename\": fname, \"Mouse_ID\": None, \"match_status\": f\"Ambiguous Mouse_ID: {hits}\"})\n",
        "        else:\n",
        "            rows.append({\"filename\": fname, \"Mouse_ID\": None, \"match_status\": \"Mouse_ID not found in filename\"})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------------------------\n",
        "# Build/Rematch function\n",
        "# ---------------------------\n",
        "status_box = widgets.Output()\n",
        "Key_Df = _make_base_key_df()  # start bare-bones\n",
        "\n",
        "def _collapse_key_suffixes(df, keep_filename_from_base=True):\n",
        "    \"\"\"\n",
        "    Remove duplicate columns created by merge suffixes.\n",
        "    - If both 'col' and 'col_key' exist, keep the key version (rename col_key -> col, drop base col).\n",
        "    - Special-case: if keep_filename_from_base and base == 'filename',\n",
        "      drop 'filename_key' and keep the base 'filename'.\n",
        "    \"\"\"\n",
        "    cols = list(df.columns)\n",
        "    rename_map = {}\n",
        "    drop_cols = []\n",
        "\n",
        "    for col in cols:\n",
        "        if col.endswith('_key'):\n",
        "            base = col[:-4]\n",
        "            if keep_filename_from_base and base == 'filename':\n",
        "                # keep the 'filename' from the bare-bones (actual loaded_files)\n",
        "                drop_cols.append(col)\n",
        "            elif base in df.columns:\n",
        "                # key version wins: drop base col, rename *_key -> base\n",
        "                drop_cols.append(base)\n",
        "                rename_map[col] = base\n",
        "            else:\n",
        "                # no base column; just strip \"_key\"\n",
        "                rename_map[col] = base\n",
        "\n",
        "    df = df.drop(columns=drop_cols, errors='ignore').rename(columns=rename_map)\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_or_rematch_key_df(key_df=None, msg_hint=\"\"):\n",
        "    \"\"\"\n",
        "    If key_df provided and valid:\n",
        "      - If a filename column exists (filename/File), always use filename-based merge.\n",
        "        - If Mouse_ID is also present, it is preserved from the key file.\n",
        "      - Else, if only Mouse_ID exists, use substring-based Mouse_ID matching.\n",
        "    Else: keep bare-bones.\n",
        "\n",
        "    When a key is used, any duplicate column names between the bare-bones\n",
        "    and the key are resolved so that the key's values win (except 'filename').\n",
        "    \"\"\"\n",
        "    global Key_Df\n",
        "    files_df = _make_base_key_df().copy()\n",
        "    files_df['_file_base_lower'] = files_df['filename'].map(_norm_base_lower)\n",
        "\n",
        "    if key_df is None:\n",
        "        # Pure bare-bones Key_Df\n",
        "        Key_Df = files_df.drop(columns=['_file_base_lower']).copy()\n",
        "        Key_Df[\"match_status\"] = \"No key\"\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Key status: No key provided; showing bare-bones Key_Df.\")\n",
        "        return\n",
        "\n",
        "    # Identify key capabilities\n",
        "    scan = _scan_key_columns(key_df)\n",
        "    kd = key_df.copy()\n",
        "\n",
        "    # Helper: make a unique version of the key for whichever join we use\n",
        "    def _dedup(df, subset_cols):\n",
        "        dup_counts = df[subset_cols].astype(str).agg('|'.join, axis=1).value_counts()\n",
        "        n_dups = int((dup_counts > 1).sum())\n",
        "        if n_dups:\n",
        "            with status_box:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Note: {n_dups} duplicate key(s) on {subset_cols}; taking the first occurrence.\")\n",
        "        return df.drop_duplicates(subset=subset_cols, keep=\"first\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # 1) Prefer filename-based route whenever filename column exists\n",
        "    #    (whether or not Mouse_ID is present)\n",
        "    # ---------------------------\n",
        "    if scan['has_filename']:\n",
        "        fcol = scan['filename_col']\n",
        "\n",
        "        # Normalize filename column in key\n",
        "        kd[fcol] = kd[fcol].astype(str).str.strip()\n",
        "        kd['_key_file_base_lower'] = kd[fcol].map(_norm_base_lower)\n",
        "\n",
        "        # One row per normalized filename in the key\n",
        "        key_unique = _dedup(kd, ['_key_file_base_lower'])\n",
        "\n",
        "        # Merge by normalized basename (case-insensitive, extension-stripped)\n",
        "        Key_Df = (\n",
        "            files_df\n",
        "            .merge(\n",
        "                key_unique,\n",
        "                left_on=\"_file_base_lower\",\n",
        "                right_on=\"_key_file_base_lower\",\n",
        "                how=\"left\",\n",
        "                suffixes=(\"\", \"_key\")\n",
        "            )\n",
        "            .drop(columns=['_file_base_lower', '_key_file_base_lower'])\n",
        "        )\n",
        "\n",
        "        # Clean up duplicate columns so key overwrites bare-bones where appropriate\n",
        "        Key_Df = _collapse_key_suffixes(Key_Df, keep_filename_from_base=True)\n",
        "\n",
        "        # match_status for filename-based matching\n",
        "        Key_Df[\"match_status\"] = np.where(\n",
        "            Key_Df[fcol].notna(),\n",
        "            \"Matched (filename)\",\n",
        "            \"Filename not found in key\"\n",
        "        )\n",
        "\n",
        "    # ---------------------------\n",
        "    # 2) If no filename column but Mouse_ID exists, use substring route\n",
        "    # ---------------------------\n",
        "    elif scan['has_mouse']:\n",
        "        # Mouse_ID route (fallback when no filename column exists)\n",
        "        kd['Mouse_ID'] = kd['Mouse_ID'].astype(str).str.strip()\n",
        "        key_unique = _dedup(kd, ['Mouse_ID'])\n",
        "\n",
        "        matched = _match_mouse_id_to_filenames(\n",
        "            files_df['filename'].tolist(),\n",
        "            key_unique\n",
        "        )[[\"filename\", \"Mouse_ID\", \"match_status\"]]\n",
        "\n",
        "        Key_Df = (\n",
        "            files_df\n",
        "            .merge(matched, on=\"filename\", how=\"left\")\n",
        "            .merge(key_unique, on=\"Mouse_ID\", how=\"left\", suffixes=(\"\", \"_key\"))\n",
        "            .drop(columns=['_file_base_lower'])\n",
        "        )\n",
        "\n",
        "        # Clean up duplicate columns (e.g., Mouse_ID vs Mouse_ID_key)\n",
        "        Key_Df = _collapse_key_suffixes(Key_Df, keep_filename_from_base=True)\n",
        "\n",
        "    # ---------------------------\n",
        "    # 3) Neither filename nor Mouse_ID in key\n",
        "    # ---------------------------\n",
        "    else:\n",
        "        # Neither route available (shouldn't happen due to earlier check)\n",
        "        Key_Df = files_df.drop(columns=['_file_base_lower']).copy()\n",
        "        Key_Df[\"match_status\"] = \"Key missing Mouse_ID and filename columns\"\n",
        "\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        if msg_hint:\n",
        "            print(msg_hint)\n",
        "        print(f\"Merged key columns into Key_Df ({len(Key_Df)} rows, {len(Key_Df.columns)} cols).\")\n",
        "\n",
        "# ---------------------------\n",
        "# Grid UI\n",
        "# ---------------------------\n",
        "def make_grid(df: pd.DataFrame):\n",
        "    g = DataGrid(\n",
        "        df,\n",
        "        editable=True,\n",
        "        selection_mode='cell',\n",
        "        layout={'height': '420px'},\n",
        "        base_row_size=28,\n",
        "        base_column_size=120,\n",
        "    )\n",
        "    g.default_renderer = TextRenderer(text_wrap=True)\n",
        "    return g\n",
        "\n",
        "def rebuild_grid(msg=\"\"):\n",
        "    global grid, ui\n",
        "    df = Key_Df.copy().reset_index(drop=True)\n",
        "    new_grid = make_grid(df)\n",
        "    ui.children = (upload_row, new_grid, controls, status_box)\n",
        "    grid = new_grid\n",
        "    with status_box:\n",
        "        if msg:\n",
        "            print(msg)\n",
        "        print(f\"Grid now shows Key_Df ({len(df)} rows, {len(df.columns)} cols)\")\n",
        "\n",
        "# ---------------------------\n",
        "# Colab-native upload button ONLY (no path UI)\n",
        "# ---------------------------\n",
        "upload_btn = widgets.Button(description=\"Upload\", button_style=\"primary\", layout=widgets.Layout(width=\"120px\"))\n",
        "reset_btn = widgets.Button(description=\"Reset Key\", button_style=\"warning\", layout=widgets.Layout(width=\"120px\"))\n",
        "download_button = widgets.Button(description='Download', button_style='success', layout=widgets.Layout(width=\"120px\"))\n",
        "\n",
        "def on_colab_upload(_):\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Opening Colab upload dialog...\")\n",
        "    uploaded = colab_files.upload()  # opens the native Colab picker\n",
        "    if not uploaded:\n",
        "        with status_box:\n",
        "            print(\"No file selected.\")\n",
        "        return\n",
        "    name, content = next(iter(uploaded.items()))\n",
        "    key_df, msg = _read_key_from_upload(name, content)\n",
        "    if key_df is None:\n",
        "        build_or_rematch_key_df(None)\n",
        "        rebuild_grid(f\"Key status: {msg}\")\n",
        "    else:\n",
        "        build_or_rematch_key_df(key_df, msg_hint=f\"Key status: {msg}\")\n",
        "        rebuild_grid()\n",
        "\n",
        "def _load_saved_key_from_disk():\n",
        "    \"\"\"Returns (key_df_or_None, message) from uploaded_key_path if present/valid.\"\"\"\n",
        "    key_path = globals().get('uploaded_key_path', None)\n",
        "    if not (key_path and os.path.exists(key_path)):\n",
        "        return None, \"No saved key on disk to reload.\"\n",
        "    try:\n",
        "        ext = key_path.lower().rsplit('.', 1)[-1] if '.' in key_path else ''\n",
        "        if ext == 'xlsx':\n",
        "            xls = pd.ExcelFile(key_path, engine='openpyxl')\n",
        "            frames = [pd.read_excel(xls, sheet_name=s) for s in xls.sheet_names]\n",
        "            key_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        elif ext == 'csv':\n",
        "            key_df = pd.read_csv(key_path, sep=None, engine='python')\n",
        "        else:\n",
        "            return None, f\"Unsupported key type .{ext}\"\n",
        "\n",
        "        key_df = key_df.copy()\n",
        "        key_df.columns = [str(c).strip() for c in key_df.columns]\n",
        "        scan = _scan_key_columns(key_df)\n",
        "        if not (scan['has_mouse'] or scan['has_filename']):\n",
        "            return None, scan['msg']\n",
        "\n",
        "        if scan['has_mouse']:\n",
        "            key_df['Mouse_ID'] = key_df['Mouse_ID'].astype(str).str.strip()\n",
        "        if scan['has_filename']:\n",
        "            fcol = scan['filename_col']\n",
        "            key_df[fcol] = key_df[fcol].astype(str).str.strip()\n",
        "            key_df['_key_file_base_lower'] = key_df[fcol].map(_norm_base_lower)\n",
        "\n",
        "        return key_df, f\"Key reloaded from {os.path.basename(key_path)}. {scan['msg']}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error reading saved uploaded key: {e}\"\n",
        "\n",
        "def on_reset(_):\n",
        "    \"\"\"\n",
        "    HARD RESET: forget any saved key and show bare-bones Key_Df.\n",
        "    Deletes _uploaded_key.(xlsx|csv) if present and clears uploaded_key_path.\n",
        "    \"\"\"\n",
        "    # 1) Forget path in memory\n",
        "    globals().pop('uploaded_key_path', None)\n",
        "\n",
        "    # 2) Remove any persisted key files from disk\n",
        "    import os\n",
        "    for ext in (\"xlsx\", \"csv\"):\n",
        "        try:\n",
        "            os.remove(f\"_uploaded_key.{ext}\")\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    # 3) Show bare-bones\n",
        "    build_or_rematch_key_df(None)\n",
        "    rebuild_grid(\"Reset: cleared saved key; showing bare-bones Key_Df.\")\n",
        "\n",
        "def download_df(_):\n",
        "    global Key_Df\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Saving latest Key_Df as XLSX ...\")\n",
        "    try:\n",
        "        path = \"/content/Key_Df.xlsx\"\n",
        "        Key_Df.to_excel(path, index=False, engine='openpyxl')\n",
        "        colab_files.download(path)\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Saved and downloading Key_Df.xlsx ...\")\n",
        "    except Exception as e:\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Error while saving/downloading: {e}\")\n",
        "\n",
        "upload_btn.on_click(on_colab_upload)\n",
        "reset_btn.on_click(on_reset)\n",
        "download_button.on_click(download_df)\n",
        "\n",
        "upload_row = widgets.HBox([\n",
        "    widgets.HTML(\"<b>Optional key:</b>\"),\n",
        "    upload_btn,\n",
        "    reset_btn,\n",
        "    download_button\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# Edit / rematch / download controls\n",
        "# ---------------------------\n",
        "new_col_name    = widgets.Text(placeholder='Enter new column name', description='New Col:')\n",
        "add_col_button  = widgets.Button(description='Add Column', button_style='info')\n",
        "apply_button    = widgets.Button(description='Apply Changes', button_style='primary', layout=widgets.Layout(width=\"120px\"))\n",
        "\n",
        "def add_column(_):\n",
        "    global Key_Df\n",
        "    col = new_col_name.value.strip()\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        if not col:\n",
        "            print(\"Please enter a column name.\"); return\n",
        "        if col in Key_Df.columns:\n",
        "            print(f\"Column '{col}' already exists.\"); return\n",
        "        Key_Df[col] = \"\"\n",
        "        print(f\"Added column '{col}' to Key_Df.\")\n",
        "    rebuild_grid()\n",
        "\n",
        "def apply_edits(_):\n",
        "    global Key_Df\n",
        "    try:\n",
        "        Key_Df = grid.data.copy().reset_index(drop=True)\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Applied grid edits to Key_Df ({len(Key_Df)} rows, {len(Key_Df.columns)} cols).\")\n",
        "    except Exception as e:\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Error applying edits: {e}\")\n",
        "\n",
        "add_col_button.on_click(add_column)\n",
        "apply_button.on_click(apply_edits)\n",
        "\n",
        "controls = widgets.HBox([new_col_name, add_col_button, apply_button])\n",
        "\n",
        "# ---------------------------\n",
        "# Initialize UI\n",
        "# ---------------------------\n",
        "Key_Df = _make_base_key_df()\n",
        "Key_Df[\"match_status\"] = \"No key\"\n",
        "\n",
        "grid = make_grid(Key_Df.copy().reset_index(drop=True))\n",
        "ui = widgets.VBox([upload_row, grid, controls, status_box])\n",
        "display(ui)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p6hqDVU-Uzm",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Analyze FR1 metrics\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from datetime import datetime\n",
        "\n",
        "files_list = feds\n",
        "assert isinstance(files_list, list) and len(files_list) > 0, \"No FED3 files loaded.\"\n",
        "assert 'Key_Df' in globals() and isinstance(Key_Df, pd.DataFrame), \"Build/rematch Key_Df first.\"\n",
        "\n",
        "# =============================================================================\n",
        "# 0) Build CLEAN metadata_df from Key_Df (keep what we need for merging + naming)\n",
        "# =============================================================================\n",
        "\n",
        "def _clean_colname(c):\n",
        "    c = str(c).strip()\n",
        "    c = c.replace(\"$\", \"\")\n",
        "    c = re.sub(r\"\\s+\", \"_\", c)\n",
        "    c = re.sub(r\"_+\", \"_\", c)\n",
        "    return c\n",
        "\n",
        "def _basename(pathlike) -> str:\n",
        "    s = str(pathlike).replace(\"\\\\\", \"/\")\n",
        "    return s.split(\"/\")[-1]\n",
        "\n",
        "# Start from Key_Df\n",
        "metadata_df = Key_Df.copy().reset_index(drop=True)\n",
        "metadata_df.columns = [_clean_colname(c) for c in metadata_df.columns]\n",
        "\n",
        "# Normalize filename values if present\n",
        "if \"filename\" in metadata_df.columns:\n",
        "    metadata_df[\"filename\"] = metadata_df[\"filename\"].astype(str).map(os.path.basename).map(_basename)\n",
        "\n",
        "# Normalize Mouse_ID values if present\n",
        "if \"Mouse_ID\" in metadata_df.columns:\n",
        "    metadata_df[\"Mouse_ID\"] = metadata_df[\"Mouse_ID\"].astype(str).str.strip()\n",
        "\n",
        "# Your requested 7 fields (after cleaning $ etc.)\n",
        "wanted7 = [\"Genotype\", \"Gene\", \"Strain\", \"Sex\", \"Diet\", \"Treatment\", \"Condition\"]\n",
        "\n",
        "# Canonicalize casing for wanted7 if they exist with different case\n",
        "lower_map = {c.lower(): c for c in metadata_df.columns}\n",
        "rename_map = {}\n",
        "for w in wanted7:\n",
        "    c = lower_map.get(w.lower(), None)\n",
        "    if c is not None and c != w:\n",
        "        rename_map[c] = w\n",
        "metadata_df = metadata_df.rename(columns=rename_map)\n",
        "\n",
        "# Keep columns needed:\n",
        "# - for merging: filename and/or Mouse_ID (keep both if present; we will drop extras at export)\n",
        "# - for your 7 fields\n",
        "# - for filename generation at end: Gene_ID / Strain_ID / Session_type if present\n",
        "always_for_merge = [c for c in [\"filename\", \"Mouse_ID\"] if c in metadata_df.columns]\n",
        "always_for_naming = [c for c in [\"Session_type\", \"Gene_ID\", \"Strain_ID\"] if c in metadata_df.columns]\n",
        "keep_cols = list(dict.fromkeys(always_for_merge + [c for c in wanted7 if c in metadata_df.columns] + always_for_naming))\n",
        "metadata_df = metadata_df.loc[:, keep_cols].copy()\n",
        "\n",
        "assert isinstance(metadata_df, pd.DataFrame) and len(metadata_df) > 0, \"metadata_df construction failed.\"\n",
        "\n",
        "# =============================================================================\n",
        "# 1) Helper functions (unchanged)\n",
        "# =============================================================================\n",
        "\n",
        "def _get_sessions():\n",
        "    if 'feds' in globals() and isinstance(feds, (list, tuple)) and len(feds) > 0:\n",
        "        return list(feds)\n",
        "    raise RuntimeError(\"No FED3 sessions found. Expecting a non-empty 'feds' list.\")\n",
        "\n",
        "def _safe_col(df, candidates):\n",
        "    norm = lambda s: str(s).strip().lower().replace('-', '_').replace(' ', '_')\n",
        "    lmap = {norm(c): c for c in df.columns}\n",
        "    for cand in candidates:\n",
        "        key = norm(cand)\n",
        "        if key in lmap:\n",
        "            return lmap[key]\n",
        "    return None\n",
        "\n",
        "def _prep_events(df):\n",
        "    ev_col = _safe_col(df, [\"Event\", \"event\"])\n",
        "    if ev_col is None:\n",
        "        return df.iloc[0:0].copy(), None\n",
        "    return df[df[ev_col].isin([\"Left\", \"Right\", \"Pellet\"])].copy(), ev_col\n",
        "\n",
        "def _pellet_times_from_df(df):\n",
        "    ev_col = _safe_col(df, [\"Event\", \"event\"])\n",
        "    if ev_col is None:\n",
        "        return []\n",
        "    pel = df[df[ev_col] == \"Pellet\"]\n",
        "    if isinstance(pel.index, pd.DatetimeIndex):\n",
        "        ts = pel.index.to_series()\n",
        "    else:\n",
        "        for cand in [\"MM:DD:YYYY hh:mm:ss\", \"DateTime\", \"Datetime\", \"Timestamp\", \"timestamp\", \"datetime\"]:\n",
        "            if cand in pel.columns:\n",
        "                ts = pd.to_datetime(pel[cand], errors=\"coerce\")\n",
        "                break\n",
        "        else:\n",
        "            ts = pd.to_datetime(pel.index, errors=\"coerce\")\n",
        "    ts = ts.dropna().sort_values()\n",
        "    return ts.to_list()\n",
        "\n",
        "def _get_ts_series_from_col_or_index(df, ts_candidates=(\"MM:DD:YYYY hh:mm:ss\",\"DateTime\",\"Datetime\",\"Timestamp\",\"timestamp\",\"datetime\")):\n",
        "    for cand in ts_candidates:\n",
        "        if cand in df.columns:\n",
        "            ts = pd.to_datetime(df[cand], errors=\"coerce\")\n",
        "            if not isinstance(ts, pd.Series):\n",
        "                ts = pd.Series(ts, index=df.index)\n",
        "            else:\n",
        "                ts = ts.reindex(df.index)\n",
        "            return ts\n",
        "    if isinstance(df.index, pd.DatetimeIndex):\n",
        "        return pd.Series(df.index, index=df.index)\n",
        "    return pd.to_datetime(pd.Series(df.index, index=df.index), errors=\"coerce\")\n",
        "\n",
        "def _split_day_night_masks(ts):\n",
        "    valid = ts.notna()\n",
        "    hrs = ts.dt.hour\n",
        "    day_mask = valid & (hrs >= 6) & (hrs < 18)\n",
        "    night_mask = valid & ~day_mask\n",
        "    return day_mask, night_mask\n",
        "\n",
        "def _cluster_times(ts_list, max_interval_sec=60):\n",
        "    if not ts_list:\n",
        "        return []\n",
        "    clusters, current = [], [ts_list[0]]\n",
        "    for i in range(1, len(ts_list)):\n",
        "        if (ts_list[i] - ts_list[i-1]).total_seconds() <= max_interval_sec:\n",
        "            current.append(ts_list[i])\n",
        "        else:\n",
        "            clusters.append(current); current = [ts_list[i]]\n",
        "    clusters.append(current)\n",
        "    return clusters\n",
        "\n",
        "def _subset_meal_metrics(df, ts_series, max_interval_sec=60):\n",
        "    out = {\n",
        "        \"%MealPellets\": np.nan,\n",
        "        \"%GrazingPellets\": np.nan,\n",
        "        \"Pellets\": 0,\n",
        "        \"NumMeals\": np.nan,\n",
        "        \"AvgMealSize\": np.nan,\n",
        "        \"AvgMealDuration\": np.nan,\n",
        "        \"MealsPerHour\": np.nan,\n",
        "        \"Accuracy\": np.nan,\n",
        "    }\n",
        "    ev_col = _safe_col(df, [\"Event\", \"event\"])\n",
        "    if ev_col is None or df.empty:\n",
        "        return out\n",
        "\n",
        "    left_n = int((df[ev_col] == \"Left\").sum())\n",
        "    right_n = int((df[ev_col] == \"Right\").sum())\n",
        "    denom = left_n + right_n\n",
        "    if denom > 0:\n",
        "        out[\"Accuracy\"] = 100.0 * (left_n / denom)\n",
        "\n",
        "    pel_mask = (df[ev_col] == \"Pellet\")\n",
        "    pel_ts = ts_series[pel_mask].dropna().sort_values()\n",
        "    out[\"Pellets\"] = int(pel_ts.size)\n",
        "    if pel_ts.size == 0:\n",
        "        return out\n",
        "\n",
        "    ts_list = pel_ts.to_list()\n",
        "    clusters = _cluster_times(ts_list, max_interval_sec=max_interval_sec)\n",
        "    meal_clusters = [c for c in clusters if len(c) >= 3]\n",
        "    grazing_clusters = [c for c in clusters if 1 <= len(c) < 3]\n",
        "\n",
        "    meal_pellets = sum(len(c) for c in meal_clusters)\n",
        "    grazing_pellets = sum(len(c) for c in grazing_clusters)\n",
        "    total_pg = meal_pellets + grazing_pellets\n",
        "    if total_pg > 0:\n",
        "        out[\"%MealPellets\"] = 100.0 * meal_pellets / total_pg\n",
        "        out[\"%GrazingPellets\"] = 100.0 * grazing_pellets / total_pg\n",
        "\n",
        "    if len(meal_clusters) > 0:\n",
        "        out[\"NumMeals\"] = float(len(meal_clusters))\n",
        "        out[\"AvgMealSize\"] = float(np.mean([len(c) for c in meal_clusters]))\n",
        "        out[\"AvgMealDuration\"] = float(np.mean([(c[-1] - c[0]).total_seconds() for c in meal_clusters]))\n",
        "    else:\n",
        "        out[\"NumMeals\"] = 0.0\n",
        "\n",
        "    if len(ts_list) >= 2:\n",
        "        rec_hours = (ts_list[-1] - ts_list[0]).total_seconds() / 3600.0\n",
        "        if rec_hours > 0:\n",
        "            out[\"MealsPerHour\"] = (out[\"NumMeals\"] / rec_hours) if np.isfinite(out[\"NumMeals\"]) else np.nan\n",
        "    return out\n",
        "\n",
        "def _estimate_daily_pellets(df):\n",
        "    ts = _get_ts_series_from_col_or_index(df)\n",
        "    ts = ts.dropna().sort_values()\n",
        "    if ts.size < 2:\n",
        "        return np.nan\n",
        "    duration_hours = (ts.iloc[-1] - ts.iloc[0]).total_seconds() / 3600.0\n",
        "    if duration_hours <= 0:\n",
        "        return np.nan\n",
        "\n",
        "    pellet_events = np.nan\n",
        "    pc_col = _safe_col(df, [\"Pellet_Count\", \"pellet_count\"])\n",
        "    if pc_col is not None:\n",
        "        pc = pd.to_numeric(df[pc_col], errors='coerce')\n",
        "        if pc.notna().any():\n",
        "            diffs = pc.diff().fillna(0).clip(lower=0)\n",
        "            pellet_events = float(diffs.sum())\n",
        "            if pellet_events == 0 and pc.iloc[-1] >= pc.iloc[0]:\n",
        "                pellet_events = float(pc.iloc[-1] - pc.iloc[0])\n",
        "\n",
        "    if np.isnan(pellet_events):\n",
        "        ev_col = _safe_col(df, [\"Event\", \"event\"])\n",
        "        if ev_col is not None:\n",
        "            pellet_events = float((df[ev_col] == \"Pellet\").sum())\n",
        "\n",
        "    if np.isnan(pellet_events):\n",
        "        return np.nan\n",
        "    return (pellet_events / duration_hours) * 24.0\n",
        "\n",
        "# =============================================================================\n",
        "# 2) Load sessions\n",
        "# =============================================================================\n",
        "\n",
        "_sessions = _get_sessions()\n",
        "\n",
        "# Use md as the cleaned key table\n",
        "md = metadata_df.copy()\n",
        "\n",
        "# Ensure filename is basenames if present\n",
        "if \"filename\" in md.columns:\n",
        "    md[\"filename\"] = md[\"filename\"].astype(str).map(_basename)\n",
        "\n",
        "# =============================================================================\n",
        "# 3) Core FR1 metrics per file\n",
        "# =============================================================================\n",
        "\n",
        "rows = []\n",
        "for idx, c_df in enumerate(_sessions):\n",
        "    file_name = _basename(getattr(c_df, \"name\", f\"File_{idx}\"))\n",
        "    d, ev = _prep_events(c_df)\n",
        "\n",
        "    if ev is None or d.empty:\n",
        "        pellets = left = right = lwp = 0\n",
        "        rt_med = ipi_med = pt_med = np.nan\n",
        "    else:\n",
        "        pellets = int((d[ev] == \"Pellet\").sum())\n",
        "        left    = int((d[ev] == \"Left\").sum())\n",
        "        right   = int((d[ev] == \"Right\").sum())\n",
        "        lwp     = int((c_df[ev] == \"LeftWithPellet\").sum()) if ev in c_df.columns else 0\n",
        "\n",
        "        rt_col  = _safe_col(d, [\"Retrieval_Time\", \"retrieval_time\"])\n",
        "        ipi_col = _safe_col(d, [\"InterPelletInterval\", \"interpelletinterval\", \"inter_pellet_interval\"])\n",
        "        pt_col  = _safe_col(d, [\"Poke_Time\", \"poke_time\"])\n",
        "\n",
        "        rt_med  = pd.to_numeric(d.get(rt_col, pd.Series(dtype=float)), errors=\"coerce\").median() if rt_col else np.nan\n",
        "        ipi_med = pd.to_numeric(d.get(ipi_col, pd.Series(dtype=float)), errors=\"coerce\").median() if ipi_col else np.nan\n",
        "        pt_med  = pd.to_numeric(d.get(pt_col, pd.Series(dtype=float)), errors=\"coerce\").median() if pt_col else np.nan\n",
        "\n",
        "    total_pokes = left + right\n",
        "    acc = (left / total_pokes * 100.0) if total_pokes > 0 else np.nan\n",
        "    ppp = (total_pokes / pellets) if pellets > 0 else np.nan\n",
        "    daily_pel = _estimate_daily_pellets(c_df)\n",
        "\n",
        "    rows.append({\n",
        "        \"File\": file_name,\n",
        "        \"FileIndex\": idx,\n",
        "        \"Pellets\": pellets,\n",
        "        \"Left_Poke\": left,\n",
        "        \"Right_Poke\": right,\n",
        "        \"Total_Pokes\": total_pokes,\n",
        "        \"Accuracy\": acc,\n",
        "        \"PokesPerPellet\": ppp,\n",
        "        \"RetrievalTime\": rt_med,\n",
        "        \"InterPelletInterval\": ipi_med,\n",
        "        \"PokeTime\": pt_med,\n",
        "        \"Daily_Pellets\": daily_pel,\n",
        "        \"Left Poke with Pellet\": lwp,\n",
        "    })\n",
        "\n",
        "FR1metrics = pd.DataFrame(rows)\n",
        "if FR1metrics.empty:\n",
        "    display(HTML(\"<b style='color:#b00'>No files to analyze.</b>\"))\n",
        "    raise SystemExit\n",
        "\n",
        "# =============================================================================\n",
        "# 4) Meal/grazing metrics\n",
        "# =============================================================================\n",
        "\n",
        "def compute_within_meal_mode_from_sessions(sessions, max_interval_sec=60, min_samples=5):\n",
        "    rows = []\n",
        "    try:\n",
        "        from scipy.stats import gaussian_kde\n",
        "        use_kde = True\n",
        "    except Exception:\n",
        "        use_kde = False\n",
        "\n",
        "    for i, df in enumerate(sessions):\n",
        "        file = _basename(getattr(df, \"name\", f\"File_{i}\"))\n",
        "        ipi_col = _safe_col(df, [\"InterPelletInterval\", \"interpelletinterval\", \"inter_pellet_interval\"])\n",
        "        if ipi_col is None or df.empty:\n",
        "            rows.append({\"File\": file, \"Within_meal_pellet_rate\": np.nan}); continue\n",
        "\n",
        "        vals = pd.to_numeric(df[ipi_col], errors=\"coerce\").dropna()\n",
        "        vals = vals[(vals > 0) & (vals <= max_interval_sec)]\n",
        "        if vals.size < min_samples:\n",
        "            rows.append({\"File\": file, \"Within_meal_pellet_rate\": np.nan}); continue\n",
        "\n",
        "        logv = np.log10(vals.values)\n",
        "\n",
        "        if use_kde:\n",
        "            kde = gaussian_kde(logv, bw_method=\"scott\")\n",
        "            xs  = np.linspace(logv.min(), logv.max(), 1000)\n",
        "            ys  = kde(xs)\n",
        "            peak_log10 = xs[np.argmax(ys)]\n",
        "        else:\n",
        "            xs = np.linspace(logv.min(), logv.max(), 256)\n",
        "            hist, edges = np.histogram(logv, bins=xs)\n",
        "            centers = 0.5 * (edges[:-1] + edges[1:])\n",
        "            peak_log10 = centers[np.argmax(hist)]\n",
        "\n",
        "        peak_seconds = float(10.0 ** peak_log10)\n",
        "        rows.append({\"File\": file, \"Within_meal_pellet_rate\": peak_seconds})\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def compute_meal_bout_metrics_from_sessions(sessions, max_interval_sec=60):\n",
        "    rows, recinfo = [], []\n",
        "    for i, df in enumerate(sessions):\n",
        "        file = _basename(getattr(df, \"name\", f\"File_{i}\"))\n",
        "        ts = _pellet_times_from_df(df)\n",
        "        if len(ts) < 2:\n",
        "            continue\n",
        "        duration_hr = (ts[-1] - ts[0]).total_seconds() / 3600.0\n",
        "        recinfo.append({\"File\": file, \"RecordingHours\": duration_hr})\n",
        "\n",
        "        clusters = _cluster_times(ts, max_interval_sec=max_interval_sec)\n",
        "        meal_id = 0\n",
        "        for c in clusters:\n",
        "            if len(c) >= 3:\n",
        "                rows.append({\n",
        "                    \"File\": file,\n",
        "                    \"MealID\": meal_id,\n",
        "                    \"MealSize\": len(c),\n",
        "                    \"MealDuration_sec\": (c[-1] - c[0]).total_seconds(),\n",
        "                })\n",
        "                meal_id += 1\n",
        "    meal_df = pd.DataFrame(rows)\n",
        "    rec_df  = pd.DataFrame(recinfo)\n",
        "    if meal_df.empty:\n",
        "        out = pd.DataFrame(columns=[\"File\",\"NumMeals\",\"AvgMealSize\",\"AvgMealDuration\",\"RecordingHours\",\"MealsPerHour\"])\n",
        "    else:\n",
        "        out = (meal_df.groupby(\"File\")\n",
        "               .agg(NumMeals=(\"MealID\",\"count\"), AvgMealSize=(\"MealSize\",\"mean\"), AvgMealDuration=(\"MealDuration_sec\",\"mean\"))\n",
        "               .reset_index())\n",
        "    out = out.merge(rec_df, on=\"File\", how=\"left\")\n",
        "    out[\"MealsPerHour\"] = out[\"NumMeals\"] / out[\"RecordingHours\"]\n",
        "    return out\n",
        "\n",
        "def compute_meal_pellet_distribution_from_sessions(sessions, max_interval_sec=60):\n",
        "    rows = []\n",
        "    for i, df in enumerate(sessions):\n",
        "        file = _basename(getattr(df, \"name\", f\"File_{i}\"))\n",
        "        ts = _pellet_times_from_df(df)\n",
        "        if len(ts) == 0:\n",
        "            rows.append({\"File\": file, \"%MealPellets\": np.nan, \"%GrazingPellets\": np.nan})\n",
        "            continue\n",
        "        clusters = _cluster_times(ts, max_interval_sec=max_interval_sec)\n",
        "        meal_pellets    = sum(len(c) for c in clusters if len(c) >= 3)\n",
        "        grazing_pellets = sum(len(c) for c in clusters if 1 <= len(c) < 3)\n",
        "        total = meal_pellets + grazing_pellets\n",
        "        rows.append({\n",
        "            \"File\": file,\n",
        "            \"%MealPellets\": (100 * meal_pellets / total) if total else np.nan,\n",
        "            \"%GrazingPellets\": (100 * grazing_pellets / total) if total else np.nan\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "meal_info_df   = compute_meal_bout_metrics_from_sessions(_sessions, max_interval_sec=60)\n",
        "pellet_dist_df = compute_meal_pellet_distribution_from_sessions(_sessions, max_interval_sec=60)\n",
        "\n",
        "FR1_enriched = (\n",
        "    FR1metrics\n",
        "    .merge(pellet_dist_df, on=\"File\", how=\"left\")\n",
        "    .merge(meal_info_df, on=\"File\", how=\"left\")\n",
        ")\n",
        "\n",
        "wmpr_df = compute_within_meal_mode_from_sessions(_sessions, max_interval_sec=60, min_samples=5)\n",
        "FR1_enriched = FR1_enriched.merge(wmpr_df, on=\"File\", how=\"left\")\n",
        "\n",
        "# =============================================================================\n",
        "# 5) Day/Night metrics\n",
        "# =============================================================================\n",
        "\n",
        "_day_night_bases = [\"%MealPellets\",\"%GrazingPellets\",\"Pellets\",\"NumMeals\",\"AvgMealSize\",\"AvgMealDuration\",\"MealsPerHour\",\"Accuracy\"]\n",
        "for base in _day_night_bases:\n",
        "    FR1_enriched[f\"{base}_Day\"] = np.nan\n",
        "    FR1_enriched[f\"{base}_Night\"] = np.nan\n",
        "\n",
        "for i, df_all in enumerate(_sessions):\n",
        "    file = _basename(getattr(df_all, \"name\", f\"File_{i}\"))\n",
        "    ts_all = _get_ts_series_from_col_or_index(df_all)\n",
        "    day_mask, night_mask = _split_day_night_masks(ts_all)\n",
        "\n",
        "    day_df = df_all.loc[day_mask]\n",
        "    night_df = df_all.loc[night_mask]\n",
        "\n",
        "    day_vals = _subset_meal_metrics(day_df, ts_all.loc[day_mask])\n",
        "    night_vals = _subset_meal_metrics(night_df, ts_all.loc[night_mask])\n",
        "\n",
        "    row_mask = FR1_enriched[\"File\"] == file\n",
        "    for base in _day_night_bases:\n",
        "        FR1_enriched.loc[row_mask, f\"{base}_Day\"] = day_vals[base]\n",
        "        FR1_enriched.loc[row_mask, f\"{base}_Night\"] = night_vals[base]\n",
        "\n",
        "# =============================================================================\n",
        "# 6) Attach metadata (robust: can work whether you match by filename or Mouse_ID)\n",
        "# =============================================================================\n",
        "\n",
        "# Decide match key for OUTPUT (your KEY_MATCH_MODE)\n",
        "match_mode = globals().get('KEY_MATCH_MODE', None)\n",
        "if match_mode == 'filename':\n",
        "    id_col = 'filename'\n",
        "elif match_mode == 'mouse_id':\n",
        "    id_col = 'Mouse_ID'\n",
        "else:\n",
        "    # fallback output ID\n",
        "    id_col = 'Mouse_ID' if 'Mouse_ID' in md.columns else 'filename'\n",
        "\n",
        "# Normalize a filename key in FR1_enriched for merging if possible\n",
        "FR1_enriched[\"filename\"] = FR1_enriched[\"File\"].astype(str).map(_basename)\n",
        "\n",
        "# If we have filename<->Mouse_ID in md, populate Mouse_ID from filename mapping (helps even if output is mouse_id)\n",
        "if \"filename\" in md.columns and \"Mouse_ID\" in md.columns:\n",
        "    mouse_map = md.dropna(subset=[\"filename\"]).drop_duplicates(\"filename\").set_index(\"filename\")[\"Mouse_ID\"]\n",
        "    FR1_enriched[\"Mouse_ID\"] = FR1_enriched[\"filename\"].map(mouse_map)\n",
        "\n",
        "# Build a single md_unique for merging:\n",
        "# - prefer Mouse_ID merge when Mouse_ID exists in both and is populated\n",
        "fm = FR1_enriched.copy()\n",
        "\n",
        "if \"Mouse_ID\" in fm.columns and \"Mouse_ID\" in md.columns and fm[\"Mouse_ID\"].notna().any():\n",
        "    md_mouse_unique = md.drop_duplicates(subset=[\"Mouse_ID\"], keep=\"first\")\n",
        "    fm = fm.merge(md_mouse_unique, on=\"Mouse_ID\", how=\"left\", suffixes=(\"\", \"_md\"))\n",
        "\n",
        "# Fallback/secondary merge by filename for any rows still missing the 7 fields\n",
        "need_cols = [c for c in wanted7 if c in md.columns]\n",
        "if \"filename\" in md.columns and need_cols:\n",
        "    missing_any = fm[need_cols].isna().all(axis=1) if all(c in fm.columns for c in need_cols) else pd.Series([True]*len(fm))\n",
        "    if missing_any.any():\n",
        "        md_file_unique = md.drop_duplicates(subset=[\"filename\"], keep=\"first\")\n",
        "        fb = fm.loc[missing_any].drop(columns=[c for c in md_file_unique.columns if c in fm.columns and c != \"filename\"], errors=\"ignore\")\n",
        "        fb = fb.merge(md_file_unique, on=\"filename\", how=\"left\", suffixes=(\"\", \"_md\"))\n",
        "        fm.loc[missing_any, fb.columns] = fb.values\n",
        "\n",
        "# =============================================================================\n",
        "# 7) Session-type suffixing (unchanged)\n",
        "# =============================================================================\n",
        "\n",
        "metric_cols_all = [\n",
        "    \"Pellets\", \"Left_Poke\", \"Right_Poke\", \"Total_Pokes\", \"Accuracy\",\n",
        "    \"PokesPerPellet\", \"RetrievalTime\", \"InterPelletInterval\", \"PokeTime\",\n",
        "    \"%MealPellets\", \"%GrazingPellets\", \"NumMeals\", \"AvgMealSize\",\n",
        "    \"AvgMealDuration\", \"RecordingHours\", \"MealsPerHour\",\n",
        "    \"Daily_Pellets\", \"Left Poke with Pellet\",\"Within_meal_pellet_rate\",\n",
        "    \"%MealPellets_Day\",\"%GrazingPellets_Day\",\"Pellets_Day\",\"NumMeals_Day\",\"AvgMealSize_Day\",\"AvgMealDuration_Day\",\"MealsPerHour_Day\",\"Accuracy_Day\",\n",
        "    \"%MealPellets_Night\",\"%GrazingPellets_Night\",\"Pellets_Night\",\"NumMeals_Night\",\"AvgMealSize_Night\",\"AvgMealDuration_Night\",\"MealsPerHour_Night\",\"Accuracy_Night\",\n",
        "]\n",
        "\n",
        "if 'Session_type' in fm.columns:\n",
        "    session_series = fm['Session_type'].astype(str).str.strip()\n",
        "else:\n",
        "    sess_map = {\n",
        "        _basename(getattr(_sessions[i], \"name\", f\"File_{i}\")):\n",
        "        (_sessions[i].attrs.get(\"Session_type\") or \"Unknown\")\n",
        "        for i in range(len(_sessions))\n",
        "    }\n",
        "    session_series = fm[\"File\"].map(sess_map).fillna(\"Unknown\").astype(str)\n",
        "\n",
        "session_series = session_series.str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "fm[\"_Session_type_for_csv\"] = session_series\n",
        "\n",
        "def with_session_suffix_for_csv(df, metrics=metric_cols_all, session_col=\"_Session_type_for_csv\"):\n",
        "    df = df.copy()\n",
        "    for m in metrics:\n",
        "        if m not in df.columns:\n",
        "            continue\n",
        "        for sess in df[session_col].dropna().unique():\n",
        "            mask = df[session_col] == sess\n",
        "            col_name = f\"{m}_{sess}\"\n",
        "            if col_name not in df.columns:\n",
        "                df[col_name] = np.nan\n",
        "            df.loc[mask, col_name] = df.loc[mask, m]\n",
        "        df.drop(columns=[m], inplace=True)\n",
        "    return df.drop(columns=[session_col])\n",
        "\n",
        "FR1metrics_merged = fm.copy()\n",
        "FR1metrics_csv    = with_session_suffix_for_csv(FR1metrics_merged)\n",
        "\n",
        "# =============================================================================\n",
        "# 8) FINAL EXPORT: ONLY [match key] + 7 key fields + metrics (NO File/FileIndex, NO duplicate IDs)\n",
        "# =============================================================================\n",
        "\n",
        "def _metric_match(col):\n",
        "    return any(col.startswith(base + \"_\") for base in metric_cols_all)\n",
        "\n",
        "metric_keep = [c for c in FR1metrics_csv.columns if _metric_match(c)]\n",
        "if not metric_keep:\n",
        "    raise RuntimeError(\"No session-suffixed metric columns matched; check 'metric_cols_all'.\")\n",
        "\n",
        "# Ensure output ID exists; if matching by filename, use filename; if mouse_id, use Mouse_ID\n",
        "if id_col not in FR1metrics_csv.columns:\n",
        "    raise ValueError(f\"Output id_col '{id_col}' not found in FR1metrics_csv.\")\n",
        "\n",
        "meta_keep = [c for c in wanted7 if c in FR1metrics_csv.columns]\n",
        "\n",
        "# Drop stuff you never want exported\n",
        "drop_cols = [c for c in [\"File\", \"FileIndex\"] if c in FR1metrics_csv.columns]\n",
        "other_id = \"Mouse_ID\" if id_col == \"filename\" else \"filename\"\n",
        "if other_id in FR1metrics_csv.columns:\n",
        "    drop_cols.append(other_id)\n",
        "\n",
        "FR1metrics_csv = FR1metrics_csv.drop(columns=drop_cols, errors=\"ignore\")\n",
        "\n",
        "# Final column order\n",
        "cols_out = [id_col] + meta_keep + metric_keep\n",
        "cols_out = [c for c in cols_out if c in FR1metrics_csv.columns]\n",
        "FR1metrics_csv = FR1metrics_csv.loc[:, cols_out].copy()\n",
        "\n",
        "# =============================================================================\n",
        "# 9) Save & present (unchanged, but now uses cleaned merged metadata)\n",
        "# =============================================================================\n",
        "\n",
        "example = FR1metrics_merged.iloc[0]\n",
        "\n",
        "strain_name = str(example.get(\"Gene\", example.get(\"Strain\", \"FR1\"))).replace(\" \", \"_\")\n",
        "\n",
        "strain_num_raw = example.get(\"Gene_ID\", example.get(\"Strain_ID\", \"Metrics\"))\n",
        "try:\n",
        "    strain_num = f\"{int(strain_num_raw):03d}\"\n",
        "except Exception:\n",
        "    strain_num = str(strain_num_raw).zfill(3)\n",
        "\n",
        "task_name = str(example.get(\"Session_type\", \"Unknown\")).replace(\" \", \"_\")\n",
        "\n",
        "fname = f\"{strain_name}_{strain_num}_{task_name}_L3.csv\"\n",
        "\n",
        "FR1metrics_csv.to_csv(fname, index=False)\n",
        "display(HTML(f\"<b> Saved metrics CSV to:</b> <code>{fname}</code>\"))\n",
        "\n",
        "# Download button\n",
        "btn = widgets.Button(\n",
        "    description=f\"Download {os.path.basename(fname)}\",\n",
        "    icon=\"download\",\n",
        "    tooltip=\"Click to download the metrics CSV\",\n",
        "    layout=widgets.Layout(width=\"auto\"),\n",
        ")\n",
        "status = widgets.HTML()\n",
        "\n",
        "def _on_click(b):\n",
        "    clear_output(wait=True)\n",
        "    display(btn, status)\n",
        "    if not os.path.exists(fname):\n",
        "        status.value = f\"<b style='color:#b00'>File not found:</b> {fname}\"\n",
        "        return\n",
        "    try:\n",
        "        from google.colab import files as gfiles\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(fname)}</code>\"\n",
        "        gfiles.download(fname)\n",
        "    except Exception:\n",
        "        status.value = (\n",
        "            \"Not running in Colab. File saved locally at:<br>\"\n",
        "            f\"<code>{fname}</code><br>\"\n",
        "            \"Use the link above to open it.\"\n",
        "        )\n",
        "\n",
        "display(btn, status)\n",
        "btn.on_click(_on_click)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmpnolCCKyW6"
      },
      "outputs": [],
      "source": [
        "# @title Group for plotting\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# --- sanity ---\n",
        "if 'metadata_df' not in globals() or metadata_df is None or metadata_df.empty:\n",
        "    raise RuntimeError(\"metadata_df is missing or empty. Build metadata_df (copy of Key_Df) first.\")\n",
        "\n",
        "EXCLUDE_LOWER = {\"match_status\"}   # everything else is allowed\n",
        "\n",
        "def _build_file_column(df):\n",
        "    if \"filename\" in df.columns:\n",
        "        return df[\"filename\"].apply(lambda p: os.path.basename(str(p)))\n",
        "    if \"FED3_from_file\" in df.columns and \"Date_from_file\" in df.columns:\n",
        "        return \"FED\" + df[\"FED3_from_file\"].astype(str) + \"_\" + df[\"Date_from_file\"].astype(str)\n",
        "    if \"FED3_from_file\" in df.columns:\n",
        "        return \"FED\" + df[\"FED3_from_file\"].astype(str)\n",
        "    return df.index.astype(str)\n",
        "\n",
        "def _norm_val(x):\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s.lower() in {\"nan\", \"none\"}:\n",
        "        return \"UNK\"\n",
        "    return s.upper()\n",
        "\n",
        "def _build_group_row(row, ordered_cols):\n",
        "    if not ordered_cols:\n",
        "        return \"ALL\"\n",
        "    return \" \".join(_norm_val(row[c]) for c in ordered_cols)\n",
        "\n",
        "def build_mapping(ordered_cols):\n",
        "    _meta = metadata_df.copy()\n",
        "    _meta[\"File\"] = _build_file_column(_meta)\n",
        "    _meta[\"Group\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols), axis=1)\n",
        "    mapping = (\n",
        "        _meta[[\"File\", \"Group\"]]\n",
        "        .dropna(subset=[\"File\"])\n",
        "        .drop_duplicates()\n",
        "        .sort_values([\"Group\", \"File\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    return mapping\n",
        "\n",
        "def _unique_keep_order(seq):\n",
        "    seen = set(); out = []\n",
        "    for x in seq:\n",
        "        if x not in seen:\n",
        "            seen.add(x); out.append(x)\n",
        "    return out\n",
        "\n",
        "# ---------- UI (fixed sizes + grid) ----------\n",
        "PX_W = \"260px\"   # list box width\n",
        "PX_H = \"160px\"   # list box height\n",
        "BTN_W = \"160px\"  # button column width\n",
        "HDR_H = \"28px\"   # header cell height (consistent across all headers)\n",
        "\n",
        "title = widgets.HTML(\"<h3>Select columns to group by for X and Hue, then reorder X to set hierarchy</h3>\")\n",
        "\n",
        "all_cols = sorted((c for c in metadata_df.columns if str(c).lower() not in EXCLUDE_LOWER), key=str.lower)\n",
        "\n",
        "def header(text):\n",
        "    # Normalize header height/margins so they align perfectly in the grid row\n",
        "    return widgets.HTML(\n",
        "        f\"<div style='height:{HDR_H};display:flex;align-items:flex-end;'>\"\n",
        "        f\"<h4 style=\\\"margin:0;\\\">{text}</h4></div>\"\n",
        "    )\n",
        "\n",
        "# Headers (row 1 of grid)\n",
        "available_hdr = header(\"Available\")\n",
        "actions_hdr   = header(\"Actions\")\n",
        "x_hdr         = header(\"X grouping\")\n",
        "hue_hdr       = header(\"Hue grouping\")\n",
        "\n",
        "# Widgets (row 2 of grid)\n",
        "available = widgets.SelectMultiple(\n",
        "    options=all_cols, value=tuple(), rows=14,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "right_x = widgets.Select(\n",
        "    options=[], value=None, rows=8,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "right_hue = widgets.Select(\n",
        "    options=[], value=None, rows=8,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Buttons\n",
        "btn_add_x    = widgets.Button(description=\"Add to X \", button_style='primary', layout=widgets.Layout(width=BTN_W))\n",
        "btn_add_hue  = widgets.Button(description=\"Add to Hue \",button_style='primary', layout=widgets.Layout(width=BTN_W))\n",
        "btn_clear    = widgets.Button(description=\"Clear\", button_style='danger', layout=widgets.Layout(width=BTN_W))\n",
        "btn_up       = widgets.Button(description=\" Up (X only)\", layout=widgets.Layout(width=BTN_W))\n",
        "btn_down     = widgets.Button(description=\" Down (X only)\", layout=widgets.Layout(width=BTN_W))\n",
        "btn_build    = widgets.Button(description=\"Build Groups\", button_style='success', layout=widgets.Layout(width=\"160px\"))\n",
        "\n",
        "controls_col = widgets.VBox(\n",
        "    [btn_add_x, btn_add_hue, btn_clear, btn_up, btn_down],\n",
        "    layout=widgets.Layout(\n",
        "        align_items=\"center\",\n",
        "        width=BTN_W, min_width=BTN_W, max_width=BTN_W,\n",
        "        height=PX_H, min_height=PX_H, max_height=PX_H,\n",
        "        flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "btn_build = widgets.Button(description=\"Build Groups\", button_style='success', layout=widgets.Layout(width=\"160px\"))\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- Callbacks ---\n",
        "def on_add_x(_):\n",
        "    sel = list(available.value)\n",
        "    if not sel: return\n",
        "    new_opts = _unique_keep_order(list(right_x.options) + sel)\n",
        "    right_x.value = None\n",
        "    right_x.options = new_opts\n",
        "    right_x.value = new_opts[-1] if new_opts else None\n",
        "\n",
        "def on_add_hue(_):\n",
        "    sel = list(available.value)\n",
        "    if not sel: return\n",
        "    new_opts = _unique_keep_order(list(right_hue.options) + sel)\n",
        "    right_hue.value = None\n",
        "    right_hue.options = new_opts\n",
        "    right_hue.value = new_opts[-1] if new_opts else None\n",
        "\n",
        "def on_clear(_):\n",
        "    right_x.value = None; right_x.options = []\n",
        "    right_hue.value = None; right_hue.options = []\n",
        "\n",
        "def on_up(_):\n",
        "    item = right_x.value\n",
        "    if item is None: return\n",
        "    opts = list(right_x.options)\n",
        "    i = opts.index(item)\n",
        "    if i > 0:\n",
        "        opts[i-1], opts[i] = opts[i], opts[i-1]\n",
        "        right_x.value = None; right_x.options = opts; right_x.value = item\n",
        "\n",
        "def on_down(_):\n",
        "    item = right_x.value\n",
        "    if item is None: return\n",
        "    opts = list(right_x.options)\n",
        "    i = opts.index(item)\n",
        "    if i < len(opts) - 1:\n",
        "        opts[i+1], opts[i] = opts[i], opts[i+1]\n",
        "        right_x.value = None; right_x.options = opts; right_x.value = item\n",
        "\n",
        "def on_build(_):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        ordered_cols_x = list(right_x.options)\n",
        "        ordered_cols_hue = list(right_hue.options)\n",
        "\n",
        "        mapping_x = build_mapping(ordered_cols_x)\n",
        "        mapping_hue = build_mapping(ordered_cols_hue)\n",
        "\n",
        "        _meta = metadata_df.copy()\n",
        "        _meta[\"File\"] = _build_file_column(_meta)\n",
        "        _meta[\"XGroup\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols_x), axis=1)\n",
        "        _meta[\"HueGroup\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols_hue), axis=1)\n",
        "        mapping_both = (\n",
        "            _meta[[\"File\", \"XGroup\", \"HueGroup\"]]\n",
        "            .dropna(subset=[\"File\"])\n",
        "            .drop_duplicates()\n",
        "            .sort_values([\"XGroup\", \"HueGroup\", \"File\"])\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        globals()['files_to_group_x'] = mapping_x.copy()\n",
        "        globals()['files_to_group_hue'] = mapping_hue.copy()\n",
        "        globals()['files_to_group_both'] = mapping_both.copy()\n",
        "        globals()['selected_group_cols_x'] = ordered_cols_x.copy()\n",
        "        globals()['selected_group_cols_hue'] = ordered_cols_hue.copy()\n",
        "\n",
        "        print(\"X-axis grouping (hierarchy):\", ordered_cols_x if ordered_cols_x else [\"ALL\"])\n",
        "        print(f\"Total unique files (X map): {mapping_x['File'].nunique()}\")\n",
        "        display(widgets.HTML(\"<b>X-group summary</b>\"))\n",
        "        display((mapping_x.groupby(\"Group\", dropna=False)[\"File\"]\n",
        "                 .nunique().sort_values(ascending=False)\n",
        "                 .rename(\"UniqueFiles\").to_frame()))\n",
        "\n",
        "        print(\"\\nHue grouping:\", ordered_cols_hue if ordered_cols_hue else [\"ALL\"])\n",
        "        print(f\"Total unique files (Hue map): {mapping_hue['File'].nunique()}\")\n",
        "        display(widgets.HTML(\"<b>Hue-group summary</b>\"))\n",
        "        display((mapping_hue.groupby(\"Group\", dropna=False)[\"File\"]\n",
        "                 .nunique().sort_values(ascending=False)\n",
        "                 .rename(\"UniqueFiles\").to_frame()))\n",
        "        print(\"\\nCombined mapping available as `files_to_group_both` (File, XGroup, HueGroup)\")\n",
        "\n",
        "# Wire up\n",
        "btn_add_x.on_click(on_add_x)\n",
        "btn_add_hue.on_click(on_add_hue)\n",
        "btn_clear.on_click(on_clear)\n",
        "btn_up.on_click(on_up)\n",
        "btn_down.on_click(on_down)\n",
        "btn_build.on_click(on_build)\n",
        "\n",
        "# ----- Grid layout -----\n",
        "grid = widgets.GridBox(\n",
        "    children=[\n",
        "        available_hdr, actions_hdr, x_hdr, hue_hdr,     # row 1: headers\n",
        "        available,     controls_col, right_x, right_hue # row 2: widgets\n",
        "    ],\n",
        "    layout=widgets.Layout(\n",
        "        grid_template_columns=f\"{PX_W} {BTN_W} {PX_W} {PX_W}\",\n",
        "        grid_template_rows=\"auto auto\",\n",
        "        grid_gap=\"6px 16px\",\n",
        "        align_items=\"flex-start\",\n",
        "        justify_items=\"flex-start\",\n",
        "        width=\"100%\"\n",
        "    )\n",
        ")\n",
        "\n",
        "ui = widgets.VBox([title, grid, widgets.HBox([btn_build]), output])\n",
        "display(ui)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKKkCThnHE0H"
      },
      "outputs": [],
      "source": [
        "# @title Plot metrics\n",
        "\n",
        "import os, time, shutil, re, itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "import pingouin as pg\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "except Exception:\n",
        "    colab_files = None\n",
        "\n",
        "ALPHA = 0.6\n",
        "\n",
        "# -----------------------\n",
        "# 0) Preconditions & source\n",
        "# -----------------------\n",
        "if 'FR1metrics_merged' in globals() and FR1metrics_merged is not None and not FR1metrics_merged.empty:\n",
        "    bm = FR1metrics_merged.copy()\n",
        "elif 'FR1metrics' in globals() and FR1metrics is not None and not FR1metrics.empty:\n",
        "    bm = FR1metrics.copy()\n",
        "elif 'FR1metrics_csv' in globals() and FR1metrics_csv is not None and not FR1metrics_csv.empty:\n",
        "    bm = FR1metrics_csv.copy()\n",
        "else:\n",
        "    raise RuntimeError(\"No FR1 metrics DataFrame found: expected one of FR1metrics_merged, FR1metrics, FR1metrics_csv.\")\n",
        "\n",
        "if \"filename\" not in bm.columns:\n",
        "    if \"File\" in bm.columns:\n",
        "        bm[\"filename\"] = bm[\"File\"].astype(str)\n",
        "    else:\n",
        "        raise RuntimeError(\"Metrics table must include a 'filename' column (or legacy 'File').\")\n",
        "\n",
        "# -----------------------\n",
        "# Merge in XGroup/HueGroup from grouping widget\n",
        "# -----------------------\n",
        "def _basename_col(s):\n",
        "    return os.path.basename(str(s))\n",
        "\n",
        "def _src_name(df):\n",
        "    if \"filename\" in df.columns: return \"filename\"\n",
        "    if \"File\" in df.columns: return \"File\"\n",
        "    return None\n",
        "\n",
        "if (\"XGroup\" not in bm.columns) or (\"HueGroup\" not in bm.columns):\n",
        "    grp_both = globals().get('files_to_group_both', None)\n",
        "    if grp_both is not None and not grp_both.empty:\n",
        "        m = grp_both.copy()\n",
        "        m_src = _src_name(m)\n",
        "        if m_src is None:\n",
        "            raise RuntimeError(\"Grouping table must include 'filename' or 'File'.\")\n",
        "        m[\"file_base\"]  = m[m_src].astype(str).apply(_basename_col)\n",
        "        bm[\"file_base\"] = bm[\"filename\"].astype(str).apply(_basename_col)\n",
        "        bm = bm.merge(m[[\"file_base\",\"XGroup\",\"HueGroup\"]], on=\"file_base\", how=\"left\").drop(columns=[\"file_base\"])\n",
        "        bm[\"XGroup\"]   = bm[\"XGroup\"].fillna(\"UNASSIGNED\")\n",
        "        bm[\"HueGroup\"] = bm[\"HueGroup\"].fillna(\"UNASSIGNED\")\n",
        "    else:\n",
        "        raise RuntimeError(\"Missing X/Hue mapping. Build Groups first (two-column version).\")\n",
        "\n",
        "# -----------------------\n",
        "# 1) Melt to long format\n",
        "# -----------------------\n",
        "base_metric_names = [\n",
        "    \"Pellets\", \"Left_Poke\", \"Right_Poke\", \"Total_Pokes\", \"Accuracy\",\n",
        "    \"PokesPerPellet\", \"RetrievalTime\", \"InterPelletInterval\", \"PokeTime\",\n",
        "    \"%MealPellets\", \"%GrazingPellets\", \"NumMeals\", \"AvgMealSize\",\n",
        "    \"AvgMealDuration\", \"RecordingHours\", \"MealsPerHour\",\n",
        "    \"Daily_Pellets\", \"Left Poke with Pellet\", \"Within_meal_pellet_rate\",\n",
        "]\n",
        "\n",
        "metric_cols, seen = [], set()\n",
        "for c in bm.columns:\n",
        "    if not pd.api.types.is_numeric_dtype(bm[c]):\n",
        "        continue\n",
        "    for base in base_metric_names:\n",
        "        if c == base or c.startswith(base + \"_\"):\n",
        "            if c not in seen:\n",
        "                metric_cols.append(c); seen.add(c)\n",
        "            break\n",
        "\n",
        "if not metric_cols:\n",
        "    raise RuntimeError(\"No numeric metric columns found among expected FR1 metrics.\")\n",
        "\n",
        "candidate_id_vars = [\"Genotype\",\"Sex\",\"Strain\",\"Start_Date\",\"filename\",\"Mouse_ID\",\"Session_type\",\"XGroup\",\"HueGroup\"]\n",
        "id_vars = [c for c in candidate_id_vars if c in bm.columns]\n",
        "for need in [\"XGroup\",\"HueGroup\",\"filename\"]:\n",
        "    if need not in id_vars:\n",
        "        id_vars.append(need)\n",
        "\n",
        "long_df = pd.melt(\n",
        "    bm, id_vars=id_vars, value_vars=metric_cols,\n",
        "    var_name=\"variable\", value_name=\"value\"\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 2) Hierarchical ordering + display formatting (no '|' shown)\n",
        "# -----------------------\n",
        "def _is_unassigned_token(s):\n",
        "    return (str(s).strip().upper() in {\"\", \"UNASSIGNED\", \"NONE\", \"NA\", \"N/A\"})\n",
        "\n",
        "def _split_levels(s):\n",
        "    # keep hierarchical sort, but never display separators\n",
        "    s = str(s)\n",
        "    parts = [p.strip() for p in re.split(r\"\\s*\\|\\s*|\\s*[]\\s*\", s) if p.strip() != \"\"]\n",
        "    wanted = globals().get(\"selected_group_cols_x\", None)\n",
        "    if isinstance(wanted, (list, tuple)) and wanted:\n",
        "        if len(parts) < len(wanted):\n",
        "            parts += [\"\"] * (len(wanted) - len(parts))\n",
        "        else:\n",
        "            parts = parts[:len(wanted)]\n",
        "    return parts\n",
        "\n",
        "def _display_group_label(s):\n",
        "    parts = _split_levels(s)\n",
        "    return str(s) if len(parts) <= 1 else \" \".join(parts)\n",
        "\n",
        "def _is_wt_group(g):\n",
        "    u = str(g).strip().upper()\n",
        "    tokens = [t for t in re.split(r'[^A-Z0-9]+', u) if t]\n",
        "    WT_ALIASES = {\"WT\", \"WILDTYPE\", \"CONTROL\", \"CTRL\"}\n",
        "    return any(t in WT_ALIASES for t in tokens)\n",
        "\n",
        "def _hier_sort_key(g):\n",
        "    lv = _split_levels(g)\n",
        "    norm = []\n",
        "    for tok in lv:\n",
        "        is_blank = 1 if _is_unassigned_token(tok) else 0\n",
        "        norm.append((is_blank, str(tok).upper()))\n",
        "    wt_present = any(_is_wt_group(tok) for tok in lv) or _is_wt_group(g)\n",
        "    wt_rank = 0 if wt_present else 1\n",
        "    return (wt_rank,) + tuple(norm) + (str(g).upper(),)\n",
        "\n",
        "def _order_x_groups(groups):\n",
        "    return sorted(groups, key=_hier_sort_key)\n",
        "\n",
        "def _choose_ref_group(order):\n",
        "    for g in order:\n",
        "        if _is_wt_group(g):\n",
        "            return g\n",
        "    return order[0] if order else None\n",
        "\n",
        "all_x_raw = [g for g in long_df[\"XGroup\"].dropna().unique().tolist() if g != \"UNASSIGNED\"] or [\"UNASSIGNED\"]\n",
        "ordered_x_raw = _order_x_groups(all_x_raw)\n",
        "\n",
        "raw_to_disp = {g: _display_group_label(g) for g in ordered_x_raw}\n",
        "disp_to_raw = {}\n",
        "for g in ordered_x_raw:\n",
        "    d = raw_to_disp[g]\n",
        "    if d in disp_to_raw:\n",
        "        d = f\"{d} [{g}]\"\n",
        "        raw_to_disp[g] = d\n",
        "    disp_to_raw[d] = g\n",
        "\n",
        "# -----------------------\n",
        "# 3) Controls (left column: groups & colors)\n",
        "# -----------------------\n",
        "named_defaults = [\"blue\",\"orange\",\"green\",\"red\",\"purple\",\"brown\",\"pink\",\"gray\",\"olive\",\"cyan\"]\n",
        "\n",
        "x_checks, x_colors = {}, {}\n",
        "group_rows = []\n",
        "for i, g_raw in enumerate(ordered_x_raw):\n",
        "    chk = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description=raw_to_disp[g_raw],\n",
        "        indent=False,\n",
        "        layout=widgets.Layout(width=\"260px\")\n",
        "    )\n",
        "    col = widgets.Text(value=named_defaults[i % len(named_defaults)],\n",
        "                       layout=widgets.Layout(width=\"120px\"))\n",
        "    x_checks[g_raw] = chk\n",
        "    x_colors[g_raw] = col\n",
        "    group_rows.append(widgets.HBox([chk, widgets.Label(\"\"), col],\n",
        "                                   layout=widgets.Layout(align_items=\"center\", height=\"28px\")))\n",
        "\n",
        "picker = widgets.VBox(group_rows, layout=widgets.Layout(gap=\"2px\"))\n",
        "\n",
        "btn_all  = widgets.Button(description=\"Select all\", layout=widgets.Layout(width=\"140px\"))\n",
        "btn_none = widgets.Button(description=\"Clear\", layout=widgets.Layout(width=\"140px\"))\n",
        "\n",
        "def _set_all(val):\n",
        "    for c in x_checks.values():\n",
        "        c.value = val\n",
        "\n",
        "btn_all.on_click(lambda _: _set_all(True))\n",
        "btn_none.on_click(lambda _: _set_all(False))\n",
        "\n",
        "picker_container = widgets.Box(\n",
        "    [picker],\n",
        "    layout=widgets.Layout(overflow=\"auto\", max_height=\"420px\",\n",
        "                          border=\"1px solid #ddd\", padding=\"6px\", width=\"360px\")\n",
        ")\n",
        "\n",
        "left_col = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Groups & Colors</b>\"),\n",
        "    widgets.HBox([btn_all, btn_none], layout=widgets.Layout(gap=\"8px\")),\n",
        "    picker_container\n",
        "], layout=widgets.Layout(width=\"380px\"))\n",
        "\n",
        "# -----------------------\n",
        "# 4) Comparison controls (right column)\n",
        "# -----------------------\n",
        "mode_radio = widgets.ToggleButtons(\n",
        "    options=[(\"Reference group\", \"ref\"), (\"Select Pairs\", \"pairs\")],\n",
        "    value=\"ref\",\n",
        "    layout=widgets.Layout(width=\"320px\")\n",
        ")\n",
        "\n",
        "ref_dropdown = widgets.Dropdown(\n",
        "    options=ordered_x_raw,\n",
        "    value=_choose_ref_group(ordered_x_raw),\n",
        "    description=\"Reference:\",\n",
        "    layout=widgets.Layout(width=\"320px\")\n",
        ")\n",
        "\n",
        "pairs_select = widgets.SelectMultiple(\n",
        "    options=[],\n",
        "    value=[],\n",
        "    description=\"Pairs\",\n",
        "    layout=widgets.Layout(width=\"360px\", height=\"320px\")\n",
        ")\n",
        "\n",
        "def _selected_x_raw():\n",
        "    return _order_x_groups([g for g, cb in x_checks.items() if cb.value])\n",
        "\n",
        "def _pair_value(a, b):\n",
        "    return (a, b) if a <= b else (b, a)\n",
        "\n",
        "def _pair_sort_key(a, b):\n",
        "    A = _split_levels(a); B = _split_levels(b)\n",
        "    L = max(len(A), len(B))\n",
        "    if len(A) < L: A += [\"\"] * (L - len(A))\n",
        "    if len(B) < L: B += [\"\"] * (L - len(B))\n",
        "    first_diff = next((i for i, (xa, xb) in enumerate(zip(A, B)) if xa != xb), L)\n",
        "    prefix = tuple(A[:first_diff])\n",
        "    return (-first_diff, prefix, tuple(A), tuple(B))\n",
        "\n",
        "def _update_ref_and_pairs(*_):\n",
        "    sel = _selected_x_raw()\n",
        "    ref_dropdown.options = sel or [\"\"]\n",
        "    if sel:\n",
        "        if ref_dropdown.value not in sel:\n",
        "            ref_dropdown.value = _choose_ref_group(sel)\n",
        "    else:\n",
        "        ref_dropdown.value = None\n",
        "\n",
        "    opts = []\n",
        "    for a, b in itertools.combinations(sel, 2):\n",
        "        lbl = f\"{raw_to_disp.get(a, str(a))}  {raw_to_disp.get(b, str(b))}\"\n",
        "        opts.append((lbl, _pair_value(a, b)))\n",
        "    opts.sort(key=lambda kv: _pair_sort_key(*kv[1]))\n",
        "    pairs_select.options = opts\n",
        "\n",
        "for cb in x_checks.values():\n",
        "    cb.observe(_update_ref_and_pairs, names=\"value\")\n",
        "_update_ref_and_pairs()\n",
        "\n",
        "plot_btn = widgets.Button(description=\"Plot\", button_style=\"primary\",\n",
        "                          layout=widgets.Layout(width=\"160px\"))\n",
        "save_btn = widgets.Button(description=\"Save Plots\", button_style=\"success\",\n",
        "                          layout=widgets.Layout(width=\"160px\"))\n",
        "\n",
        "right_col = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Statistical comparisons</b>\"),\n",
        "    mode_radio,\n",
        "    ref_dropdown,\n",
        "    pairs_select,\n",
        "    widgets.HBox([plot_btn, save_btn], layout=widgets.Layout(gap=\"8px\"))\n",
        "], layout=widgets.Layout(width=\"360px\"))\n",
        "\n",
        "# -----------------------\n",
        "# 5) Clean factor labels (use what was selected)\n",
        "# -----------------------\n",
        "def _grouping_label(which=\"X\"):\n",
        "    if which.lower().startswith(\"x\"):\n",
        "        cols = globals().get(\"selected_group_cols_x\", [])\n",
        "        default = \"Group\"\n",
        "    else:\n",
        "        cols = globals().get(\"selected_group_cols_hue\", [])\n",
        "        default = \"Hue\"\n",
        "    cols = [str(c).strip() for c in (cols or []) if str(c).strip()]\n",
        "    return \" \".join(cols) if cols else default\n",
        "\n",
        "# -----------------------\n",
        "# 6) Stats helpers\n",
        "# -----------------------\n",
        "def _fmt_p_text(p):\n",
        "    if p is None or (isinstance(p, float) and (not np.isfinite(p))):\n",
        "        return \"p = n/a\"\n",
        "    p = float(p)\n",
        "    return f\"p = {p:.3f}\" if p >= 0.001 else \"p < 0.001\"\n",
        "\n",
        "def _fmt_p_num(p):\n",
        "    if p is None or (isinstance(p, float) and (not np.isfinite(p))):\n",
        "        return \"n/a\"\n",
        "    p = float(p)\n",
        "    return f\"{p:.4f}\" if p >= 0.0001 else \"<0.0001\"\n",
        "\n",
        "def _p_to_stars(p):\n",
        "    if p is None or (isinstance(p, float) and (not np.isfinite(p))): return \"\"\n",
        "    p = float(p)\n",
        "    if p < 1e-4: return \"****\"\n",
        "    if p < 1e-3: return \"***\"\n",
        "    if p < 1e-2: return \"**\"\n",
        "    if p < 5e-2: return \"*\"\n",
        "    return \"\"\n",
        "\n",
        "def _fmt_F(df_num, df_den, F):\n",
        "    if any(x is None for x in [df_num, df_den, F]): return \"n/a\"\n",
        "    if not np.isfinite(F): return \"n/a\"\n",
        "    return f\"F({int(df_num)}, {int(df_den)}) = {float(F):.3f}\"\n",
        "\n",
        "def _anova_subset(df):\n",
        "    out = {\"p_x\": np.nan, \"n_h\": 0, \"ok\": False}\n",
        "    d = df.dropna(subset=[\"value\",\"XGroup\"]).copy()\n",
        "    if d.empty or d[\"XGroup\"].nunique() < 2:\n",
        "        return out\n",
        "    n_h = d[\"HueGroup\"].nunique(dropna=True)\n",
        "    out[\"n_h\"] = int(n_h)\n",
        "    try:\n",
        "        if n_h >= 2:\n",
        "            model = ols(\"value ~ C(XGroup) + C(HueGroup) + C(XGroup):C(HueGroup)\", data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            out[\"p_x\"] = float(an.loc[\"C(XGroup)\", \"PR(>F)\"])\n",
        "        else:\n",
        "            model = ols(\"value ~ C(XGroup)\", data=d).fit()\n",
        "            out[\"p_x\"] = float(model.f_pvalue)\n",
        "        out[\"ok\"] = True\n",
        "    except Exception:\n",
        "        pass\n",
        "    return out\n",
        "\n",
        "def _oneway_anova_stats(df):\n",
        "    d = df.dropna(subset=[\"value\",\"XGroup\"]).copy()\n",
        "    if d[\"XGroup\"].nunique() < 2:\n",
        "        return {\"ok\": False, \"err\": \"Too few groups\"}\n",
        "    try:\n",
        "        model = ols(\"value ~ C(XGroup)\", data=d).fit()\n",
        "        an = sm.stats.anova_lm(model, typ=2)\n",
        "        return {\n",
        "            \"ok\": True,\n",
        "            \"test\": \"One-way ANOVA\",\n",
        "            \"F_x\": float(an.loc[\"C(XGroup)\", \"F\"]),\n",
        "            \"df_x_num\": int(an.loc[\"C(XGroup)\", \"df\"]),\n",
        "            \"df_x_den\": int(model.df_resid),\n",
        "            \"p_x\": float(an.loc[\"C(XGroup)\", \"PR(>F)\"]),\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"ok\": False, \"err\": str(e)}\n",
        "\n",
        "def _twoway_anova_stats(df):\n",
        "    d = df.dropna(subset=[\"value\",\"XGroup\",\"HueGroup\"]).copy()\n",
        "    if d[\"XGroup\"].nunique() < 2:\n",
        "        return {\"ok\": False, \"err\": \"Too few X groups\"}\n",
        "\n",
        "    if d[\"HueGroup\"].nunique(dropna=True) < 2:\n",
        "        return _oneway_anova_stats(d)\n",
        "\n",
        "    try:\n",
        "        model = ols(\"value ~ C(XGroup) + C(HueGroup) + C(XGroup):C(HueGroup)\", data=d).fit()\n",
        "        an = sm.stats.anova_lm(model, typ=2)\n",
        "        df_den = int(model.df_resid)\n",
        "        return {\n",
        "            \"ok\": True,\n",
        "            \"test\": \"Two-way ANOVA\",\n",
        "\n",
        "            \"F_x\": float(an.loc[\"C(XGroup)\", \"F\"]),\n",
        "            \"df_x_num\": int(an.loc[\"C(XGroup)\", \"df\"]),\n",
        "            \"df_x_den\": df_den,\n",
        "            \"p_x\": float(an.loc[\"C(XGroup)\", \"PR(>F)\"]),\n",
        "\n",
        "            \"F_h\": float(an.loc[\"C(HueGroup)\", \"F\"]),\n",
        "            \"df_h_num\": int(an.loc[\"C(HueGroup)\", \"df\"]),\n",
        "            \"df_h_den\": df_den,\n",
        "            \"p_h\": float(an.loc[\"C(HueGroup)\", \"PR(>F)\"]),\n",
        "\n",
        "            \"F_int\": float(an.loc[\"C(XGroup):C(HueGroup)\", \"F\"]),\n",
        "            \"df_int_num\": int(an.loc[\"C(XGroup):C(HueGroup)\", \"df\"]),\n",
        "            \"df_int_den\": df_den,\n",
        "            \"p_int\": float(an.loc[\"C(XGroup):C(HueGroup)\", \"PR(>F)\"]),\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"ok\": False, \"err\": str(e)}\n",
        "\n",
        "def build_group_header_df(long_df, sel_x_raw, test_label, raw_to_disp=None):\n",
        "    \"\"\"\n",
        "    Build a compact header block for Excel.\n",
        "\n",
        "    Example output (one cell per row in first column):\n",
        "      Test: Two-way ANOVA (Note: Results are not corrected for multiple comparisons)\n",
        "      Factors:\n",
        "      Genotype (X)\n",
        "      Sex (Hue)\n",
        "      Groups:\n",
        "      WT 20 (10F, 10M)\n",
        "      HET 20 (10F, 10M)\n",
        "    \"\"\"\n",
        "    if raw_to_disp is None:\n",
        "        raw_to_disp = {}\n",
        "\n",
        "    x_label = _grouping_label(\"X\")      # e.g., \"Genotype\"\n",
        "    h_label = _grouping_label(\"Hue\")    # e.g., \"Sex\"\n",
        "\n",
        "    rows = []\n",
        "    rows.append([f\"Test: {test_label} (Note: Results are not corrected for multiple comparisons)\", \"\"])\n",
        "    rows.append([\"Factors:\", \"\"])\n",
        "    rows.append([f\"{x_label} (X)\", \"\"])\n",
        "    rows.append([f\"{h_label} (Hue)\", \"\"])\n",
        "    rows.append([\"Groups:\", \"\"])\n",
        "\n",
        "    # Count UNIQUE files only (one entry per animal/file within group combo)\n",
        "    dfu = long_df.drop_duplicates(subset=[\"filename\", \"XGroup\", \"HueGroup\"])\n",
        "\n",
        "    for g in _order_x_groups(sel_x_raw):\n",
        "        sub = dfu[dfu[\"XGroup\"] == g]\n",
        "        total_n = sub[\"filename\"].nunique()\n",
        "\n",
        "        hue_counts = (\n",
        "            sub.groupby(\"HueGroup\")[\"filename\"]\n",
        "               .nunique()\n",
        "               .to_dict()\n",
        "        )\n",
        "\n",
        "\n",
        "        hues = list(hue_counts.keys())\n",
        "        hue_str = \", \".join([f\"{hue_counts[h]}{h}\" for h in hues])\n",
        "        g_disp = raw_to_disp.get(g, g)\n",
        "        rows.append([f\"{g_disp} n = {total_n} ({hue_str})\", \"\"])\n",
        "\n",
        "    return pd.DataFrame(rows, columns=[\"\", \"\"])\n",
        "\n",
        "\n",
        "def build_stats_table(long_df, metrics, sel_x_raw):\n",
        "    x_label = _grouping_label(\"X\")\n",
        "    h_label = _grouping_label(\"Hue\")\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for metric in metrics:\n",
        "        dfm = long_df[\n",
        "            (long_df[\"variable\"] == metric) &\n",
        "            (long_df[\"XGroup\"].isin(sel_x_raw))\n",
        "        ].copy()\n",
        "\n",
        "        dfm = dfm.dropna(subset=[\"value\"])\n",
        "        if dfm.empty:\n",
        "            continue\n",
        "\n",
        "        stats = _twoway_anova_stats(dfm)\n",
        "\n",
        "        # Error case\n",
        "        if not stats.get(\"ok\", False):\n",
        "            rows.append({\n",
        "                \"Figure\": metric,\n",
        "                f\"Effect of {x_label}\": stats.get(\"err\", \"unknown error\"),\n",
        "                f\"Effect of {h_label}\": \"\",\n",
        "                f\"{x_label}  {h_label} interaction\": \"\",\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Two-way ANOVA\n",
        "        if stats[\"test\"] == \"Two-way ANOVA\":\n",
        "            x_eff = (\n",
        "                f\"{_fmt_F(stats['df_x_num'], stats['df_x_den'], stats['F_x'])}; \"\n",
        "                f\"p={_fmt_p_num(stats['p_x'])} {_p_to_stars(stats['p_x'])}\"\n",
        "            ).strip()\n",
        "\n",
        "            h_eff = (\n",
        "                f\"{_fmt_F(stats['df_h_num'], stats['df_h_den'], stats['F_h'])}; \"\n",
        "                f\"p={_fmt_p_num(stats['p_h'])} {_p_to_stars(stats['p_h'])}\"\n",
        "            ).strip()\n",
        "\n",
        "            i_eff = (\n",
        "                f\"{_fmt_F(stats['df_int_num'], stats['df_int_den'], stats['F_int'])}; \"\n",
        "                f\"p={_fmt_p_num(stats['p_int'])} {_p_to_stars(stats['p_int'])}\"\n",
        "            ).strip()\n",
        "\n",
        "        # One-way fallback (Hue < 2 levels)\n",
        "        else:\n",
        "            x_eff = (\n",
        "                f\"{_fmt_F(stats['df_x_num'], stats['df_x_den'], stats['F_x'])}; \"\n",
        "                f\"p={_fmt_p_num(stats['p_x'])} {_p_to_stars(stats['p_x'])}\"\n",
        "            ).strip()\n",
        "            h_eff = \"\"\n",
        "            i_eff = \"\"\n",
        "\n",
        "        rows.append({\n",
        "            \"Figure\": metric,\n",
        "            f\"Effect of {x_label}\": x_eff,\n",
        "            f\"Effect of {h_label}\": h_eff,\n",
        "            f\"{x_label}  {h_label} interaction\": i_eff,\n",
        "        })\n",
        "\n",
        "    out = pd.DataFrame(rows)\n",
        "\n",
        "    # Consistent column order\n",
        "    col_order = [\n",
        "        \"Figure\",\n",
        "        f\"Effect of {x_label}\",\n",
        "        f\"Effect of {h_label}\",\n",
        "        f\"{x_label}  {h_label} interaction\",\n",
        "    ]\n",
        "\n",
        "    return out[[c for c in col_order if c in out.columns]]\n",
        "\n",
        "def _stats_text(dfm, x_label, hue_label, *, mode=\"ref\", ref_group=None, pair_list=None):\n",
        "    df = dfm.dropna(subset=[\"value\", \"XGroup\"]).copy()\n",
        "    if df.empty or df[\"XGroup\"].nunique() < 2:\n",
        "        return \"Too few groups for stats\"\n",
        "\n",
        "    groups = _order_x_groups(df[\"XGroup\"].dropna().unique().tolist())\n",
        "\n",
        "    stats = _twoway_anova_stats(df)\n",
        "    if not stats.get(\"ok\", False):\n",
        "        return \"\\n\\nANOVA failed:\\n\" + str(stats.get(\"err\", \"unknown error\"))\n",
        "\n",
        "    lines = [stats[\"test\"]]\n",
        "    if stats[\"test\"] == \"Two-way ANOVA\":\n",
        "        lines.append(f\"{x_label}: {_fmt_F(stats['df_x_num'], stats['df_x_den'], stats['F_x'])}; {_fmt_p_text(stats['p_x'])}\")\n",
        "        lines.append(f\"{hue_label}: {_fmt_F(stats['df_h_num'], stats['df_h_den'], stats['F_h'])}; {_fmt_p_text(stats['p_h'])}\")\n",
        "        lines.append(f\"{x_label}  {hue_label}: {_fmt_F(stats['df_int_num'], stats['df_int_den'], stats['F_int'])}; {_fmt_p_text(stats['p_int'])}\")\n",
        "    else:\n",
        "        lines.append(f\"{x_label}: {_fmt_F(stats['df_x_num'], stats['df_x_den'], stats['F_x'])}; {_fmt_p_text(stats['p_x'])}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# -----------------------\n",
        "# 7) Plotting helpers\n",
        "# -----------------------\n",
        "def _dot_palette(hues):\n",
        "    hues = list(hues)\n",
        "    if len(hues) == 0: return {}\n",
        "    if len(hues) == 1: return {hues[0]: \"black\"}\n",
        "    if len(hues) == 2: return {hues[0]: \"white\", hues[1]: \"black\"}\n",
        "    defaults = plt.rcParams.get('axes.prop_cycle', None)\n",
        "    colors = defaults.by_key()['color'] if defaults else [\"C0\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\"]\n",
        "    return {h: colors[i % len(colors)] for i, h in enumerate(hues)}\n",
        "\n",
        "def _draw_bracket(ax, x1, x2, y, h, text):\n",
        "    ax.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=\"black\", zorder=5)\n",
        "    ax.text((x1+x2)/2, y+h, text, ha=\"center\", va=\"bottom\", fontsize=16, fontweight=\"bold\")\n",
        "\n",
        "def _plot_metric_clean(df_metric, variable, x_color_map, *, mode=\"ref\", ref_group=None, pair_list=None, return_fig=False):\n",
        "    dfm = df_metric.copy()\n",
        "    order_raw = _order_x_groups(dfm[\"XGroup\"].dropna().unique().tolist())\n",
        "    if not order_raw:\n",
        "        return None\n",
        "\n",
        "    if (not ref_group) or (ref_group not in order_raw):\n",
        "        ref_group = _choose_ref_group(order_raw)\n",
        "\n",
        "    x_label_name   = _grouping_label(\"X\")\n",
        "    hue_label_name = _grouping_label(\"Hue\")\n",
        "\n",
        "    hue_levels = [h for h in dfm[\"HueGroup\"].dropna().unique().tolist()]\n",
        "    pal_dots = _dot_palette(hue_levels)\n",
        "\n",
        "    width = max(2, 1 * len(order_raw))\n",
        "    height = 4.0\n",
        "    fig, (ax_plot, ax_text) = plt.subplots(\n",
        "        1, 2, figsize=(width/1.2, height), gridspec_kw={'width_ratios': [3, 1]}\n",
        "    )\n",
        "\n",
        "    bar_palette = [x_color_map.get(g, \"tab:blue\") for g in order_raw]\n",
        "    sns.barplot(data=dfm, x=\"XGroup\", y=\"value\", order=order_raw, ci=None, alpha=ALPHA, ax=ax_plot, palette=bar_palette)\n",
        "\n",
        "    sns.stripplot(\n",
        "        data=dfm, x=\"XGroup\", y=\"value\", order=order_raw, hue=\"HueGroup\",\n",
        "        jitter=True, dodge=False, size=7, edgecolor=\"black\", linewidth=1,\n",
        "        palette=pal_dots, ax=ax_plot, zorder=3, alpha=ALPHA\n",
        "    )\n",
        "    if ax_plot.legend_ is not None:\n",
        "        ax_plot.legend_.remove()\n",
        "\n",
        "    ax_plot.set_xticklabels([raw_to_disp.get(g, str(g)) for g in order_raw], rotation=45, ha=\"right\")\n",
        "\n",
        "    y_min, y_max = ax_plot.get_ylim()\n",
        "    span = (y_max - y_min) if y_max > y_min else 1.0\n",
        "    bump = 0.06 * span\n",
        "    data_max = dfm[\"value\"].max() if dfm[\"value\"].notna().any() else y_max\n",
        "\n",
        "    if mode == \"ref\" and (ref_group in order_raw):\n",
        "        ref_vals = dfm[dfm[\"XGroup\"] == ref_group][\"value\"].dropna().to_numpy()\n",
        "        for g in order_raw:\n",
        "            if g == ref_group:\n",
        "                continue\n",
        "            vals = dfm[dfm[\"XGroup\"] == g][\"value\"].dropna().to_numpy()\n",
        "            if len(vals) >= 2 and len(ref_vals) >= 2:\n",
        "                try:\n",
        "                    p = float(pg.ttest(vals, ref_vals, paired=False)[\"p-val\"].values[0])\n",
        "                except Exception:\n",
        "                    p = np.nan\n",
        "                if np.isfinite(p) and p < 0.05:\n",
        "                    xloc = order_raw.index(g)\n",
        "                    gmax = dfm[dfm[\"XGroup\"] == g][\"value\"].max()\n",
        "                    y_star = (gmax if np.isfinite(gmax) else data_max) + bump\n",
        "                    ax_plot.text(xloc, y_star, _p_to_stars(p),\n",
        "                                 ha=\"center\", va=\"bottom\", fontsize=16, fontweight=\"bold\")\n",
        "                    y_max = max(y_max, y_star + bump)\n",
        "        ax_plot.set_ylim(y_min, y_max)\n",
        "\n",
        "    elif mode == \"pairs\" and pair_list:\n",
        "        base = (dfm[\"value\"].max() if dfm[\"value\"].notna().any() else y_max) + bump\n",
        "        step = 0.12 * span\n",
        "        k = 0\n",
        "        for a, b in pair_list:\n",
        "            if (a not in order_raw) or (b not in order_raw):\n",
        "                continue\n",
        "            sub = dfm[dfm[\"XGroup\"].isin([a, b])].dropna(subset=[\"value\"])\n",
        "            if sub[\"XGroup\"].nunique() < 2:\n",
        "                continue\n",
        "            res = _anova_subset(sub)\n",
        "            if res[\"ok\"] and np.isfinite(res[\"p_x\"]) and (res[\"p_x\"] < 0.05):\n",
        "                x1 = order_raw.index(a); x2 = order_raw.index(b)\n",
        "                if x1 > x2: x1, x2 = x2, x1\n",
        "                y_here = base + k * step\n",
        "                _draw_bracket(ax_plot, x1, x2, y_here, 0.04 * span, _p_to_stars(res[\"p_x\"]))\n",
        "                y_max = max(y_max, y_here + 0.08 * span)\n",
        "                k += 1\n",
        "        ax_plot.set_ylim(y_min, y_max)\n",
        "\n",
        "    ax_plot.set_title(variable, fontsize=14)\n",
        "    ax_plot.set_xlabel(\"\")\n",
        "    ax_plot.set_ylabel(variable)\n",
        "    sns.despine(ax=ax_plot)\n",
        "\n",
        "    ax_text.axis(\"off\")\n",
        "    ax_text.text(\n",
        "        0, 1,\n",
        "        _stats_text(dfm, x_label_name, hue_label_name, mode=mode, ref_group=ref_group, pair_list=pair_list),\n",
        "        va=\"top\", ha=\"left\", fontsize=11, transform=ax_text.transAxes\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig if return_fig else plt.show()\n",
        "\n",
        "# -----------------------\n",
        "# 8) Actions\n",
        "# -----------------------\n",
        "out = widgets.Output()\n",
        "\n",
        "def _selected_x_and_colors():\n",
        "    sel = _selected_x_raw()\n",
        "    color_map = {}\n",
        "    for g in sel:\n",
        "        val = x_colors[g].value.strip()\n",
        "        color_map[g] = val if val else \"tab:blue\"\n",
        "    return sel, color_map\n",
        "\n",
        "def _current_pairs():\n",
        "    return list(pairs_select.value)\n",
        "\n",
        "def _get_metrics_list():\n",
        "    exclude = {\"PeakAccuracy_Day\",\"PeakAccuracy_Night\",\n",
        "               \"Win-stay_Day\",\"Win-stay_Night\",\n",
        "               \"Lose-shift_Day\",\"Lose-shift_Night\"}\n",
        "    return [m for m in long_df[\"variable\"].dropna().unique() if m not in exclude]\n",
        "\n",
        "def _run_plots(_=None):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        sel_x, color_map = _selected_x_and_colors()\n",
        "        if len(sel_x) < 1:\n",
        "            print(\"Select at least one group.\"); return\n",
        "\n",
        "        mode = mode_radio.value\n",
        "        if mode == \"ref\":\n",
        "            ref = ref_dropdown.value if (ref_dropdown.value in sel_x) else _choose_ref_group(sel_x)\n",
        "            print(f\"Groups: {[raw_to_disp.get(g, g) for g in sel_x]}    reference: {raw_to_disp.get(ref, ref)}\")\n",
        "        else:\n",
        "            pair_list = _current_pairs()\n",
        "            if not pair_list:\n",
        "                print(f\"Groups: {[raw_to_disp.get(g, g) for g in sel_x]}    no pairs selected.\"); return\n",
        "            pretty_pairs = [(raw_to_disp.get(a, a), raw_to_disp.get(b, b)) for a, b in pair_list]\n",
        "            print(f\"Groups: {[raw_to_disp.get(g, g) for g in sel_x]}    pairs: {pretty_pairs}\")\n",
        "\n",
        "        metrics = _get_metrics_list()\n",
        "        for metric in metrics:\n",
        "            subset = long_df[(long_df[\"variable\"] == metric) & (long_df[\"XGroup\"].isin(sel_x))]\n",
        "            if subset[\"value\"].dropna().empty:\n",
        "                continue\n",
        "            if mode == \"ref\":\n",
        "                _plot_metric_clean(\n",
        "                    subset, metric,\n",
        "                    x_color_map={g: color_map[g] for g in sel_x if g in subset[\"XGroup\"].unique()},\n",
        "                    mode=\"ref\", ref_group=ref\n",
        "                )\n",
        "            else:\n",
        "                _plot_metric_clean(\n",
        "                    subset, metric,\n",
        "                    x_color_map={g: color_map[g] for g in sel_x if g in subset[\"XGroup\"].unique()},\n",
        "                    mode=\"pairs\", pair_list=_current_pairs()\n",
        "                )\n",
        "\n",
        "def _save_plots(_=None):\n",
        "    with out:\n",
        "        clear_output()\n",
        "\n",
        "        sel_x, color_map = _selected_x_and_colors()\n",
        "        if len(sel_x) < 1:\n",
        "            print(\"Select at least one group.\"); return\n",
        "\n",
        "        mode = mode_radio.value\n",
        "        ref = ref_dropdown.value if (mode == \"ref\") else None\n",
        "        pair_list = _current_pairs() if (mode == \"pairs\") else None\n",
        "        if mode == \"pairs\" and not pair_list:\n",
        "            print(\"Select at least one pair before saving.\"); return\n",
        "\n",
        "        src_df = globals().get(\"FR1metrics_merged\", None)\n",
        "        if src_df is None or src_df.empty:\n",
        "            src_df = bm\n",
        "\n",
        "        example = src_df.iloc[0]\n",
        "        strain_name = str(example.get(\"Gene\", example.get(\"Strain\", \"FR1\"))).replace(\" \", \"_\")\n",
        "\n",
        "        strain_num_raw = example.get(\"Gene_ID\", example.get(\"Strain_ID\", \"Metrics\"))\n",
        "        try:\n",
        "            strain_num = f\"{int(strain_num_raw):03d}\"\n",
        "        except Exception:\n",
        "            strain_num = str(strain_num_raw).zfill(3)\n",
        "\n",
        "        task_name = str(example.get(\"Session_type\", \"Unknown\")).replace(\" \", \"_\")\n",
        "        out_dir = f\"{strain_name}_{strain_num}_{task_name}_Figures\"\n",
        "\n",
        "        if os.path.exists(out_dir):\n",
        "            shutil.rmtree(out_dir)\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "        metrics = _get_metrics_list()\n",
        "\n",
        "        stats_df  = build_stats_table(long_df, metrics, sel_x)\n",
        "        dfu = long_df[long_df[\"XGroup\"].isin(sel_x)].dropna(subset=[\"value\"])\n",
        "        test_label = \"Two-way ANOVA\" if dfu[\"HueGroup\"].nunique(dropna=True) >= 2 else \"One-way ANOVA\"\n",
        "\n",
        "        header_df = build_group_header_df(long_df, sel_x, test_label)\n",
        "\n",
        "        stats_path = f\"{out_dir}/FR1_stats_table.xlsx\"\n",
        "        with pd.ExcelWriter(stats_path, engine=\"openpyxl\") as writer:\n",
        "            header_df.to_excel(writer, index=False, header=False, sheet_name=\"Stats\")\n",
        "            stats_df.to_excel(writer, index=False, startrow=len(header_df) + 1, sheet_name=\"Stats\")\n",
        "\n",
        "        saved = 0\n",
        "        for metric in metrics:\n",
        "            subset = long_df[(long_df[\"variable\"] == metric) & (long_df[\"XGroup\"].isin(sel_x))]\n",
        "            if subset[\"value\"].dropna().empty:\n",
        "                continue\n",
        "\n",
        "            fig = _plot_metric_clean(\n",
        "                subset, metric,\n",
        "                x_color_map={g: color_map[g] for g in sel_x if g in subset[\"XGroup\"].unique()},\n",
        "                mode=mode, ref_group=ref, pair_list=pair_list, return_fig=True\n",
        "            )\n",
        "            safe = metric.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
        "            fig.savefig(f\"{out_dir}/{safe}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
        "            plt.close(fig)\n",
        "            saved += 1\n",
        "\n",
        "        if saved == 0:\n",
        "            print(\"No figures to save.\"); return\n",
        "\n",
        "        zipname = f\"{out_dir}_{int(time.time())}.zip\"\n",
        "        shutil.make_archive(zipname.replace(\".zip\", \"\"), \"zip\", out_dir)\n",
        "\n",
        "        if colab_files is not None:\n",
        "            colab_files.download(zipname)\n",
        "\n",
        "        print(f\"Saved {zipname}\")\n",
        "        print(f\"Included stats table: {stats_path}\")\n",
        "\n",
        "# avoid double-callbacks if you re-run the cell\n",
        "try:\n",
        "    plot_btn._click_handlers.callbacks = []\n",
        "    save_btn._click_handlers.callbacks = []\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "plot_btn.on_click(_run_plots)\n",
        "save_btn.on_click(_save_plots)\n",
        "\n",
        "# -----------------------\n",
        "# 9) Assemble UI\n",
        "# -----------------------\n",
        "def _toggle_controls(*_):\n",
        "    if mode_radio.value == \"ref\":\n",
        "        ref_dropdown.layout.display = \"\"\n",
        "        pairs_select.layout.display = \"none\"\n",
        "    else:\n",
        "        ref_dropdown.layout.display = \"none\"\n",
        "        pairs_select.layout.display = \"\"\n",
        "_toggle_controls()\n",
        "mode_radio.observe(lambda _: _toggle_controls(), names=\"value\")\n",
        "\n",
        "row = widgets.HBox(\n",
        "    [left_col, right_col],\n",
        "    layout=widgets.Layout(justify_content=\"flex-start\", align_items=\"flex-start\", gap=\"16px\", width=\"auto\")\n",
        ")\n",
        "ui = widgets.VBox(\n",
        "    [widgets.HTML(\"<h3 style='margin-bottom:6px'>FR1 Metric Comparisons</h3>\"), row, out],\n",
        "    layout=widgets.Layout(width=\"auto\")\n",
        ")\n",
        "\n",
        "display(ui)\n",
        "_run_plots()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Day/Night metrics\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# ---- Config ----\n",
        "# If you want to force a specific set, set FORCE_BASES to a list of base names (without _Day/_Night)\n",
        "FORCE_BASES = None  # e.g., [\"%MealPellets\",\"%GrazingPellets\",\"Pellets\",\"NumMeals\",\"AvgMealSize\",\"AvgMealDuration\",\"MealsPerHour\",\"Accuracy\"]\n",
        "DAY_TAG, NIGHT_TAG = \"_Day\", \"_Night\"\n",
        "BAR_WIDTH = 0.36\n",
        "NIGHT_ALPHA = 0.6\n",
        "DOT_SIZE = 7\n",
        "EDGE_LW_DAY = 2.0\n",
        "EDGE_LW_NIGHT = 1.0\n",
        "\n",
        "ALLOWED_DN_BASES = {\n",
        "    \"%MealPellets\",\"%GrazingPellets\",\"Pellets\",\"NumMeals\",\"AvgMealSize\",\n",
        "    \"AvgMealDuration\",\"MealsPerHour\",\"Accuracy\"\n",
        "}\n",
        "\n",
        "# ---- Helpers reused/compatible with previous cell ----\n",
        "def _safe_order(groups):\n",
        "    groups = [g for g in groups if pd.notna(g)]\n",
        "    if '_order_x_groups' in globals():\n",
        "        try:\n",
        "            return _order_x_groups(groups)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return sorted(groups, key=lambda s: str(s).upper())\n",
        "\n",
        "def _selected_x_groups():\n",
        "    if 'x_checks' in globals() and isinstance(x_checks, dict) and len(x_checks):\n",
        "        sel = [g for g, cb in x_checks.items() if getattr(cb, \"value\", False)]\n",
        "        return _safe_order(sel)\n",
        "    if 'long_df' not in globals():\n",
        "        raise RuntimeError(\"long_df not found\")\n",
        "    return _safe_order(long_df[\"XGroup\"].dropna().unique().tolist())\n",
        "\n",
        "def _color_map(x_groups):\n",
        "    if 'x_colors' in globals() and isinstance(x_colors, dict) and len(x_colors):\n",
        "        out = {}\n",
        "        for g in x_groups:\n",
        "            w = x_colors.get(g, None)\n",
        "            val = getattr(w, \"value\", None) if w is not None else None\n",
        "            out[g] = (val.strip() if isinstance(val, str) and val.strip() else \"blue\")\n",
        "        return out\n",
        "    defaults = [\"blue\",\"orange\",\"green\",\"red\",\"purple\",\"brown\",\"pink\",\"gray\",\"olive\",\"cyan\"]\n",
        "    return {g: defaults[i % len(defaults)] for i, g in enumerate(x_groups)}\n",
        "\n",
        "def _label_for(which=\"X\"):\n",
        "    if '_grouping_label' in globals():\n",
        "        return _grouping_label(which)\n",
        "    return \"XGroup\" if which.lower().startswith(\"x\") else \"HueGroup\"\n",
        "\n",
        "def _dot_palette_local(hues):\n",
        "    if '_dot_palette' in globals():\n",
        "        return _dot_palette(hues)\n",
        "    hues = list(hues)\n",
        "    if len(hues) == 0: return {}\n",
        "    if len(hues) == 1: return {hues[0]: \"black\"}\n",
        "    if len(hues) == 2: return {hues[0]: \"white\", hues[1]: \"black\"}\n",
        "    defaults = plt.rcParams.get('axes.prop_cycle', None)\n",
        "    colors = defaults.by_key()['color'] if defaults else [\"C0\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\"]\n",
        "    return {h: colors[i % len(colors)] for i, h in enumerate(hues)}\n",
        "\n",
        "# ---- Day/Night extraction helpers ----\n",
        "def _extract_dn_base(var_name):\n",
        "    \"\"\"\n",
        "    From a long_df 'variable' like 'Pellets_Day' or 'Pellets_Day_FR1', return 'Pellets'.\n",
        "    Returns None if it doesn't look like a Day/Night variable.\n",
        "    \"\"\"\n",
        "    if \"_Day\" in var_name:\n",
        "        return var_name.split(\"_Day\", 1)[0]\n",
        "    if \"_Night\" in var_name:\n",
        "        return var_name.split(\"_Night\", 1)[0]\n",
        "    return None\n",
        "\n",
        "def _ensure_daynight_view(df, base):\n",
        "    \"\"\"\n",
        "    Build a tidy DataFrame with columns: XGroup, HueGroup, DayNight ('Day'/'Night'), value\n",
        "    Works with variables that might include session suffixes (e.g., '<base>_Day_FR1').\n",
        "    \"\"\"\n",
        "    patt_day = re.compile(rf\"^{re.escape(base)}{re.escape(DAY_TAG)}(?:_.+)?$\")\n",
        "    patt_ngt = re.compile(rf\"^{re.escape(base)}{re.escape(NIGHT_TAG)}(?:_.+)?$\")\n",
        "    keep_mask = df[\"variable\"].astype(str).str.match(patt_day) | df[\"variable\"].astype(str).str.match(patt_ngt)\n",
        "    d = df.loc[keep_mask].copy()\n",
        "    if d.empty:\n",
        "        return d\n",
        "    d[\"DayNight\"] = np.where(d[\"variable\"].str.match(patt_day), \"Day\", \"Night\")\n",
        "    return d\n",
        "\n",
        "def _discover_daynight_bases(df):\n",
        "    vars_ = df[\"variable\"].astype(str).unique().tolist()\n",
        "    bases = sorted({b for v in vars_ for b in [_extract_dn_base(v)] if b})\n",
        "    # Keep only bases we actually want to plot (and that make sense for FR1)\n",
        "    bases = [b for b in bases if b in ALLOWED_DN_BASES]\n",
        "    return bases\n",
        "\n",
        "# ---- Stats text (ANOVA with Day/Night included) ----\n",
        "def _anova_daynight_text(df_dn, x_label, hue_label):\n",
        "    d = df_dn.dropna(subset=[\"value\",\"XGroup\",\"DayNight\"]).copy()\n",
        "    if d[\"XGroup\"].nunique() < 2 or d[\"DayNight\"].nunique() < 2:\n",
        "        return \"Too few groups or missing Day/Night to run ANOVA.\"\n",
        "\n",
        "    has_hue = d[\"HueGroup\"].nunique(dropna=True) >= 2\n",
        "    try:\n",
        "        if has_hue:\n",
        "            model = ols('value ~ C(XGroup) + C(DayNight) + C(HueGroup) + '\n",
        "                        'C(XGroup):C(DayNight) + C(XGroup):C(HueGroup) + '\n",
        "                        'C(DayNight):C(HueGroup) + C(XGroup):C(DayNight):C(HueGroup)',\n",
        "                        data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            def pget(term):\n",
        "                return float(an.loc[term, 'PR(>F)']) if term in an.index else np.nan\n",
        "            def fmt(p):\n",
        "                if not np.isfinite(p): return \"n/a\"\n",
        "                return f\"p = {p:.3f}\" if p >= 0.001 else \"p < 0.001\"\n",
        "            return (\n",
        "                \"Three-way ANOVA (XGroup, Day/Night, Hue)\\n\"\n",
        "                f\"{x_label}: {fmt(pget('C(XGroup)'))}\\n\"\n",
        "                f\"Day/Night: {fmt(pget('C(DayNight)'))}\\n\"\n",
        "                f\"{hue_label}: {fmt(pget('C(HueGroup)'))}\\n\"\n",
        "                f\"{x_label}Day/Night: {fmt(pget('C(XGroup):C(DayNight)'))}\\n\"\n",
        "                f\"{x_label}{hue_label}: {fmt(pget('C(XGroup):C(HueGroup)'))}\\n\"\n",
        "                f\"Day/Night{hue_label}: {fmt(pget('C(DayNight):C(HueGroup)'))}\\n\"\n",
        "                f\"{x_label}Day/Night{hue_label}: {fmt(pget('C(XGroup):C(DayNight):C(HueGroup)'))}\"\n",
        "            )\n",
        "        else:\n",
        "            model = ols('value ~ C(XGroup) + C(DayNight) + C(XGroup):C(DayNight)', data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            def fmt(p):\n",
        "                if not np.isfinite(p): return \"n/a\"\n",
        "                return f\"p = {p:.3f}\" if p >= 0.001 else \"p < 0.001\"\n",
        "            return (\n",
        "                \"Two-way ANOVA (XGroup, Day/Night)\\n\"\n",
        "                f\"{_label_for('X')}: {fmt(float(an.loc['C(XGroup)','PR(>F)']))}\\n\"\n",
        "                f\"Day/Night: {fmt(float(an.loc['C(DayNight)','PR(>F)']))}\\n\"\n",
        "                f\"{_label_for('X')}Day/Night: {fmt(float(an.loc['C(XGroup):C(DayNight)','PR(>F)']))}\"\n",
        "            )\n",
        "    except Exception as e:\n",
        "        return f\"ANOVA failed: {e}\"\n",
        "\n",
        "# ---- Main plotting for a single base ----\n",
        "def _plot_day_night_for_base(base, x_groups, color_map):\n",
        "    df_dn = _ensure_daynight_view(long_df, base)\n",
        "    df_dn = df_dn[df_dn[\"XGroup\"].isin(x_groups)].copy()\n",
        "    if df_dn.empty:\n",
        "        print(f\"Skipping {base}: no Day/Night data for selected groups.\")\n",
        "        return\n",
        "\n",
        "    x_label = _label_for(\"X\")\n",
        "    hue_label = _label_for(\"Hue\")\n",
        "\n",
        "    hue_levels = [h for h in df_dn[\"HueGroup\"].dropna().unique().tolist()]\n",
        "    pal_dots = _dot_palette_local(hue_levels)\n",
        "\n",
        "    means = (df_dn.dropna(subset=[\"value\"])\n",
        "                  .groupby([\"XGroup\",\"DayNight\"], as_index=False)[\"value\"]\n",
        "                  .mean().rename(columns={\"value\":\"mean\"}))\n",
        "    grid = (means.set_index([\"XGroup\",\"DayNight\"])[\"mean\"]\n",
        "                 .unstack(\"DayNight\")\n",
        "                 .reindex(index=x_groups, columns=[\"Day\",\"Night\"]))\n",
        "\n",
        "    width = max(4, 1.2 * len(x_groups))\n",
        "    fig, (ax, ax_txt) = plt.subplots(1, 2, figsize=(width, 4.6), gridspec_kw={'width_ratios': [3, 1]})\n",
        "\n",
        "    x = np.arange(len(x_groups))\n",
        "    off = BAR_WIDTH/2.0 + 0.02\n",
        "    pos_day = x - off\n",
        "    pos_night = x + off\n",
        "\n",
        "    # Night bars (filled)\n",
        "    for i, g in enumerate(x_groups):\n",
        "        val = grid.loc[g, \"Night\"] if (\"Night\" in grid.columns) else np.nan\n",
        "        if pd.notna(val):\n",
        "            ax.bar(pos_night[i], val, width=BAR_WIDTH, color=color_map[g],\n",
        "                   alpha=NIGHT_ALPHA, edgecolor=\"black\", linewidth=EDGE_LW_NIGHT, zorder=2)\n",
        "\n",
        "    # Day bars (outline)\n",
        "    for i, g in enumerate(x_groups):\n",
        "        val = grid.loc[g, \"Day\"] if (\"Day\" in grid.columns) else np.nan\n",
        "        if pd.notna(val):\n",
        "            ax.bar(pos_day[i], val, width=BAR_WIDTH, facecolor=(0,0,0,0),\n",
        "                   edgecolor=color_map[g], linewidth=EDGE_LW_DAY, zorder=3)\n",
        "\n",
        "    # Overlay individual dots by HueGroup\n",
        "    rng = np.random.default_rng(42)\n",
        "    jitter = lambda n: (rng.normal(0, 0.02, size=n))\n",
        "\n",
        "    # Day dots (outline markers)\n",
        "    sdf = df_dn[df_dn[\"DayNight\"] == \"Day\"].dropna(subset=[\"value\"])\n",
        "    for g in x_groups:\n",
        "        sub = sdf[sdf[\"XGroup\"] == g]\n",
        "        if sub.empty: continue\n",
        "        px = pos_day[x_groups.index(g)]\n",
        "        for h in sub[\"HueGroup\"].unique():\n",
        "            hh = sub[sub[\"HueGroup\"] == h]\n",
        "            if hh.empty: continue\n",
        "            ax.scatter(np.full(len(hh), px) + jitter(len(hh)), hh[\"value\"],\n",
        "                       s=DOT_SIZE**2/2, facecolors=pal_dots.get(h, \"black\"),\n",
        "                       edgecolors=\"black\", linewidths=0.6, alpha=NIGHT_ALPHA, zorder=4)\n",
        "\n",
        "    # Night dots (filled markers)\n",
        "    sdf = df_dn[df_dn[\"DayNight\"] == \"Night\"].dropna(subset=[\"value\"])\n",
        "    for g in x_groups:\n",
        "        sub = sdf[sdf[\"XGroup\"] == g]\n",
        "        if sub.empty: continue\n",
        "        px = pos_night[x_groups.index(g)]\n",
        "        for h in sub[\"HueGroup\"] == sub[\"HueGroup\"]:\n",
        "            pass  # to keep structure clear\n",
        "        for h in sub[\"HueGroup\"].unique():\n",
        "            hh = sub[sub[\"HueGroup\"] == h]\n",
        "            if hh.empty: continue\n",
        "            ax.scatter(np.full(len(hh), px) + jitter(len(hh)), hh[\"value\"],\n",
        "                       s=DOT_SIZE**2/2, facecolors=pal_dots.get(h, \"black\"),\n",
        "                       edgecolors=\"black\", linewidths=0.6, alpha=NIGHT_ALPHA, zorder=4)\n",
        "\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(x_groups, rotation=45, ha=\"right\")\n",
        "    ax.set_ylabel(base)\n",
        "    ax.set_title(f\"{base}\")\n",
        "    sns.despine(ax=ax)\n",
        "\n",
        "    ax_txt.axis(\"off\")\n",
        "    ax_txt.text(0, 1, _anova_daynight_text(df_dn, x_label, hue_label),\n",
        "                va=\"top\", ha=\"left\", fontsize=12, transform=ax_txt.transAxes)\n",
        "\n",
        "    if len(hue_levels) >= 2:\n",
        "        handles = [plt.Line2D([0],[0], marker='o', linestyle='None',\n",
        "                              markerfacecolor=pal_dots[h], markeredgecolor='black',\n",
        "                              label=str(h)) for h in hue_levels]\n",
        "        ax_txt.legend(handles=handles, title=hue_label,\n",
        "                      loc=\"upper left\", bbox_to_anchor=(0, 0.3),\n",
        "                      frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---- Run ----\n",
        "if 'long_df' not in globals():\n",
        "    raise RuntimeError(\"This cell expects long_df from the previous cell.\")\n",
        "\n",
        "Xsel = _selected_x_groups()\n",
        "if not Xsel:\n",
        "    print(\"No X groups selected or available.\")\n",
        "else:\n",
        "    cmap = _color_map(Xsel)\n",
        "    if FORCE_BASES is not None:\n",
        "        BASES = [b for b in FORCE_BASES if b in ALLOWED_DN_BASES]\n",
        "    else:\n",
        "        BASES = _discover_daynight_bases(long_df)\n",
        "    if not BASES:\n",
        "        print(\"No Day/Night metrics found in long_df.\")\n",
        "    else:\n",
        "        for base in BASES:\n",
        "            _plot_day_night_for_base(base, Xsel, cmap)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NWeeNCOgstiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfhfE3F8k6Jl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Interpellet intervals\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Collect IPI from file_to_df\n",
        "if \"file_to_df\" not in globals():\n",
        "    if \"loaded_files\" in globals() and \"feds\" in globals():\n",
        "        file_to_df = {Path(str(fname)).name: df for fname, df in zip(loaded_files, feds)}\n",
        "    else:\n",
        "        raise RuntimeError(\"Missing `file_to_df`.\")\n",
        "\n",
        "rows = []\n",
        "for fname, df in file_to_df.items():\n",
        "    if df is None or df.empty:\n",
        "        continue\n",
        "    if \"Event\" in df.columns:\n",
        "        df = df[df[\"Event\"].isin([\"Left\",\"Right\",\"Pellet\"])].copy()\n",
        "    if \"InterPelletInterval\" not in df.columns:\n",
        "        continue\n",
        "    vals = pd.to_numeric(df[\"InterPelletInterval\"], errors=\"coerce\").dropna()\n",
        "    vals = vals[vals > 0]\n",
        "    if vals.empty:\n",
        "        continue\n",
        "    base = os.path.basename(str(fname))\n",
        "    rows.extend({\"filename\": base, \"IPI_s\": float(v)} for v in vals)\n",
        "\n",
        "ipi = pd.DataFrame(rows)\n",
        "if ipi.empty:\n",
        "    raise RuntimeError(\"No InterPelletInterval values found.\")\n",
        "\n",
        "# 2) Merge XGroup from files_to_group_both (ignore Hue)\n",
        "if \"files_to_group_both\" not in globals() or files_to_group_both is None or files_to_group_both.empty:\n",
        "    raise RuntimeError(\"Missing `files_to_group_both`. Build Groups first.\")\n",
        "\n",
        "grp = files_to_group_both.copy()\n",
        "src_col = \"filename\" if \"filename\" in grp.columns else (\"File\" if \"File\" in grp.columns else None)\n",
        "if src_col is None:\n",
        "    raise RuntimeError(\"`files_to_group_both` needs 'filename' or 'File'.\")\n",
        "grp[\"filename\"] = grp[src_col].astype(str).map(os.path.basename)\n",
        "ipi = ipi.merge(grp[[\"filename\",\"XGroup\"]].drop_duplicates(), on=\"filename\", how=\"left\")\n",
        "ipi[\"XGroup\"] = ipi[\"XGroup\"].fillna(\"UNASSIGNED\")\n",
        "\n",
        "# 3) Choose groups: all checked in widget, else all present\n",
        "def _selected_xgroups():\n",
        "    if 'x_checks' in globals() and isinstance(x_checks, dict) and len(x_checks):\n",
        "        return [g for g, cb in x_checks.items() if getattr(cb, \"value\", False)]\n",
        "    return sorted(ipi[\"XGroup\"].dropna().unique().tolist())\n",
        "\n",
        "chosen = _selected_xgroups()\n",
        "if not chosen:\n",
        "    raise RuntimeError(\"No XGroups selected/found.\")\n",
        "ipi2 = ipi[ipi[\"XGroup\"].isin(chosen)].copy()\n",
        "\n",
        "# 4) KDE in log10 space (correct for log-x plotting)\n",
        "ipi2 = ipi2[(ipi2[\"IPI_s\"] > 0) & np.isfinite(ipi2[\"IPI_s\"])]\n",
        "ipi2[\"log10_IPI\"] = np.log10(ipi2[\"IPI_s\"])\n",
        "\n",
        "# --- Match order & colors from the main plotting cell ---\n",
        "\n",
        "_present = ipi2[\"XGroup\"].dropna().unique().tolist()\n",
        "\n",
        "# ORDER: prefer the global ordered_x (WT/CONTROL-first); otherwise reproduce rule locally\n",
        "if \"ordered_x\" in globals():\n",
        "    group_order = [g for g in ordered_x if g in _present] + [g for g in _present if g not in ordered_x]\n",
        "else:\n",
        "    import re\n",
        "    def _is_wt(g):\n",
        "        toks = [t for t in re.split(r'[^A-Z0-9]+', str(g).upper()) if t]\n",
        "        return any(t in {\"WT\", \"WILDTYPE\", \"CONTROL\", \"CTRL\"} for t in toks)\n",
        "    def _x_levels(g): return [p.strip() for p in str(g).split(\"|\")]\n",
        "    def _key(g):\n",
        "        lv = _x_levels(g)\n",
        "        wt_rank = 0 if any(_is_wt(tok) for tok in lv) or _is_wt(g) else 1\n",
        "        blanks = [(1 if s.strip().upper() in {\"\",\"UNASSIGNED\",\"NONE\",\"NA\",\"N/A\"} else 0, s.upper()) for s in lv]\n",
        "        return (wt_rank, *blanks, str(g).upper())\n",
        "    group_order = sorted(_present, key=_key)\n",
        "\n",
        "# COLORS: pull from x_colors (widgets) so bars/lines/KDE match\n",
        "palette_map = None\n",
        "if \"x_colors\" in globals() and isinstance(x_colors, dict) and x_colors:\n",
        "    def _col(g):\n",
        "        try:\n",
        "            v = x_colors[g].value\n",
        "            return v.strip() if isinstance(v, str) and v.strip() else \"tab:blue\"\n",
        "        except Exception:\n",
        "            return \"tab:blue\"\n",
        "    palette_map = {g: _col(g) for g in group_order}\n",
        "\n",
        "# ---- PLOT ONLY (no metric computation) ----\n",
        "sns.set_style(\"white\")\n",
        "plt.figure(figsize=(9, 5))\n",
        "sns.set_style(\"white\")\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# convert seconds to minutes and compute log10 in minutes\n",
        "ipi2[\"IPI_min\"] = ipi2[\"IPI_s\"] / 60.0\n",
        "ipi2 = ipi2[(ipi2[\"IPI_min\"] > 0) & np.isfinite(ipi2[\"IPI_min\"])]\n",
        "ipi2[\"log10_IPI_min\"] = np.log10(ipi2[\"IPI_min\"])\n",
        "\n",
        "# plot KDE in log10(minutes)\n",
        "ax = sns.kdeplot(\n",
        "    data=ipi2,\n",
        "    x=\"log10_IPI_min\",\n",
        "    hue=\"XGroup\",\n",
        "    hue_order=group_order,\n",
        "    palette=palette_map,\n",
        "    fill=False,\n",
        "    common_norm=False,\n",
        "    cut=0,\n",
        "    bw_adjust=0.9,\n",
        "    gridsize=512,\n",
        "    linewidth=2\n",
        ")\n",
        "\n",
        "# Nice decade ticks based on data range (in log10 minutes)\n",
        "lo = np.floor(ipi2[\"log10_IPI_min\"].min())\n",
        "hi = np.ceil(ipi2[\"log10_IPI_min\"].max())\n",
        "ticks_log = np.arange(lo, hi + 1)\n",
        "ax.set_xticks(ticks_log)\n",
        "\n",
        "def _min_label(t):\n",
        "    v = 10.0 ** float(t)  # minutes\n",
        "    # format: show 2 decimals if <1 min, otherwise integer minutes\n",
        "    return f\"{v:.2f}\" if v < 1 else f\"{int(round(v))}\"\n",
        "\n",
        "ax.set_xticklabels([_min_label(t) for t in ticks_log])\n",
        "ax.set_xlabel(\"Interpellet Interval (min)\")\n",
        "ax.set_ylabel(\"Density\")\n",
        "ax.set_title(\"\")\n",
        "leg = ax.get_legend()\n",
        "if leg:\n",
        "    leg.set_title(\"\")\n",
        "    leg.set_frame_on(False)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Embed TrueType fonts so text stays editable in Illustrator/Inkscape\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['pdf.fonttype'] = 42\n",
        "mpl.rcParams['ps.fonttype'] = 42\n",
        "\n",
        "outdir = \"figures\"\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "from datetime import datetime\n",
        "fname = f\"interpellet_histogram_{datetime.now():%Y-%m-%d}.pdf\"\n",
        "fig = ax.get_figure()\n",
        "fig.savefig(\n",
        "    os.path.join(outdir, fname),\n",
        "    format=\"pdf\",\n",
        "    bbox_inches=\"tight\",\n",
        "    transparent=True,\n",
        ")\n",
        "files.download(os.path.join(outdir, fname))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DKm0nsGv5f0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Cluster FR1 by heatmap & PCA (X/Hue from Group UI; generic labels/markers)\n",
        "\n",
        "import os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as mcolors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ---------- Pick FR1 source ----------\n",
        "if 'FR1metrics_merged' in globals() and FR1metrics_merged is not None and not FR1metrics_merged.empty:\n",
        "    FR1_src = FR1metrics_merged.copy()\n",
        "elif 'FR1metrics_csv' in globals() and FR1metrics_csv is not None and not FR1metrics_csv.empty:\n",
        "    FR1_src = FR1metrics_csv.copy()\n",
        "elif 'FR1metrics' in globals() and FR1metrics is not None and not FR1metrics.empty:\n",
        "    FR1_src = FR1metrics.copy()\n",
        "else:\n",
        "    raise RuntimeError(\"No FR1 table found. Expect FR1metrics_merged, FR1metrics_csv, FR1metrics, or FR1_metrics.\")\n",
        "\n",
        "# Ensure we have a File column (fallback from filename if needed)\n",
        "if 'File' not in FR1_src.columns:\n",
        "    if 'filename' in FR1_src.columns:\n",
        "        FR1_src = FR1_src.copy()\n",
        "        FR1_src['File'] = FR1_src['filename'].astype(str)\n",
        "    else:\n",
        "        raise RuntimeError(\"FR1 table must include 'File' or 'filename' column.\")\n",
        "\n",
        "# ---------- Groups from the Group UI ----------\n",
        "def _basename(p): return os.path.basename(str(p))\n",
        "\n",
        "if 'files_to_group_both' in globals() and files_to_group_both is not None and not files_to_group_both.empty:\n",
        "    grp_map = files_to_group_both.copy()\n",
        "    src_col = 'filename' if 'filename' in grp_map.columns else 'File'\n",
        "    grp_map[\"File_base\"] = grp_map[src_col].astype(str).apply(_basename)\n",
        "    grp_map = grp_map[[\"File_base\",\"XGroup\",\"HueGroup\"]]\n",
        "else:\n",
        "    raise RuntimeError(\"No groups found. Run the 'Group for plotting' widget (two-column) and click 'Build Groups'.\")\n",
        "# Ensure both tables have a File_base column built from the filename/File\n",
        "FR1_src = FR1_src.copy()\n",
        "src_col_fr1 = 'filename' if 'filename' in FR1_src.columns else 'File'\n",
        "FR1_src['File_base'] = FR1_src[src_col_fr1].astype(str).apply(_basename)\n",
        "\n",
        "# grp_map already has File_base from earlier code\n",
        "# Merge groups onto FR1; each file should map to at most one (XGroup, HueGroup)\n",
        "ldf = (\n",
        "    FR1_src.merge(\n",
        "        grp_map, on='File_base', how='left', validate='m:1'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Helpful checks\n",
        "if {'XGroup','HueGroup'}.issubset(ldf.columns) and ldf[['XGroup','HueGroup']].isna().all().all():\n",
        "    raise RuntimeError(\n",
        "        \"Group merge completed but all XGroup/HueGroup are NaN. \"\n",
        "        \"Check that the basename in FR1 matches the Group UI table.\"\n",
        "    )\n",
        "# ---------- STRICT METRIC SELECTION (whitelist patterns) ----------\n",
        "# Base metric names (covers base, _Day/_Night, and any extra suffix like _FR1)\n",
        "_METRIC_BASES = [\n",
        "    \"Pellets\",\"Left_Poke\",\"Right_Poke\",\"Total_Pokes\",\"Accuracy\",\"PokesPerPellet\",\n",
        "    \"RetrievalTime\",\"InterPelletInterval\",\"PokeTime\",\n",
        "    \"%MealPellets\",\"%GrazingPellets\",\"NumMeals\",\"AvgMealSize\",\"AvgMealDuration\",\n",
        "    \"RecordingHours\",\"MealsPerHour\",\"Daily_Pellets\",\"Left Poke with Pellet\",\n",
        "]\n",
        "\n",
        "def _metric_regex_from_bases(bases):\n",
        "    safes = [re.escape(b) for b in bases]\n",
        "    # Match base; optionally _Day/_Night; optionally any further suffix (e.g., _FR1)\n",
        "    return re.compile(rf\"^(?:{'|'.join(safes)})(?:_(?:Day|Night))?(?:_.+)?$\")\n",
        "\n",
        "_METRIC_RX = _metric_regex_from_bases(_METRIC_BASES)\n",
        "\n",
        "# Identify metric columns by name pattern only (ignore dtype), exclude ids explicitly\n",
        "ID_LIKE = {\n",
        "    \"File\",\"filename\",\"Mouse_ID\",\"Strain\",\"Sex\",\"Genotype\",\"Session_type\",\n",
        "    \"File_base\",\"XGroup\",\"HueGroup\"\n",
        "}\n",
        "metric_columns = [c for c in ldf.columns if isinstance(c, str) and _METRIC_RX.match(c) and c not in ID_LIKE]\n",
        "if not metric_columns:\n",
        "    raise RuntimeError(\"No metric columns matched the whitelist patterns. Check column names or base lists.\")\n",
        "\n",
        "# Coerce metric columns to numeric safely\n",
        "for c in metric_columns:\n",
        "    ldf[c] = pd.to_numeric(ldf[c], errors=\"coerce\")\n",
        "\n",
        "# ---------- Build long & wide ----------\n",
        "id_keep = [c for c in [\"File\",\"filename\",\"Mouse_ID\",\"Strain\",\"Sex\",\"Genotype\",\"Session_type\",\"XGroup\",\"HueGroup\"] if c in ldf.columns]\n",
        "\n",
        "long_df = ldf.melt(\n",
        "    id_vars=id_keep,\n",
        "    value_vars=metric_columns,\n",
        "    var_name=\"metric\",\n",
        "    value_name=\"value\"\n",
        ").copy()\n",
        "\n",
        "# Wide (one row per file/animal), average duplicates\n",
        "wide_index = [c for c in [\"File\",\"filename\",\"Mouse_ID\",\"Strain\",\"Sex\",\"Genotype\",\"XGroup\",\"HueGroup\"] if c in long_df.columns]\n",
        "wide_metrics = (\n",
        "    long_df.pivot_table(\n",
        "        index=wide_index,\n",
        "        columns=\"metric\",\n",
        "        values=\"value\",\n",
        "        aggfunc=\"mean\",\n",
        "        observed=True\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# The metric columns are exactly those we whitelisted\n",
        "metric_columns = [c for c in wide_metrics.columns if c not in wide_index]\n",
        "\n",
        "# ---------- Group means by XGroup (for heatmap & PCA feature set) ----------\n",
        "if (\"XGroup\" not in wide_metrics.columns) or (\"HueGroup\" not in wide_metrics.columns):\n",
        "    raise RuntimeError(\"XGroup and HueGroup are required to cluster on both.\")\n",
        "\n",
        "# Order rows as a stable product of XGroup then HueGroup\n",
        "_x_order = sorted(wide_metrics[\"XGroup\"].astype(str).unique())\n",
        "_h_order = sorted(wide_metrics[\"HueGroup\"].astype(str).unique())\n",
        "\n",
        "group_means = (\n",
        "    wide_metrics\n",
        "    .assign(XGroup=wide_metrics[\"XGroup\"].astype(str),\n",
        "            HueGroup=wide_metrics[\"HueGroup\"].astype(str))\n",
        "    .groupby([\"XGroup\", \"HueGroup\"], dropna=False)[metric_columns]\n",
        "    .mean()\n",
        "    .reindex(pd.MultiIndex.from_product([_x_order, _h_order],\n",
        "                                        names=[\"XGroup\",\"HueGroup\"]))\n",
        ")\n",
        "\n",
        "# Build a readable index like \"X | Hue\" for the heatmap rows\n",
        "group_means.index = [f\"{x} | {h}\" for x, h in group_means.index]\n",
        "\n",
        "# ---------- Heatmap (minmax per column, with numeric annotations) ----------\n",
        "def _fmt_cell(x):\n",
        "    if pd.isna(x): return \"\"\n",
        "    ax = abs(float(x))\n",
        "    return f\"{x:.0f}\" if ax >= 100 else f\"{x:.1f}\" if ax >= 10 else f\"{x:.2f}\"\n",
        "\n",
        "annot_data = group_means.applymap(_fmt_cell)\n",
        "\n",
        "heatmap_scaled = group_means.copy()\n",
        "for col in heatmap_scaled.columns:\n",
        "    col_min, col_max = heatmap_scaled[col].min(), heatmap_scaled[col].max()\n",
        "    if pd.isna(col_min) or pd.isna(col_max):\n",
        "        heatmap_scaled[col] = 0.0\n",
        "    elif col_max == col_min:\n",
        "        heatmap_scaled[col] = 0.5  # constant column  mid tone\n",
        "    else:\n",
        "        heatmap_scaled[col] = (heatmap_scaled[col] - col_min) / (col_max - col_min)\n",
        "\n",
        "heatmap_scaled = heatmap_scaled.sort_index()\n",
        "annot_data = annot_data.loc[heatmap_scaled.index]\n",
        "\n",
        "# ---------- PCA (same metric set) ----------\n",
        "pca_features = metric_columns[:]  # same set used in heatmap\n",
        "mouse_data = wide_metrics.dropna(subset=pca_features).copy()\n",
        "\n",
        "# Labels: prefer Mouse_ID, else filename (basename), else File\n",
        "if \"Mouse_ID\" in mouse_data.columns and mouse_data[\"Mouse_ID\"].notna().any():\n",
        "    labels = mouse_data[\"Mouse_ID\"].astype(str)\n",
        "elif \"filename\" in mouse_data.columns and mouse_data[\"filename\"].notna().any():\n",
        "    labels = mouse_data[\"filename\"].astype(str).apply(lambda p: os.path.basename(str(p)))\n",
        "else:\n",
        "    labels = mouse_data[\"File\"].astype(str)\n",
        "mouse_data[\"Label\"] = labels\n",
        "\n",
        "# Ensure grouping columns exist as strings\n",
        "for col, default in [(\"XGroup\", \"UNASSIGNED\"), (\"HueGroup\", \"ALL\")]:\n",
        "    if col in mouse_data.columns:\n",
        "        mouse_data[col] = mouse_data[col].astype(str)\n",
        "    else:\n",
        "        mouse_data[col] = default\n",
        "\n",
        "# Standardize metrics and run PCA\n",
        "X = StandardScaler().fit_transform(mouse_data[pca_features].values)\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(X)\n",
        "\n",
        "pca_df = pd.DataFrame(pca_result, columns=[\"PC1\", \"PC2\"])\n",
        "pca_df[\"Label\"]    = mouse_data[\"Label\"].values\n",
        "pca_df[\"XGroup\"]   = mouse_data[\"XGroup\"].values\n",
        "pca_df[\"HueGroup\"] = mouse_data[\"HueGroup\"].values\n",
        "\n",
        "# Loadings (top 8 by |PC1|)\n",
        "loadings = pd.DataFrame(pca.components_.T, index=pca_features, columns=[\"PC1\", \"PC2\"])\n",
        "top8_features = loadings.reindex(loadings[\"PC1\"].abs().sort_values(ascending=False).head(8).index)\n",
        "loadings_melted = top8_features[[\"PC1\", \"PC2\"]].reset_index().melt(id_vars=\"index\", var_name=\"PC\", value_name=\"Loading\")\n",
        "\n",
        "# ---------- Plot (13 grid) ----------\n",
        "fig = plt.figure(figsize=(18, 12), constrained_layout=True)\n",
        "gs = gridspec.GridSpec(\n",
        "    2, 2, figure=fig,\n",
        "    height_ratios=[0.8, 1.0],\n",
        "    width_ratios=[1.0, 0.8],\n",
        "    hspace=0.1, wspace=0.1\n",
        ")\n",
        "\n",
        "# Top: heatmap spans both columns\n",
        "ax_heat    = fig.add_subplot(gs[0, :])\n",
        "# Bottom: scatter (left) and bar (right)\n",
        "ax_scatter = fig.add_subplot(gs[1, 0])\n",
        "ax_bar     = fig.add_subplot(gs[1, 1])\n",
        "\n",
        "# 1) Heatmap of XGroup means\n",
        "sns.heatmap(\n",
        "    heatmap_scaled,\n",
        "    ax=ax_heat,\n",
        "    annot=annot_data.values,\n",
        "    fmt=\"\",\n",
        "    cmap=\"Blues\",\n",
        "    linewidths=0.5,\n",
        "    linecolor='gray',\n",
        "    alpha=0.5,\n",
        "    cbar=True\n",
        ")\n",
        "ax_heat.tick_params(axis='x', rotation=70)\n",
        "ax_heat.tick_params(axis='y', rotation=0)\n",
        "ax_heat.set_title(\"\", fontsize=16, color=\"darkblue\")\n",
        "ax_heat.set_xlabel(\"\"); ax_heat.set_ylabel(\"\")\n",
        "\n",
        "# 2) PCA: color by XGroup; marker encodes HueGroup\n",
        "unique_x = sorted(pca_df[\"XGroup\"].unique())\n",
        "\n",
        "# color map for XGroup\n",
        "x_colors = {}\n",
        "if len(unique_x) >= 1: x_colors[unique_x[0]] = \"dodgerblue\"\n",
        "if len(unique_x) >= 2: x_colors[unique_x[1]] = \"red\"\n",
        "if len(unique_x) > 2:\n",
        "    cmap = cm.get_cmap(\"tab20\", len(unique_x) - 2)\n",
        "    for i, grp in enumerate(unique_x[2:]):\n",
        "        x_colors[grp] = mcolors.to_hex(cmap(i))\n",
        "\n",
        "# marker selection for HueGroup\n",
        "all_hues = sorted([h for h in pca_df[\"HueGroup\"].unique()])\n",
        "if len(all_hues) == 0:\n",
        "    all_hues = [\"ALL\"]\n",
        "if len(all_hues) == 1:\n",
        "    hue_to_marker = {all_hues[0]: (\"o\", \"filled\")}\n",
        "elif len(all_hues) == 2:\n",
        "    hue_to_marker = {all_hues[0]: (\"o\", \"hollow\"), all_hues[1]: (\"o\", \"filled\")}\n",
        "else:\n",
        "    marker_cycle = [\"o\", \"s\", \"^\", \"D\", \"P\", \"X\", \"*\", \"v\", \"<\", \">\"]\n",
        "    hue_to_marker = {h: (marker_cycle[i % len(marker_cycle)], \"filled\") for i, h in enumerate(all_hues)}\n",
        "\n",
        "# draw points: loop XGroup (color), then HueGroup (marker style)\n",
        "for xg in unique_x:\n",
        "    sub_x = pca_df[pca_df[\"XGroup\"] == xg]\n",
        "    for hg in all_hues:\n",
        "        sub = sub_x[sub_x[\"HueGroup\"] == hg]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        marker, fill = hue_to_marker[hg]\n",
        "        if fill == \"hollow\":\n",
        "            ax_scatter.scatter(\n",
        "                sub[\"PC1\"], sub[\"PC2\"],\n",
        "                edgecolors=x_colors.get(xg, \"black\"), facecolors=\"none\",\n",
        "                s=110, linewidth=1.2, marker=marker, alpha=0.85,\n",
        "                label=f\"{xg} | {hg}\"\n",
        "            )\n",
        "        else:  # filled\n",
        "            ax_scatter.scatter(\n",
        "                sub[\"PC1\"], sub[\"PC2\"],\n",
        "                color=x_colors.get(xg, \"black\"),\n",
        "                s=110, linewidth=0.8, marker=marker, alpha=0.85,\n",
        "                label=f\"{xg} | {hg}\"\n",
        "            )\n",
        "\n",
        "# point labels\n",
        "for _, row in pca_df.iterrows():\n",
        "    ax_scatter.text(\n",
        "        row[\"PC1\"] + 0.05, row[\"PC2\"] + 0.12, str(row[\"Label\"]),\n",
        "        fontsize=9, color=\"gray\", alpha=0.6, ha=\"center\", va=\"bottom\"\n",
        "    )\n",
        "\n",
        "ax_scatter.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
        "ax_scatter.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
        "\n",
        "# legend (dedup + sorted)\n",
        "handles, labels = ax_scatter.get_legend_handles_labels()\n",
        "pairs = sorted({(lab, h) for lab, h in zip(labels, handles)}, key=lambda x: x[0])\n",
        "if pairs:\n",
        "    sorted_labels, sorted_handles = zip(*pairs)\n",
        "    ax_scatter.legend(sorted_handles, sorted_labels, frameon=True, title=\"XGroup | HueGroup\", fontsize=9)\n",
        "\n",
        "# 3) Loadings barplot\n",
        "sns.barplot(\n",
        "    data=loadings_melted, y=\"index\", x=\"Loading\",\n",
        "    hue=\"PC\", hue_order=[\"PC1\", \"PC2\"],\n",
        "    ax=ax_bar, palette=[\"purple\", \"orange\"], alpha=0.5\n",
        ")\n",
        "ax_bar.set_xlabel(\"Loading Weight\")\n",
        "ax_bar.set_title(\"Top 8 PC Loadings (metrics only)\", fontsize=14)\n",
        "ax_bar.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
        "ax_bar.set_ylabel(\"\")\n",
        "ax_bar.legend(title=\"\", frameon=False, fontsize=9)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}