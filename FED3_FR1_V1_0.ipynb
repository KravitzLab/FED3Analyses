{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/FED3Analyses/blob/main/FED3_FR1_V1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEm7saj7IRD2"
      },
      "source": [
        "## FED3 FR1 data analysis\n",
        "<br>\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqe_1a1j1bQaqhOxq0VvukPqfolLRUqOdl-g&s\" width=\"200\" />\n",
        "\n",
        "Updated: 12-9-25\n",
        "<br>\n",
        "Authors: Chantelle Murrell and Sebastian Alves  \n",
        "Version 1.0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k-YGEmW-KCNK"
      },
      "outputs": [],
      "source": [
        "# @title Install libraries and import them {\"run\":\"auto\"}\n",
        "\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Packages to ensure are installed (add others here if you like)\n",
        "packages = {\n",
        "    \"fed3\": \"git+https://github.com/earnestt1234/fed3.git\",\n",
        "    \"fed3bandit\": \"fed3bandit\",\n",
        "    \"pingouin\": \"pingouin\",\n",
        "    \"ipydatagrid\": \"ipydatagrid\",\n",
        "    \"openpyxl\": \"openpyxl\",\n",
        "}\n",
        "\n",
        "for name, source in packages.items():\n",
        "    if importlib.util.find_spec(name) is None:\n",
        "        print(f\"Installing {name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", source])\n",
        "\n",
        "# ----------------------------\n",
        "# Imports\n",
        "# ----------------------------\n",
        "# Standard library\n",
        "import copy\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tempfile\n",
        "import threading\n",
        "import time\n",
        "import warnings\n",
        "import zipfile\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "from os.path import basename, splitext\n",
        "\n",
        "# Third-party\n",
        "from ipydatagrid import DataGrid, TextRenderer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "import fed3\n",
        "import fed3.plot as fplot\n",
        "import fed3bandit as f3b\n",
        "from scipy.stats import f_oneway\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.gridspec as gridspec\n",
        "from google.colab import files\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration\n",
        "# ----------------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.rcParams.update({'font.size': 12, 'figure.autolayout': True})\n",
        "plt.rcParams['figure.figsize'] = [6, 4]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "print(\"Packages installed and imports ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TONj_5IvKdNs",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Upload raw FED3 files (individual csvs or a zip folder of multiple csvs)\n",
        "\n",
        "\n",
        "# Reset caches to avoid duplicates if you re-run this cell\n",
        "feds, loaded_files, session_types = [], [], []\n",
        "\n",
        "def extract_session_type(csv_path, fallback=\"Unknown\"):\n",
        "    \"\"\"Read 'Session_Type ' or variants; return first non-empty value.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, sep=None, engine='python', dtype=str)\n",
        "        df.columns = [c.strip() for c in df.columns]\n",
        "        lower = {c.casefold(): c for c in df.columns}\n",
        "        for cand in [\"session_type\", \"session type\", \"sessiontype\", \"session\"]:\n",
        "            if cand in lower:\n",
        "                col = lower[cand]\n",
        "                vals = df[col].dropna().astype(str).str.strip()\n",
        "                vals = vals[vals.ne(\"\")]\n",
        "                if not vals.empty:\n",
        "                    return vals.iloc[0]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return fallback\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for name, data in uploaded.items():\n",
        "    if name.lower().endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(io.BytesIO(data)) as zf:\n",
        "            for zi in zf.infolist():\n",
        "                if not zi.filename.lower().endswith(\".csv\"):\n",
        "                    continue\n",
        "                file_data = zf.read(zi)\n",
        "                if len(file_data) <= 1024:\n",
        "                    continue\n",
        "                with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".csv\", delete=False) as tmp:\n",
        "                    tmp.write(file_data); tmp_path = tmp.name\n",
        "                try:\n",
        "                    session_type = extract_session_type(tmp_path)\n",
        "                    df = fed3.load(tmp_path)\n",
        "                    df.name = os.path.basename(zi.filename)\n",
        "                    df.attrs = {\"Session_type\": session_type}\n",
        "                    feds.append(df)\n",
        "                    loaded_files.append(os.path.basename(zi.filename))\n",
        "                    session_types.append(session_type)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {zi.filename}: {e}\")\n",
        "                finally:\n",
        "                    os.remove(tmp_path)\n",
        "    elif name.lower().endswith(\".csv\"):\n",
        "        if len(data) <= 1024:\n",
        "            continue\n",
        "        with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".csv\", delete=False) as tmp:\n",
        "            tmp.write(data); tmp_path = tmp.name\n",
        "        try:\n",
        "            session_type = extract_session_type(tmp_path)\n",
        "            df = fed3.load(tmp_path)\n",
        "            df.name = os.path.basename(name)\n",
        "            df.attrs = {\"Session_type\": session_type}\n",
        "            feds.append(df)\n",
        "            loaded_files.append(os.path.basename(name))\n",
        "            session_types.append(session_type)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {name}: {e}\")\n",
        "        finally:\n",
        "            os.remove(tmp_path)\n",
        "\n",
        "print(f\"Loaded {len(loaded_files)} files. Session types captured for all.\")\n",
        "# Optional quick plot\n",
        "if feds:\n",
        "    try:\n",
        "        fed3.as_aligned(feds, alignment=\"datetime\", inplace=True)\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        fplot.line(feds, y='pellets'); plt.legend().remove(); plt.tight_layout(); plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Plotting skipped: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K311oswozte",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Build Key\n",
        "\n",
        "\n",
        "import os, glob, io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ipydatagrid import DataGrid, TextRenderer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from google.colab import files as colab_files\n",
        "from google.colab import output as colab_output\n",
        "\n",
        "# Require that the file-upload cell has already populated these:\n",
        "assert 'loaded_files' in globals() and 'session_types' in globals(), \\\n",
        "    \"Run the 'Upload FED3 files' cell first.\"\n",
        "\n",
        "colab_output.enable_custom_widget_manager()\n",
        "\n",
        "# ---------------------------\n",
        "# Base: bare-bones Key_Df from loaded data\n",
        "# ---------------------------\n",
        "def _make_base_key_df():\n",
        "    return pd.DataFrame({\"filename\": loaded_files, \"Session_type\": session_types})\n",
        "\n",
        "def _file_base(s):\n",
        "    return os.path.splitext(os.path.basename(str(s)))[0].strip()\n",
        "\n",
        "def _norm_base_lower(s):\n",
        "    return _file_base(s).lower()\n",
        "\n",
        "# ---------------------------\n",
        "# Key scanner: detect Mouse_ID or filename columns\n",
        "# ---------------------------\n",
        "def _scan_key_columns(df):\n",
        "    \"\"\"\n",
        "    Returns dict:\n",
        "      {\n",
        "        'has_mouse': bool,\n",
        "        'has_filename': bool,\n",
        "        'filename_col': 'filename'|'File'|None,\n",
        "        'msg': str\n",
        "      }\n",
        "    Accepts keys that have either Mouse_ID or a filename column (filename/File).\n",
        "    \"\"\"\n",
        "    info = {'has_mouse': False, 'has_filename': False, 'filename_col': None, 'msg': ''}\n",
        "    try:\n",
        "        cols = [str(c).strip() for c in df.columns]\n",
        "        has_mouse = 'Mouse_ID' in cols\n",
        "        fname_col = 'filename' if 'filename' in cols else ('File' if 'File' in cols else None)\n",
        "        info.update({\n",
        "            'has_mouse': has_mouse,\n",
        "            'has_filename': fname_col is not None,\n",
        "            'filename_col': fname_col\n",
        "        })\n",
        "        if has_mouse:\n",
        "            info['msg'] = \"'Mouse_ID' found.\"\n",
        "        elif fname_col:\n",
        "            info['msg'] = f\"'{fname_col}' found; will match on filename.\"\n",
        "        else:\n",
        "            info['msg'] = \"Neither 'Mouse_ID' nor 'filename'/'File' found in provided key.\"\n",
        "    except Exception as e:\n",
        "        info['msg'] = f\"Error while checking key: {e}\"\n",
        "    return info\n",
        "\n",
        "# ---------------------------\n",
        "# Read uploaded key (CSV/XLSX), accept Mouse_ID or filename\n",
        "# ---------------------------\n",
        "def _read_key_from_upload(name, content_bytes):\n",
        "    \"\"\"Return (df_or_None, message). Reads CSV/XLSX bytes from Colab upload.\"\"\"\n",
        "    ext = name.lower().rsplit('.', 1)[-1] if '.' in name else ''\n",
        "    try:\n",
        "        bio = io.BytesIO(content_bytes)\n",
        "        if ext == 'xlsx':\n",
        "            xls = pd.ExcelFile(bio, engine='openpyxl')\n",
        "            frames = [pd.read_excel(xls, sheet_name=s) for s in xls.sheet_names]\n",
        "            key_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        elif ext == 'csv':\n",
        "            key_df = pd.read_csv(bio, sep=None, engine='python')\n",
        "        else:\n",
        "            return None, f\"Unsupported key type .{ext}\"\n",
        "\n",
        "        key_df = key_df.copy()\n",
        "        key_df.columns = [str(c).strip() for c in key_df.columns]\n",
        "        scan = _scan_key_columns(key_df)\n",
        "        if not (scan['has_mouse'] or scan['has_filename']):\n",
        "            return None, scan['msg']\n",
        "\n",
        "        # Normalize types/columns we might use later\n",
        "        if scan['has_mouse']:\n",
        "            key_df['Mouse_ID'] = key_df['Mouse_ID'].astype(str).str.strip()\n",
        "\n",
        "        if scan['has_filename']:\n",
        "            fcol = scan['filename_col']\n",
        "            key_df[fcol] = key_df[fcol].astype(str).str.strip()\n",
        "            key_df['_key_file_base_lower'] = key_df[fcol].map(_norm_base_lower)\n",
        "\n",
        "        # Persist a deterministic copy on disk for reproducibility\n",
        "        fixed_path = f\"_uploaded_key.{ext}\"\n",
        "        with open(fixed_path, \"wb\") as f:\n",
        "            f.write(content_bytes)\n",
        "        globals()['uploaded_key_path'] = fixed_path\n",
        "\n",
        "        return key_df, f\"Key loaded from upload ({name}) and saved to {fixed_path}. {scan['msg']}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error reading uploaded key: {e}\"\n",
        "\n",
        "# ---------------------------\n",
        "# Matching filename <-> Mouse_ID\n",
        "# ---------------------------\n",
        "def _match_mouse_id_to_filenames(filenames, key_df):\n",
        "    \"\"\"Return DataFrame: filename, Mouse_ID, match_status based on Mouse_ID substring in filename.\"\"\"\n",
        "    base_names_lower = [_norm_base_lower(f) for f in filenames]\n",
        "    mouse_ids = (\n",
        "        key_df['Mouse_ID']\n",
        "        .dropna().astype(str).map(str.strip)\n",
        "        .replace({'': np.nan}).dropna().unique().tolist()\n",
        "    )\n",
        "    rows = []\n",
        "    for fname, base in zip(filenames, base_names_lower):\n",
        "        hits = [mid for mid in mouse_ids if str(mid).lower() in base]\n",
        "        if len(hits) == 1:\n",
        "            rows.append({\"filename\": fname, \"Mouse_ID\": hits[0], \"match_status\": \"Matched (Mouse_ID in filename)\"})\n",
        "        elif len(hits) > 1:\n",
        "            longest = max(len(str(h)) for h in hits)\n",
        "            best = [h for h in hits if len(str(h)) == longest]\n",
        "            if len(best) == 1:\n",
        "                rows.append({\"filename\": fname, \"Mouse_ID\": best[0], \"match_status\": \"Matched (longest Mouse_ID token)\"})\n",
        "            else:\n",
        "                rows.append({\"filename\": fname, \"Mouse_ID\": None, \"match_status\": f\"Ambiguous Mouse_ID: {hits}\"})\n",
        "        else:\n",
        "            rows.append({\"filename\": fname, \"Mouse_ID\": None, \"match_status\": \"Mouse_ID not found in filename\"})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------------------------\n",
        "# Build/Rematch function\n",
        "# ---------------------------\n",
        "status_box = widgets.Output()\n",
        "Key_Df = _make_base_key_df()  # start bare-bones\n",
        "\n",
        "def build_or_rematch_key_df(key_df=None, msg_hint=\"\"):\n",
        "    \"\"\"\n",
        "    If key_df provided and valid:\n",
        "      - Prefer Mouse_ID mapping if key has Mouse_ID.\n",
        "      - Else fall back to filename merge (case-insensitive basename).\n",
        "    Else: keep bare-bones.\n",
        "    \"\"\"\n",
        "    global Key_Df\n",
        "    files_df = _make_base_key_df().copy()\n",
        "    files_df['_file_base_lower'] = files_df['filename'].map(_norm_base_lower)\n",
        "\n",
        "    if key_df is None:\n",
        "        Key_Df = files_df.drop(columns=['_file_base_lower']).copy()\n",
        "        Key_Df[\"match_status\"] = \"No key\"\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Key status: No key provided; showing bare-bones Key_Df.\")\n",
        "        return\n",
        "\n",
        "    # Identify key capabilities\n",
        "    scan = _scan_key_columns(key_df)\n",
        "    kd = key_df.copy()\n",
        "\n",
        "    # Make a unique version of the key for whichever join we use\n",
        "    def _dedup(df, subset_cols):\n",
        "        dup_counts = df[subset_cols].astype(str).agg('|'.join, axis=1).value_counts()\n",
        "        n_dups = int((dup_counts > 1).sum())\n",
        "        if n_dups:\n",
        "            with status_box:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Note: {n_dups} duplicate key(s) on {subset_cols}; taking the first occurrence.\")\n",
        "        return df.drop_duplicates(subset=subset_cols, keep=\"first\")\n",
        "\n",
        "    if scan['has_mouse']:\n",
        "        # Mouse_ID route (preferred)\n",
        "        kd['Mouse_ID'] = kd['Mouse_ID'].astype(str).str.strip()\n",
        "        key_unique = _dedup(kd, ['Mouse_ID'])\n",
        "\n",
        "        matched = _match_mouse_id_to_filenames(files_df['filename'].tolist(), key_unique)[\n",
        "            [\"filename\", \"Mouse_ID\", \"match_status\"]\n",
        "        ]\n",
        "\n",
        "        Key_Df = (\n",
        "            files_df\n",
        "            .merge(matched, on=\"filename\", how=\"left\")\n",
        "            .merge(key_unique, on=\"Mouse_ID\", how=\"left\", suffixes=(\"\", \"_key\"))\n",
        "            .drop(columns=['_file_base_lower'])\n",
        "        )\n",
        "\n",
        "    elif scan['has_filename']:\n",
        "        # Filename route (fallback)\n",
        "        fcol = scan['filename_col']\n",
        "        kd['_key_file_base_lower'] = kd[fcol].map(_norm_base_lower)\n",
        "        key_unique = _dedup(kd, ['_key_file_base_lower'])\n",
        "\n",
        "        Key_Df = (\n",
        "            files_df\n",
        "            .merge(key_unique, left_on=\"_file_base_lower\", right_on=\"_key_file_base_lower\", how=\"left\", suffixes=(\"\", \"_key\"))\n",
        "            .drop(columns=['_file_base_lower', '_key_file_base_lower'])\n",
        "        )\n",
        "        # Give a simple match_status summary for filename matching\n",
        "        Key_Df[\"match_status\"] = np.where(\n",
        "            Key_Df[fcol].notna(), \"Matched (filename)\", \"Filename not found in key\"\n",
        "        )\n",
        "    else:\n",
        "        # Neither route available (shouldn't happen due to earlier check)\n",
        "        Key_Df = files_df.drop(columns=['_file_base_lower']).copy()\n",
        "        Key_Df[\"match_status\"] = \"Key missing Mouse_ID and filename columns\"\n",
        "\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        if msg_hint:\n",
        "            print(msg_hint)\n",
        "        print(f\"Merged key columns into Key_Df ({len(Key_Df)} rows, {len(Key_Df.columns)} cols).\")\n",
        "\n",
        "# ---------------------------\n",
        "# Grid UI\n",
        "# ---------------------------\n",
        "def make_grid(df: pd.DataFrame):\n",
        "    g = DataGrid(\n",
        "        df,\n",
        "        editable=True,\n",
        "        selection_mode='cell',\n",
        "        layout={'height': '420px'},\n",
        "        base_row_size=28,\n",
        "        base_column_size=120,\n",
        "    )\n",
        "    g.default_renderer = TextRenderer(text_wrap=True)\n",
        "    return g\n",
        "\n",
        "def rebuild_grid(msg=\"\"):\n",
        "    global grid, ui\n",
        "    df = Key_Df.copy().reset_index(drop=True)\n",
        "    new_grid = make_grid(df)\n",
        "    ui.children = (upload_row, new_grid, controls, status_box)\n",
        "    grid = new_grid\n",
        "    with status_box:\n",
        "        if msg:\n",
        "            print(msg)\n",
        "        print(f\"Grid now shows Key_Df ({len(df)} rows, {len(df.columns)} cols)\")\n",
        "\n",
        "# ---------------------------\n",
        "# Colab-native upload button ONLY (no path UI)\n",
        "# ---------------------------\n",
        "upload_btn = widgets.Button(description=\"Upload\", button_style=\"primary\", layout=widgets.Layout(width=\"120px\"))\n",
        "reset_btn = widgets.Button(description=\"Reset Key\", button_style=\"warning\", layout=widgets.Layout(width=\"120px\"))\n",
        "download_button = widgets.Button(description='Download', button_style='success', layout=widgets.Layout(width=\"120px\"))\n",
        "\n",
        "def on_colab_upload(_):\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Opening Colab upload dialog...\")\n",
        "    uploaded = colab_files.upload()  # opens the native Colab picker\n",
        "    if not uploaded:\n",
        "        with status_box:\n",
        "            print(\"No file selected.\")\n",
        "        return\n",
        "    name, content = next(iter(uploaded.items()))\n",
        "    key_df, msg = _read_key_from_upload(name, content)\n",
        "    if key_df is None:\n",
        "        build_or_rematch_key_df(None)\n",
        "        rebuild_grid(f\"Key status: {msg}\")\n",
        "    else:\n",
        "        build_or_rematch_key_df(key_df, msg_hint=f\"Key status: {msg}\")\n",
        "        rebuild_grid()\n",
        "\n",
        "def _load_saved_key_from_disk():\n",
        "    \"\"\"Returns (key_df_or_None, message) from uploaded_key_path if present/valid.\"\"\"\n",
        "    key_path = globals().get('uploaded_key_path', None)\n",
        "    if not (key_path and os.path.exists(key_path)):\n",
        "        return None, \"No saved key on disk to reload.\"\n",
        "    try:\n",
        "        ext = key_path.lower().rsplit('.', 1)[-1] if '.' in key_path else ''\n",
        "        if ext == 'xlsx':\n",
        "            xls = pd.ExcelFile(key_path, engine='openpyxl')\n",
        "            frames = [pd.read_excel(xls, sheet_name=s) for s in xls.sheet_names]\n",
        "            key_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        elif ext == 'csv':\n",
        "            key_df = pd.read_csv(key_path, sep=None, engine='python')\n",
        "        else:\n",
        "            return None, f\"Unsupported key type .{ext}\"\n",
        "\n",
        "        key_df = key_df.copy()\n",
        "        key_df.columns = [str(c).strip() for c in key_df.columns]\n",
        "        scan = _scan_key_columns(key_df)\n",
        "        if not (scan['has_mouse'] or scan['has_filename']):\n",
        "            return None, scan['msg']\n",
        "\n",
        "        if scan['has_mouse']:\n",
        "            key_df['Mouse_ID'] = key_df['Mouse_ID'].astype(str).str.strip()\n",
        "        if scan['has_filename']:\n",
        "            fcol = scan['filename_col']\n",
        "            key_df[fcol] = key_df[fcol].astype(str).str.strip()\n",
        "            key_df['_key_file_base_lower'] = key_df[fcol].map(_norm_base_lower)\n",
        "\n",
        "        return key_df, f\"Key reloaded from {os.path.basename(key_path)}. {scan['msg']}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error reading saved uploaded key: {e}\"\n",
        "\n",
        "def on_reset(_):\n",
        "    \"\"\"\n",
        "    HARD RESET: forget any saved key and show bare-bones Key_Df.\n",
        "    Deletes _uploaded_key.(xlsx|csv) if present and clears uploaded_key_path.\n",
        "    \"\"\"\n",
        "    # 1) Forget path in memory\n",
        "    globals().pop('uploaded_key_path', None)\n",
        "\n",
        "    # 2) Remove any persisted key files from disk\n",
        "    import os\n",
        "    for ext in (\"xlsx\", \"csv\"):\n",
        "        try:\n",
        "            os.remove(f\"_uploaded_key.{ext}\")\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    # 3) Show bare-bones\n",
        "    build_or_rematch_key_df(None)\n",
        "    rebuild_grid(\"Reset: cleared saved key; showing bare-bones Key_Df.\")\n",
        "\n",
        "def download_df(_):\n",
        "    global Key_Df\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Saving latest Key_Df as XLSX ...\")\n",
        "    try:\n",
        "        path = \"/content/Key_Df.xlsx\"\n",
        "        Key_Df.to_excel(path, index=False, engine='openpyxl')\n",
        "        colab_files.download(path)\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Saved and downloading Key_Df.xlsx ...\")\n",
        "    except Exception as e:\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Error while saving/downloading: {e}\")\n",
        "\n",
        "upload_btn.on_click(on_colab_upload)\n",
        "reset_btn.on_click(on_reset)\n",
        "download_button.on_click(download_df)\n",
        "\n",
        "upload_row = widgets.HBox([\n",
        "    widgets.HTML(\"<b>Optional key:</b>\"),\n",
        "    upload_btn,\n",
        "    reset_btn,\n",
        "    download_button\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# Edit / rematch / download controls\n",
        "# ---------------------------\n",
        "new_col_name    = widgets.Text(placeholder='Enter new column name', description='New Col:')\n",
        "add_col_button  = widgets.Button(description='Add Column', button_style='info')\n",
        "apply_button    = widgets.Button(description='Apply Changes', button_style='primary', layout=widgets.Layout(width=\"120px\"))\n",
        "\n",
        "def add_column(_):\n",
        "    global Key_Df\n",
        "    col = new_col_name.value.strip()\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        if not col:\n",
        "            print(\"Please enter a column name.\"); return\n",
        "        if col in Key_Df.columns:\n",
        "            print(f\"Column '{col}' already exists.\"); return\n",
        "        Key_Df[col] = \"\"\n",
        "        print(f\"Added column '{col}' to Key_Df.\")\n",
        "    rebuild_grid()\n",
        "\n",
        "def apply_edits(_):\n",
        "    global Key_Df\n",
        "    try:\n",
        "        Key_Df = grid.data.copy().reset_index(drop=True)\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Applied grid edits to Key_Df ({len(Key_Df)} rows, {len(Key_Df.columns)} cols).\")\n",
        "    except Exception as e:\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Error applying edits: {e}\")\n",
        "\n",
        "add_col_button.on_click(add_column)\n",
        "apply_button.on_click(apply_edits)\n",
        "\n",
        "controls = widgets.HBox([new_col_name, add_col_button, apply_button])\n",
        "\n",
        "# ---------------------------\n",
        "# Initialize UI\n",
        "# ---------------------------\n",
        "Key_Df = _make_base_key_df()\n",
        "Key_Df[\"match_status\"] = \"No key\"\n",
        "\n",
        "grid = make_grid(Key_Df.copy().reset_index(drop=True))\n",
        "ui = widgets.VBox([upload_row, grid, controls, status_box])\n",
        "display(ui)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p6hqDVU-Uzm",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Analyze FR1 metrics\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from datetime import datetime\n",
        "\n",
        "# Use the cropped list if present, else the full list\n",
        "files_list = feds\n",
        "assert isinstance(files_list, list) and len(files_list) > 0, \"No FED3 files loaded.\"\n",
        "assert 'Key_Df' in globals() and isinstance(Key_Df, pd.DataFrame), \"Build/rematch Key_Df first.\"\n",
        "\n",
        "# metadata_df = copy of Key_Df\n",
        "metadata_df = Key_Df.copy().reset_index(drop=True)\n",
        "if 'filename' in metadata_df.columns:\n",
        "    metadata_df['filename'] = metadata_df['filename'].astype(str).map(os.path.basename)\n",
        "\n",
        "# ---------------------------\n",
        "# Requirements\n",
        "# ---------------------------\n",
        "assert 'metadata_df' in globals() and isinstance(metadata_df, pd.DataFrame), \\\n",
        "    \"metadata_df not found. Build/rematch Key_Df and set metadata_df = Key_Df.copy() first.\"\n",
        "\n",
        "def _basename(pathlike) -> str:\n",
        "    s = str(pathlike).replace(\"\\\\\", \"/\")\n",
        "    return s.split(\"/\")[-1]\n",
        "\n",
        "def _get_sessions():\n",
        "    if 'feds' in globals() and isinstance(feds, (list, tuple)) and len(feds) > 0:\n",
        "        return list(feds)\n",
        "    raise RuntimeError(\"No FED3 sessions found. Expecting a non-empty 'feds' list.\")\n",
        "\n",
        "def _safe_col(df, candidates):\n",
        "    # case/spacing/hyphen tolerant lookup\n",
        "    norm = lambda s: str(s).strip().lower().replace('-', '_').replace(' ', '_')\n",
        "    lmap = {norm(c): c for c in df.columns}\n",
        "    for cand in candidates:\n",
        "        key = norm(cand)\n",
        "        if key in lmap:\n",
        "            return lmap[key]\n",
        "    return None\n",
        "\n",
        "def _prep_events(df):\n",
        "    \"\"\"Return filtered df with only Left/Right/Pellet and the resolved Event column name.\"\"\"\n",
        "    ev_col = _safe_col(df, [\"Event\", \"event\"])\n",
        "    if ev_col is None:\n",
        "        return df.iloc[0:0].copy(), None\n",
        "    return df[df[ev_col].isin([\"Left\", \"Right\", \"Pellet\"])].copy(), ev_col\n",
        "\n",
        "def _pellet_times_from_df(df):\n",
        "    ev_col = _safe_col(df, [\"Event\", \"event\"])\n",
        "    if ev_col is None:\n",
        "        return []\n",
        "    pel = df[df[ev_col] == \"Pellet\"]\n",
        "    # prefer datetime-like index; else common timestamp columns; else index\n",
        "    if isinstance(pel.index, pd.DatetimeIndex):\n",
        "        ts = pel.index.to_series()\n",
        "    else:\n",
        "        for cand in [\"MM:DD:YYYY hh:mm:ss\", \"DateTime\", \"Datetime\", \"Timestamp\", \"timestamp\", \"datetime\"]:\n",
        "            if cand in pel.columns:\n",
        "                ts = pd.to_datetime(pel[cand], errors=\"coerce\")\n",
        "                break\n",
        "        else:\n",
        "            ts = pd.to_datetime(pel.index, errors=\"coerce\")\n",
        "    ts = ts.dropna().sort_values()\n",
        "    return ts.to_list()\n",
        "\n",
        "def _get_ts_series_from_col_or_index(df, ts_candidates=(\"MM:DD:YYYY hh:mm:ss\",\"DateTime\",\"Datetime\",\"Timestamp\",\"timestamp\",\"datetime\")):\n",
        "    \"\"\"Return a pandas Series of datetimes aligned to df.index.\"\"\"\n",
        "    for cand in ts_candidates:\n",
        "        if cand in df.columns:\n",
        "            ts = pd.to_datetime(df[cand], errors=\"coerce\")\n",
        "            # align to index shape\n",
        "            if not isinstance(ts, pd.Series):\n",
        "                ts = pd.Series(ts, index=df.index)\n",
        "            else:\n",
        "                ts = ts.reindex(df.index)\n",
        "            return ts\n",
        "    if isinstance(df.index, pd.DatetimeIndex):\n",
        "        return pd.Series(df.index, index=df.index)\n",
        "    return pd.to_datetime(pd.Series(df.index, index=df.index), errors=\"coerce\")\n",
        "\n",
        "def _split_day_night_masks(ts):\n",
        "    \"\"\"Day: 06:00<=h<18:00; Night: otherwise.\"\"\"\n",
        "    valid = ts.notna()\n",
        "    hrs = ts.dt.hour\n",
        "    day_mask = valid & (hrs >= 6) & (hrs < 18)\n",
        "    night_mask = valid & ~day_mask\n",
        "    return day_mask, night_mask\n",
        "\n",
        "def _cluster_times(ts_list, max_interval_sec=60):\n",
        "    if not ts_list:\n",
        "        return []\n",
        "    clusters, current = [], [ts_list[0]]\n",
        "    for i in range(1, len(ts_list)):\n",
        "        if (ts_list[i] - ts_list[i-1]).total_seconds() <= max_interval_sec:\n",
        "            current.append(ts_list[i])\n",
        "        else:\n",
        "            clusters.append(current); current = [ts_list[i]]\n",
        "    clusters.append(current)\n",
        "    return clusters\n",
        "\n",
        "def _subset_meal_metrics(df, ts_series, max_interval_sec=60):\n",
        "    \"\"\"\n",
        "    Compute pellet/meal metrics for a subset given df and ts_series (Series).\n",
        "    Returns dict with keys:\n",
        "      '%MealPellets','%GrazingPellets','Pellets',\n",
        "      'NumMeals','AvgMealSize','AvgMealDuration','MealsPerHour','Accuracy'\n",
        "    \"\"\"\n",
        "    out = {\n",
        "        \"%MealPellets\": np.nan,\n",
        "        \"%GrazingPellets\": np.nan,\n",
        "        \"Pellets\": 0,\n",
        "        \"NumMeals\": np.nan,\n",
        "        \"AvgMealSize\": np.nan,\n",
        "        \"AvgMealDuration\": np.nan,\n",
        "        \"MealsPerHour\": np.nan,\n",
        "        \"Accuracy\": np.nan,\n",
        "    }\n",
        "    ev_col = _safe_col(df, [\"Event\", \"event\"])\n",
        "    if ev_col is None or df.empty:\n",
        "        return out\n",
        "\n",
        "    left_n = int((df[ev_col] == \"Left\").sum())\n",
        "    right_n = int((df[ev_col] == \"Right\").sum())\n",
        "    denom = left_n + right_n\n",
        "    if denom > 0:\n",
        "        out[\"Accuracy\"] = 100.0 * (left_n / denom)\n",
        "\n",
        "    pel_mask = (df[ev_col] == \"Pellet\")\n",
        "    pel_ts = ts_series[pel_mask].dropna().sort_values()\n",
        "    out[\"Pellets\"] = int(pel_ts.size)\n",
        "    if pel_ts.size == 0:\n",
        "        return out\n",
        "\n",
        "    ts_list = pel_ts.to_list()\n",
        "    clusters = _cluster_times(ts_list, max_interval_sec=max_interval_sec)\n",
        "    meal_clusters = [c for c in clusters if len(c) >= 3]\n",
        "    grazing_clusters = [c for c in clusters if 1 <= len(c) < 3]\n",
        "\n",
        "    meal_pellets = sum(len(c) for c in meal_clusters)\n",
        "    grazing_pellets = sum(len(c) for c in grazing_clusters)\n",
        "    total_pg = meal_pellets + grazing_pellets\n",
        "    if total_pg > 0:\n",
        "        out[\"%MealPellets\"] = 100.0 * meal_pellets / total_pg\n",
        "        out[\"%GrazingPellets\"] = 100.0 * grazing_pellets / total_pg\n",
        "\n",
        "    if len(meal_clusters) > 0:\n",
        "        out[\"NumMeals\"] = float(len(meal_clusters))\n",
        "        out[\"AvgMealSize\"] = float(np.mean([len(c) for c in meal_clusters]))\n",
        "        out[\"AvgMealDuration\"] = float(np.mean([(c[-1] - c[0]).total_seconds() for c in meal_clusters]))\n",
        "    else:\n",
        "        out[\"NumMeals\"] = 0.0\n",
        "\n",
        "    if len(ts_list) >= 2:\n",
        "        rec_hours = (ts_list[-1] - ts_list[0]).total_seconds() / 3600.0\n",
        "        if rec_hours > 0:\n",
        "            out[\"MealsPerHour\"] = (out[\"NumMeals\"] / rec_hours) if np.isfinite(out[\"NumMeals\"]) else np.nan\n",
        "    return out\n",
        "\n",
        "def _estimate_daily_pellets(df):\n",
        "    \"\"\"\n",
        "    24h-normalized pellet rate; prefer Pellet_Count if present (numeric non-NaN),\n",
        "    fallback to counting 'Pellet' events. Uses total span between first and last timestamp.\n",
        "    \"\"\"\n",
        "    # Need at least two timestamps\n",
        "    ts = _get_ts_series_from_col_or_index(df)\n",
        "    ts = ts.dropna().sort_values()\n",
        "    if ts.size < 2:\n",
        "        return np.nan\n",
        "    duration_hours = (ts.iloc[-1] - ts.iloc[0]).total_seconds() / 3600.0\n",
        "    if duration_hours <= 0:\n",
        "        return np.nan\n",
        "\n",
        "    pellet_events = np.nan\n",
        "    pc_col = _safe_col(df, [\"Pellet_Count\", \"pellet_count\"])\n",
        "    if pc_col is not None:\n",
        "        pc = pd.to_numeric(df[pc_col], errors='coerce')\n",
        "        if pc.notna().any():\n",
        "            diffs = pc.diff().fillna(0).clip(lower=0)\n",
        "            pellet_events = float(diffs.sum())\n",
        "            if pellet_events == 0 and pc.iloc[-1] >= pc.iloc[0]:\n",
        "                pellet_events = float(pc.iloc[-1] - pc.iloc[0])\n",
        "\n",
        "    if np.isnan(pellet_events):\n",
        "        ev_col = _safe_col(df, [\"Event\", \"event\"])\n",
        "        if ev_col is not None:\n",
        "            pellet_events = float((df[ev_col] == \"Pellet\").sum())\n",
        "\n",
        "    if np.isnan(pellet_events):\n",
        "        return np.nan\n",
        "    return (pellet_events / duration_hours) * 24.0\n",
        "\n",
        "# ---------------------------\n",
        "# Load sessions and normalize metadata_df\n",
        "# ---------------------------\n",
        "_sessions = _get_sessions()\n",
        "\n",
        "md = metadata_df.copy()\n",
        "if 'filename' not in md.columns:\n",
        "    raise ValueError(\"metadata_df must contain a 'filename' column.\")\n",
        "if 'Mouse_ID' not in md.columns:\n",
        "    raise ValueError(\"metadata_df must contain a 'Mouse_ID' column.\")\n",
        "md['filename'] = md['filename'].astype(str).map(_basename)\n",
        "md['Mouse_ID'] = md['Mouse_ID'].astype(str).str.strip()\n",
        "\n",
        "# ---------------------------\n",
        "# Core FR1 metrics per file (includes Daily_Pellets & LeftWithPellet)\n",
        "# ---------------------------\n",
        "rows = []\n",
        "for idx, c_df in enumerate(_sessions):\n",
        "    file_name = _basename(getattr(c_df, \"name\", f\"File_{idx}\"))\n",
        "    d, ev = _prep_events(c_df)\n",
        "\n",
        "    if ev is None or d.empty:\n",
        "        pellets = left = right = lwp = 0\n",
        "        rt_med = ipi_med = pt_med = np.nan\n",
        "    else:\n",
        "        pellets = int((d[ev] == \"Pellet\").sum())\n",
        "        left    = int((d[ev] == \"Left\").sum())\n",
        "        right   = int((d[ev] == \"Right\").sum())\n",
        "        # LeftWithPellet counted on the raw df to be safe\n",
        "        lwp     = int((c_df[ev] == \"LeftWithPellet\").sum()) if ev in c_df.columns else 0\n",
        "\n",
        "        rt_col  = _safe_col(d, [\"Retrieval_Time\", \"retrieval_time\"])\n",
        "        ipi_col = _safe_col(d, [\"InterPelletInterval\", \"interpelletinterval\", \"inter_pellet_interval\"])\n",
        "        pt_col  = _safe_col(d, [\"Poke_Time\", \"poke_time\"])\n",
        "\n",
        "        rt_med  = pd.to_numeric(d.get(rt_col, pd.Series(dtype=float)), errors=\"coerce\").median() if rt_col else np.nan\n",
        "        ipi_med = pd.to_numeric(d.get(ipi_col, pd.Series(dtype=float)), errors=\"coerce\").median() if ipi_col else np.nan\n",
        "        pt_med  = pd.to_numeric(d.get(pt_col, pd.Series(dtype=float)), errors=\"coerce\").median() if pt_col else np.nan\n",
        "\n",
        "    total_pokes = left + right\n",
        "    acc = (left / total_pokes * 100.0) if total_pokes > 0 else np.nan\n",
        "    ppp = (total_pokes / pellets) if pellets > 0 else np.nan\n",
        "\n",
        "    daily_pel = _estimate_daily_pellets(c_df)\n",
        "\n",
        "    rows.append({\n",
        "        \"File\": file_name,\n",
        "        \"FileIndex\": idx,\n",
        "        \"Pellets\": pellets,\n",
        "        \"Left_Poke\": left,\n",
        "        \"Right_Poke\": right,\n",
        "        \"Total_Pokes\": total_pokes,\n",
        "        \"Accuracy\": acc,\n",
        "        \"PokesPerPellet\": ppp,\n",
        "        \"RetrievalTime\": rt_med,\n",
        "        \"InterPelletInterval\": ipi_med,\n",
        "        \"PokeTime\": pt_med,\n",
        "        \"Daily_Pellets\": daily_pel,\n",
        "        \"Left Poke with Pellet\": lwp,\n",
        "    })\n",
        "\n",
        "FR1metrics = pd.DataFrame(rows)\n",
        "if FR1metrics.empty:\n",
        "    display(HTML(\"<b style='color:#b00'>No files to analyze.</b>\"))\n",
        "    raise SystemExit\n",
        "\n",
        "def compute_within_meal_mode_from_sessions(sessions, max_interval_sec=60, min_samples=5):\n",
        "    \"\"\"\n",
        "    For each session/file:\n",
        "      - take InterPelletInterval > 0 and <= max_interval_sec\n",
        "      - find the mode of the distribution in log10 space\n",
        "      - return peak converted back to seconds\n",
        "    Falls back to a log-hist peak if scipy is unavailable.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    try:\n",
        "        from scipy.stats import gaussian_kde\n",
        "        use_kde = True\n",
        "    except Exception:\n",
        "        use_kde = False\n",
        "\n",
        "    for i, df in enumerate(sessions):\n",
        "        file = _basename(getattr(df, \"name\", f\"File_{i}\"))\n",
        "        ipi_col = _safe_col(df, [\"InterPelletInterval\", \"interpelletinterval\", \"inter_pellet_interval\"])\n",
        "        if ipi_col is None or df.empty:\n",
        "            rows.append({\"File\": file, \"Within_meal_pellet_rate\": np.nan}); continue\n",
        "\n",
        "        vals = pd.to_numeric(df[ipi_col], errors=\"coerce\").dropna()\n",
        "        vals = vals[(vals > 0) & (vals <= max_interval_sec)]\n",
        "        if vals.size < min_samples:\n",
        "            rows.append({\"File\": file, \"Within_meal_pellet_rate\": np.nan}); continue\n",
        "\n",
        "        logv = np.log10(vals.values)\n",
        "\n",
        "        if use_kde:\n",
        "            kde = gaussian_kde(logv, bw_method=\"scott\")\n",
        "            xs  = np.linspace(logv.min(), logv.max(), 1000)\n",
        "            ys  = kde(xs)\n",
        "            peak_log10 = xs[np.argmax(ys)]\n",
        "        else:\n",
        "            # robust fallback: log-histogram mode\n",
        "            xs = np.linspace(logv.min(), logv.max(), 256)\n",
        "            hist, edges = np.histogram(logv, bins=xs)\n",
        "            centers = 0.5 * (edges[:-1] + edges[1:])\n",
        "            peak_log10 = centers[np.argmax(hist)]\n",
        "\n",
        "        peak_seconds = float(10.0 ** peak_log10)\n",
        "        rows.append({\"File\": file, \"Within_meal_pellet_rate\": peak_seconds})\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "# ---------------------------\n",
        "# Meal/grazing metrics (file-level)\n",
        "# ---------------------------\n",
        "def compute_meal_bout_metrics_from_sessions(sessions, max_interval_sec=60):\n",
        "    rows, recinfo = [], []\n",
        "    for i, df in enumerate(sessions):\n",
        "        file = _basename(getattr(df, \"name\", f\"File_{i}\"))\n",
        "        ts = _pellet_times_from_df(df)\n",
        "        if len(ts) < 2:\n",
        "            continue\n",
        "        duration_hr = (ts[-1] - ts[0]).total_seconds() / 3600.0\n",
        "        recinfo.append({\"File\": file, \"RecordingHours\": duration_hr})\n",
        "\n",
        "        clusters = _cluster_times(ts, max_interval_sec=max_interval_sec)\n",
        "        meal_id = 0\n",
        "        for c in clusters:\n",
        "            if len(c) >= 3:\n",
        "                rows.append({\n",
        "                    \"File\": file,\n",
        "                    \"MealID\": meal_id,\n",
        "                    \"MealSize\": len(c),\n",
        "                    \"MealDuration_sec\": (c[-1] - c[0]).total_seconds(),\n",
        "                })\n",
        "                meal_id += 1\n",
        "    meal_df = pd.DataFrame(rows)\n",
        "    rec_df  = pd.DataFrame(recinfo)\n",
        "    if meal_df.empty:\n",
        "        out = pd.DataFrame(columns=[\"File\",\"NumMeals\",\"AvgMealSize\",\"AvgMealDuration\",\"RecordingHours\",\"MealsPerHour\"])\n",
        "    else:\n",
        "        out = (meal_df.groupby(\"File\")\n",
        "               .agg(NumMeals=(\"MealID\",\"count\"), AvgMealSize=(\"MealSize\",\"mean\"), AvgMealDuration=(\"MealDuration_sec\",\"mean\"))\n",
        "               .reset_index())\n",
        "    out = out.merge(rec_df, on=\"File\", how=\"left\")\n",
        "    out[\"MealsPerHour\"] = out[\"NumMeals\"] / out[\"RecordingHours\"]\n",
        "    return out\n",
        "\n",
        "def compute_meal_pellet_distribution_from_sessions(sessions, max_interval_sec=60):\n",
        "    rows = []\n",
        "    for i, df in enumerate(sessions):\n",
        "        file = _basename(getattr(df, \"name\", f\"File_{i}\"))\n",
        "        ts = _pellet_times_from_df(df)\n",
        "        if len(ts) == 0:\n",
        "            rows.append({\"File\": file, \"%MealPellets\": np.nan, \"%GrazingPellets\": np.nan})\n",
        "            continue\n",
        "        clusters = _cluster_times(ts, max_interval_sec=max_interval_sec)\n",
        "        meal_pellets    = sum(len(c) for c in clusters if len(c) >= 3)\n",
        "        grazing_pellets = sum(len(c) for c in clusters if 1 <= len(c) < 3)\n",
        "        total = meal_pellets + grazing_pellets\n",
        "        rows.append({\n",
        "            \"File\": file,\n",
        "            \"%MealPellets\": (100 * meal_pellets / total) if total else np.nan,\n",
        "            \"%GrazingPellets\": (100 * grazing_pellets / total) if total else np.nan\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "meal_info_df   = compute_meal_bout_metrics_from_sessions(_sessions, max_interval_sec=60)\n",
        "pellet_dist_df = compute_meal_pellet_distribution_from_sessions(_sessions, max_interval_sec=60)\n",
        "\n",
        "FR1_enriched = (\n",
        "    FR1metrics\n",
        "    .merge(pellet_dist_df, on=\"File\", how=\"left\")\n",
        "    .merge(meal_info_df, on=\"File\", how=\"left\")\n",
        ")\n",
        "wmpr_df = compute_within_meal_mode_from_sessions(_sessions, max_interval_sec=60, min_samples=5)\n",
        "FR1_enriched = FR1_enriched.merge(wmpr_df, on=\"File\", how=\"left\")\n",
        "# ---------------------------\n",
        "# Day/Night metrics per file\n",
        "# ---------------------------\n",
        "_day_night_bases = [\"%MealPellets\",\"%GrazingPellets\",\"Pellets\",\"NumMeals\",\"AvgMealSize\",\"AvgMealDuration\",\"MealsPerHour\",\"Accuracy\"]\n",
        "for base in _day_night_bases:\n",
        "    FR1_enriched[f\"{base}_Day\"] = np.nan\n",
        "    FR1_enriched[f\"{base}_Night\"] = np.nan\n",
        "\n",
        "for i, df_all in enumerate(_sessions):\n",
        "    file = _basename(getattr(df_all, \"name\", f\"File_{i}\"))\n",
        "    ts_all = _get_ts_series_from_col_or_index(df_all)\n",
        "    day_mask, night_mask = _split_day_night_masks(ts_all)\n",
        "\n",
        "    day_df = df_all.loc[day_mask]\n",
        "    night_df = df_all.loc[night_mask]\n",
        "\n",
        "    day_vals = _subset_meal_metrics(day_df, ts_all.loc[day_mask])\n",
        "    night_vals = _subset_meal_metrics(night_df, ts_all.loc[night_mask])\n",
        "\n",
        "    row_mask = FR1_enriched[\"File\"] == file\n",
        "    for base in _day_night_bases:\n",
        "        FR1_enriched.loc[row_mask, f\"{base}_Day\"] = day_vals[base]\n",
        "        FR1_enriched.loc[row_mask, f\"{base}_Night\"] = night_vals[base]\n",
        "\n",
        "# ---------------------------\n",
        "# Attach metadata: Mouse_ID primary, filename fallback\n",
        "# ---------------------------\n",
        "# 1) Map filename -> Mouse_ID from metadata_df\n",
        "mouse_map = md.set_index('filename')['Mouse_ID']\n",
        "FR1_enriched['Mouse_ID'] = FR1_enriched['File'].map(mouse_map)\n",
        "\n",
        "# 2) Primary merge by Mouse_ID\n",
        "md_mouse_unique = md.drop_duplicates(subset=['Mouse_ID'], keep='first')\n",
        "fm = FR1_enriched.merge(md_mouse_unique, on='Mouse_ID', how='left', suffixes=('', '_md'))\n",
        "\n",
        "\n",
        "# 3) Fallback for rows still missing metadata -> merge by filename\n",
        "needs_fallback = fm['Mouse_ID'].isna() | fm.filter(regex='_md$').isna().all(axis=1)\n",
        "if needs_fallback.any():\n",
        "    md_file_unique = md.drop_duplicates(subset=['filename'], keep='first')\n",
        "    fm_fb = FR1_enriched.loc[needs_fallback].merge(\n",
        "        md_file_unique, left_on='File', right_on='filename', how='left', suffixes=('', '_md')\n",
        "    )\n",
        "    fm = pd.concat([fm.loc[~needs_fallback], fm_fb], ignore_index=True)\n",
        "\n",
        "# ---------------------------\n",
        "# Session-type suffixing (apply to ALL metrics)\n",
        "# ---------------------------\n",
        "# Include day/night columns plus the new metrics\n",
        "metric_cols_all = [\n",
        "    \"Pellets\", \"Left_Poke\", \"Right_Poke\", \"Total_Pokes\", \"Accuracy\",\n",
        "    \"PokesPerPellet\", \"RetrievalTime\", \"InterPelletInterval\", \"PokeTime\",\n",
        "    \"%MealPellets\", \"%GrazingPellets\", \"NumMeals\", \"AvgMealSize\",\n",
        "    \"AvgMealDuration\", \"RecordingHours\", \"MealsPerHour\",\n",
        "    \"Daily_Pellets\", \"Left Poke with Pellet\",\"Within_meal_pellet_rate\",\n",
        "    # Day/Night exact column names:\n",
        "    \"%MealPellets_Day\",\"%GrazingPellets_Day\",\"Pellets_Day\",\"NumMeals_Day\",\"AvgMealSize_Day\",\"AvgMealDuration_Day\",\"MealsPerHour_Day\",\"Accuracy_Day\",\n",
        "    \"%MealPellets_Night\",\"%GrazingPellets_Night\",\"Pellets_Night\",\"NumMeals_Night\",\"AvgMealSize_Night\",\"AvgMealDuration_Night\",\"MealsPerHour_Night\",\"Accuracy_Night\",\n",
        "]\n",
        "\n",
        "# Prefer metadata_df's Session_type; fallback to df.attrs or \"Unknown\"\n",
        "if 'Session_type' in fm.columns:\n",
        "    session_series = fm['Session_type'].astype(str).str.strip()\n",
        "else:\n",
        "    sess_map = {\n",
        "        _basename(getattr(_sessions[i], \"name\", f\"File_{i}\")):\n",
        "        (_sessions[i].attrs.get(\"Session_type\") or \"Unknown\")\n",
        "        for i in range(len(_sessions))\n",
        "    }\n",
        "    session_series = fm[\"File\"].map(sess_map).fillna(\"Unknown\").astype(str)\n",
        "\n",
        "session_series = session_series.str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "fm[\"_Session_type_for_csv\"] = session_series\n",
        "\n",
        "def with_session_suffix_for_csv(df, metrics=metric_cols_all, session_col=\"_Session_type_for_csv\"):\n",
        "    df = df.copy()\n",
        "    for m in metrics:\n",
        "        if m not in df.columns:\n",
        "            continue\n",
        "        for sess in df[session_col].dropna().unique():\n",
        "            mask = df[session_col] == sess\n",
        "            col_name = f\"{m}_{sess}\"\n",
        "            if col_name not in df.columns:\n",
        "                df[col_name] = np.nan\n",
        "            df.loc[mask, col_name] = df.loc[mask, m]\n",
        "        df.drop(columns=[m], inplace=True)\n",
        "    return df.drop(columns=[session_col])\n",
        "\n",
        "FR1metrics_merged = fm.copy()      # optional to inspect in notebook\n",
        "FR1metrics_csv    = with_session_suffix_for_csv(FR1metrics_merged)\n",
        "\n",
        "# ---------------------------\n",
        "# KEEP ONLY requested columns for CSV\n",
        "# ---------------------------\n",
        "def _norm_name(s: str) -> str:\n",
        "    # lowercase, strip non-alphanumerics (so \"Sex$\", \"Session type\" normalize nicely)\n",
        "    return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n",
        "\n",
        "# Meta targets we want to keep (prefix match allows *_md, etc.)\n",
        "meta_targets = [\"mouse_id\", \"filename\", \"session_type\", \"sex\", \"genotype\"]\n",
        "#meta_targets = [\"mouseid\", \"filename\", \"sessiontype\", \"sex\", \"genotype\",\n",
        "#                \"geneid\", \"fed3\", \"beam\"]\n",
        "\n",
        "# For each target, pick the best matching column:\n",
        "#   1) exact normalized == target (preferred)\n",
        "#   2) otherwise any column whose normalized name startswith(target)\n",
        "allowed_meta = []\n",
        "all_cols_norm = {c: _norm_name(c) for c in FR1metrics_csv.columns}\n",
        "for tgt in meta_targets:\n",
        "    exact = [c for c, k in all_cols_norm.items() if k == tgt]\n",
        "    if exact:\n",
        "        allowed_meta.append(exact[0])\n",
        "        continue\n",
        "    prefixed = [c for c, k in all_cols_norm.items() if k.startswith(tgt)]\n",
        "    if prefixed:\n",
        "        # Prefer the shortest normalized name (likely the least-suffixed)\n",
        "        prefixed.sort(key=lambda c: len(all_cols_norm[c]))\n",
        "        allowed_meta.append(prefixed[0])\n",
        "\n",
        "# Metric columns: base or any session-suffixed variant (e.g., Pellets_FR1)\n",
        "def _metric_match(col):\n",
        "    return any(col == base or col.startswith(base + \"_\") for base in metric_cols_all)\n",
        "\n",
        "metric_keep = [c for c in FR1metrics_csv.columns if _metric_match(c)]\n",
        "\n",
        "# Final selection (keep only the meta we found + metrics)\n",
        "keep_cols = [c for c in allowed_meta if c is not None] + metric_keep\n",
        "if not metric_keep:\n",
        "    raise RuntimeError(\"No metric columns matched; check 'metric_cols_all' and your column names.\")\n",
        "\n",
        "FR1metrics_csv = FR1metrics_csv.loc[:, keep_cols]\n",
        "\n",
        "# ---------------------------\n",
        "# Save & present\n",
        "# ---------------------------\n",
        "# Try to build filename from key: Gene_GeneID_FR1_L3.csv\n",
        "gene_col   = _safe_col(md, [\"Gene\", \"gene\"])\n",
        "geneid_col = _safe_col(md, [\"Gene_ID\", \"GeneID\", \"gene_id\", \"geneid\"])\n",
        "if gene_col is not None and geneid_col is not None:\n",
        "    # Extract unique values\n",
        "    gene_vals   = md[gene_col].dropna().astype(str).str.strip().unique()\n",
        "    geneid_vals = md[geneid_col].dropna().astype(str).str.strip().unique()\n",
        "    gene = gene_vals[0] if gene_vals.size > 0 else \"GENE\"\n",
        "    # Zero-pad Gene_ID  3 digits\n",
        "    if geneid_vals.size > 0:\n",
        "        raw_gene_id = geneid_vals[0]\n",
        "        # remove accidental non-digits but keep fallback behavior\n",
        "        cleaned = re.sub(r\"[^0-9]\", \"\", raw_gene_id)\n",
        "        if cleaned.isdigit():\n",
        "            gene_id = cleaned.zfill(3)   # <<< zero-padding here\n",
        "        else:\n",
        "            gene_id = raw_gene_id        # fallback if not numeric\n",
        "    else:\n",
        "        gene_id = \"000\"\n",
        "    # Build name: Gene_GeneID_FR1_L3\n",
        "    base_name = f\"{gene}_{gene_id}_FR1_L3\"\n",
        "    base_name = re.sub(r\"[^A-Za-z0-9_\\-]+\", \"_\", base_name)  # sanitize\n",
        "    fname = f\"{base_name}.csv\"\n",
        "else:\n",
        "    # Fallback to timestamp\n",
        "    fname = f\"FR1metrics_{datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
        "FR1metrics_csv.to_csv(fname, index=False)\n",
        "display(HTML(f\"<b> Saved FR1 metrics CSV (trimmed columns) to:</b> <code>{fname}</code>\"))\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "    btn = widgets.Button(description=f\"Download {os.path.basename(fname)}\", icon=\"download\",\n",
        "                         tooltip=\"Click to download the FR1 metrics CSV\")\n",
        "    status = widgets.HTML()\n",
        "    def _on_click(_):\n",
        "        clear_output(wait=True); display(btn, status)\n",
        "        try:\n",
        "            status.value = f\"Starting download: <code>{os.path.basename(fname)}</code>\"\n",
        "            gfiles.download(fname)\n",
        "        except Exception as e:\n",
        "            status.value = f\"<b style='color:#b00'>Download failed:</b> {e}\"\n",
        "    display(btn, status)\n",
        "    btn.on_click(_on_click)\n",
        "except Exception:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FmpnolCCKyW6"
      },
      "outputs": [],
      "source": [
        "# @title Group for plotting\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# --- sanity ---\n",
        "if 'metadata_df' not in globals() or metadata_df is None or metadata_df.empty:\n",
        "    raise RuntimeError(\"metadata_df is missing or empty. Build metadata_df (copy of Key_Df) first.\")\n",
        "\n",
        "EXCLUDE_LOWER = {\"match_status\"}   # everything else is allowed\n",
        "\n",
        "def _build_file_column(df):\n",
        "    if \"filename\" in df.columns:\n",
        "        return df[\"filename\"].apply(lambda p: os.path.basename(str(p)))\n",
        "    if \"FED3_from_file\" in df.columns and \"Date_from_file\" in df.columns:\n",
        "        return \"FED\" + df[\"FED3_from_file\"].astype(str) + \"_\" + df[\"Date_from_file\"].astype(str)\n",
        "    if \"FED3_from_file\" in df.columns:\n",
        "        return \"FED\" + df[\"FED3_from_file\"].astype(str)\n",
        "    return df.index.astype(str)\n",
        "\n",
        "def _norm_val(x):\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s.lower() in {\"nan\", \"none\"}:\n",
        "        return \"UNK\"\n",
        "    return s.upper()\n",
        "\n",
        "def _build_group_row(row, ordered_cols):\n",
        "    if not ordered_cols:\n",
        "        return \"ALL\"\n",
        "    return \" | \".join(_norm_val(row[c]) for c in ordered_cols)\n",
        "\n",
        "def build_mapping(ordered_cols):\n",
        "    _meta = metadata_df.copy()\n",
        "    _meta[\"File\"] = _build_file_column(_meta)\n",
        "    _meta[\"Group\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols), axis=1)\n",
        "    mapping = (\n",
        "        _meta[[\"File\", \"Group\"]]\n",
        "        .dropna(subset=[\"File\"])\n",
        "        .drop_duplicates()\n",
        "        .sort_values([\"Group\", \"File\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    return mapping\n",
        "\n",
        "def _unique_keep_order(seq):\n",
        "    seen = set(); out = []\n",
        "    for x in seq:\n",
        "        if x not in seen:\n",
        "            seen.add(x); out.append(x)\n",
        "    return out\n",
        "\n",
        "# ---------- UI (fixed sizes + grid) ----------\n",
        "PX_W = \"260px\"   # list box width\n",
        "PX_H = \"160px\"   # list box height\n",
        "BTN_W = \"160px\"  # button column width\n",
        "HDR_H = \"28px\"   # header cell height (consistent across all headers)\n",
        "\n",
        "title = widgets.HTML(\"<h3>Select columns to group by for X and Hue, then reorder X to set hierarchy</h3>\")\n",
        "\n",
        "all_cols = sorted((c for c in metadata_df.columns if str(c).lower() not in EXCLUDE_LOWER), key=str.lower)\n",
        "\n",
        "def header(text):\n",
        "    # Normalize header height/margins so they align perfectly in the grid row\n",
        "    return widgets.HTML(\n",
        "        f\"<div style='height:{HDR_H};display:flex;align-items:flex-end;'>\"\n",
        "        f\"<h4 style=\\\"margin:0;\\\">{text}</h4></div>\"\n",
        "    )\n",
        "\n",
        "# Headers (row 1 of grid)\n",
        "available_hdr = header(\"Available\")\n",
        "actions_hdr   = header(\"Actions\")\n",
        "x_hdr         = header(\"X grouping\")\n",
        "hue_hdr       = header(\"Hue grouping\")\n",
        "\n",
        "# Widgets (row 2 of grid)\n",
        "available = widgets.SelectMultiple(\n",
        "    options=all_cols, value=tuple(), rows=14,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "right_x = widgets.Select(\n",
        "    options=[], value=None, rows=8,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "right_hue = widgets.Select(\n",
        "    options=[], value=None, rows=8,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Buttons\n",
        "btn_add_x    = widgets.Button(description=\"Add to X \", button_style='primary', layout=widgets.Layout(width=BTN_W))\n",
        "btn_add_hue  = widgets.Button(description=\"Add to Hue \",button_style='primary', layout=widgets.Layout(width=BTN_W))\n",
        "btn_clear    = widgets.Button(description=\"Clear\", button_style='danger', layout=widgets.Layout(width=BTN_W))\n",
        "btn_up       = widgets.Button(description=\" Up (X only)\", layout=widgets.Layout(width=BTN_W))\n",
        "btn_down     = widgets.Button(description=\" Down (X only)\", layout=widgets.Layout(width=BTN_W))\n",
        "btn_build    = widgets.Button(description=\"Build Groups\", button_style='success', layout=widgets.Layout(width=\"160px\"))\n",
        "\n",
        "controls_col = widgets.VBox(\n",
        "    [btn_add_x, btn_add_hue, btn_clear, btn_up, btn_down],\n",
        "    layout=widgets.Layout(\n",
        "        align_items=\"center\",\n",
        "        width=BTN_W, min_width=BTN_W, max_width=BTN_W,\n",
        "        height=PX_H, min_height=PX_H, max_height=PX_H,\n",
        "        flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "btn_build = widgets.Button(description=\"Build Groups\", button_style='success', layout=widgets.Layout(width=\"160px\"))\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- Callbacks ---\n",
        "def on_add_x(_):\n",
        "    sel = list(available.value)\n",
        "    if not sel: return\n",
        "    new_opts = _unique_keep_order(list(right_x.options) + sel)\n",
        "    right_x.value = None\n",
        "    right_x.options = new_opts\n",
        "    right_x.value = new_opts[-1] if new_opts else None\n",
        "\n",
        "def on_add_hue(_):\n",
        "    sel = list(available.value)\n",
        "    if not sel: return\n",
        "    new_opts = _unique_keep_order(list(right_hue.options) + sel)\n",
        "    right_hue.value = None\n",
        "    right_hue.options = new_opts\n",
        "    right_hue.value = new_opts[-1] if new_opts else None\n",
        "\n",
        "def on_clear(_):\n",
        "    right_x.value = None; right_x.options = []\n",
        "    right_hue.value = None; right_hue.options = []\n",
        "\n",
        "def on_up(_):\n",
        "    item = right_x.value\n",
        "    if item is None: return\n",
        "    opts = list(right_x.options)\n",
        "    i = opts.index(item)\n",
        "    if i > 0:\n",
        "        opts[i-1], opts[i] = opts[i], opts[i-1]\n",
        "        right_x.value = None; right_x.options = opts; right_x.value = item\n",
        "\n",
        "def on_down(_):\n",
        "    item = right_x.value\n",
        "    if item is None: return\n",
        "    opts = list(right_x.options)\n",
        "    i = opts.index(item)\n",
        "    if i < len(opts) - 1:\n",
        "        opts[i+1], opts[i] = opts[i], opts[i+1]\n",
        "        right_x.value = None; right_x.options = opts; right_x.value = item\n",
        "\n",
        "def on_build(_):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        ordered_cols_x = list(right_x.options)\n",
        "        ordered_cols_hue = list(right_hue.options)\n",
        "\n",
        "        mapping_x = build_mapping(ordered_cols_x)\n",
        "        mapping_hue = build_mapping(ordered_cols_hue)\n",
        "\n",
        "        _meta = metadata_df.copy()\n",
        "        _meta[\"File\"] = _build_file_column(_meta)\n",
        "        _meta[\"XGroup\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols_x), axis=1)\n",
        "        _meta[\"HueGroup\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols_hue), axis=1)\n",
        "        mapping_both = (\n",
        "            _meta[[\"File\", \"XGroup\", \"HueGroup\"]]\n",
        "            .dropna(subset=[\"File\"])\n",
        "            .drop_duplicates()\n",
        "            .sort_values([\"XGroup\", \"HueGroup\", \"File\"])\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        globals()['files_to_group_x'] = mapping_x.copy()\n",
        "        globals()['files_to_group_hue'] = mapping_hue.copy()\n",
        "        globals()['files_to_group_both'] = mapping_both.copy()\n",
        "        globals()['selected_group_cols_x'] = ordered_cols_x.copy()\n",
        "        globals()['selected_group_cols_hue'] = ordered_cols_hue.copy()\n",
        "\n",
        "        print(\"X-axis grouping (hierarchy):\", ordered_cols_x if ordered_cols_x else [\"ALL\"])\n",
        "        print(f\"Total unique files (X map): {mapping_x['File'].nunique()}\")\n",
        "        display(widgets.HTML(\"<b>X-group summary</b>\"))\n",
        "        display((mapping_x.groupby(\"Group\", dropna=False)[\"File\"]\n",
        "                 .nunique().sort_values(ascending=False)\n",
        "                 .rename(\"UniqueFiles\").to_frame()))\n",
        "\n",
        "        print(\"\\nHue grouping:\", ordered_cols_hue if ordered_cols_hue else [\"ALL\"])\n",
        "        print(f\"Total unique files (Hue map): {mapping_hue['File'].nunique()}\")\n",
        "        display(widgets.HTML(\"<b>Hue-group summary</b>\"))\n",
        "        display((mapping_hue.groupby(\"Group\", dropna=False)[\"File\"]\n",
        "                 .nunique().sort_values(ascending=False)\n",
        "                 .rename(\"UniqueFiles\").to_frame()))\n",
        "        print(\"\\nCombined mapping available as `files_to_group_both` (File, XGroup, HueGroup)\")\n",
        "\n",
        "# Wire up\n",
        "btn_add_x.on_click(on_add_x)\n",
        "btn_add_hue.on_click(on_add_hue)\n",
        "btn_clear.on_click(on_clear)\n",
        "btn_up.on_click(on_up)\n",
        "btn_down.on_click(on_down)\n",
        "btn_build.on_click(on_build)\n",
        "\n",
        "# ----- Grid layout -----\n",
        "grid = widgets.GridBox(\n",
        "    children=[\n",
        "        available_hdr, actions_hdr, x_hdr, hue_hdr,     # row 1: headers\n",
        "        available,     controls_col, right_x, right_hue # row 2: widgets\n",
        "    ],\n",
        "    layout=widgets.Layout(\n",
        "        grid_template_columns=f\"{PX_W} {BTN_W} {PX_W} {PX_W}\",\n",
        "        grid_template_rows=\"auto auto\",\n",
        "        grid_gap=\"6px 16px\",\n",
        "        align_items=\"flex-start\",\n",
        "        justify_items=\"flex-start\",\n",
        "        width=\"100%\"\n",
        "    )\n",
        ")\n",
        "\n",
        "ui = widgets.VBox([title, grid, widgets.HBox([btn_build]), output])\n",
        "display(ui)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKKkCThnHE0H",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Plot metrics!\n",
        "\n",
        "\n",
        "import os, time, shutil, re, itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Stats\n",
        "import pingouin as pg\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Optional Colab download\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "except Exception:\n",
        "    colab_files = None\n",
        "\n",
        "ALPHA = 0.6  # apply to both bars and dots\n",
        "\n",
        "# -----------------------\n",
        "# 0) Preconditions & source\n",
        "# -----------------------\n",
        "if 'FR1metrics_merged' in globals() and FR1metrics_merged is not None and not FR1metrics_merged.empty:\n",
        "    bm = FR1metrics_merged.copy()\n",
        "elif 'FR1metrics' in globals() and FR1metrics is not None and not FR1metrics.empty:\n",
        "    bm = FR1metrics.copy()\n",
        "elif 'FR1metrics_csv' in globals() and FR1metrics_csv is not None and not FR1metrics_csv.empty:\n",
        "    bm = FR1metrics_csv.copy()\n",
        "else:\n",
        "    raise RuntimeError(\"No FR1 metrics DataFrame found: expected one of FR1metrics_merged, FR1metrics, FR1metrics_csv.\")\n",
        "\n",
        "if \"filename\" not in bm.columns:\n",
        "    if \"File\" in bm.columns:\n",
        "        bm[\"filename\"] = bm[\"File\"].astype(str)\n",
        "    else:\n",
        "        raise RuntimeError(\"Metrics table must include a 'filename' column.\")\n",
        "\n",
        "# -----------------------\n",
        "# Merge in XGroup/HueGroup from grouping widget\n",
        "# -----------------------\n",
        "def _basename_col(s):\n",
        "    return os.path.basename(str(s))\n",
        "\n",
        "def _src_name(df):\n",
        "    if \"filename\" in df.columns:\n",
        "        return \"filename\"\n",
        "    if \"File\" in df.columns:\n",
        "        return \"File\"\n",
        "    return None\n",
        "\n",
        "if (\"XGroup\" not in bm.columns) or (\"HueGroup\" not in bm.columns):\n",
        "    grp_both = globals().get('files_to_group_both', None)\n",
        "    if grp_both is not None and not grp_both.empty:\n",
        "        m = grp_both.copy()\n",
        "        m_src = _src_name(m)\n",
        "        if m_src is None:\n",
        "            raise RuntimeError(\"Grouping table must include 'filename' or 'File'.\")\n",
        "        m[\"file_base\"]  = m[m_src].astype(str).apply(_basename_col)\n",
        "        bm[\"file_base\"] = bm[\"filename\"].astype(str).apply(_basename_col)\n",
        "        bm = bm.merge(m[[\"file_base\",\"XGroup\",\"HueGroup\"]],\n",
        "                      on=\"file_base\", how=\"left\").drop(columns=[\"file_base\"])\n",
        "        bm[\"XGroup\"]   = bm[\"XGroup\"].fillna(\"UNASSIGNED\")\n",
        "        bm[\"HueGroup\"] = bm[\"HueGroup\"].fillna(\"UNASSIGNED\")\n",
        "    else:\n",
        "        raise RuntimeError(\"Missing X/Hue mapping. Build Groups first (two-column version).\")\n",
        "\n",
        "# -----------------------\n",
        "# 1) Melt to long format\n",
        "# -----------------------\n",
        "# Metric bases we support (any session suffixes or _Day/_Night variants are included by startswith)\n",
        "base_metric_names = [\n",
        "    \"Pellets\", \"Left_Poke\", \"Right_Poke\", \"Total_Pokes\", \"Accuracy\",\n",
        "    \"PokesPerPellet\", \"RetrievalTime\", \"InterPelletInterval\", \"PokeTime\",\n",
        "    \"%MealPellets\", \"%GrazingPellets\", \"NumMeals\", \"AvgMealSize\",\n",
        "    \"AvgMealDuration\", \"RecordingHours\", \"MealsPerHour\",\n",
        "    \"Daily_Pellets\", \"Left Poke with Pellet\",\"Within_meal_pellet_rate\",\n",
        "    # Day/Night bases (these appear as <base>_Day or <base>_Night)\n",
        "    \"%MealPellets\", \"%GrazingPellets\", \"Pellets\", \"NumMeals\", \"AvgMealSize\", \"AvgMealDuration\", \"MealsPerHour\", \"Accuracy\",\n",
        "]\n",
        "\n",
        "# Collect all numeric metric columns that match the bases or their suffixed variants\n",
        "metric_cols = []\n",
        "seen = set()\n",
        "for c in bm.columns:\n",
        "    if not pd.api.types.is_numeric_dtype(bm[c]):\n",
        "        continue\n",
        "    for base in base_metric_names:\n",
        "        if c == base or c.startswith(base + \"_\"):\n",
        "            if c not in seen:\n",
        "                metric_cols.append(c)\n",
        "                seen.add(c)\n",
        "            break\n",
        "\n",
        "if not metric_cols:\n",
        "    raise RuntimeError(\"No numeric metric columns found among expected FR1 metrics.\")\n",
        "\n",
        "candidate_id_vars = [\"Genotype\",\"Sex\",\"Strain\",\"Start_Date\",\"filename\",\"Mouse_ID\",\"Session_type\",\"XGroup\",\"HueGroup\"]\n",
        "id_vars = [c for c in candidate_id_vars if c in bm.columns]\n",
        "for need in [\"XGroup\",\"HueGroup\",\"filename\"]:\n",
        "    if need not in id_vars:\n",
        "        id_vars.append(need)\n",
        "\n",
        "long_df = pd.melt(\n",
        "    bm,\n",
        "    id_vars=id_vars,\n",
        "    value_vars=metric_cols,\n",
        "    var_name=\"variable\",\n",
        "    value_name=\"value\"\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 2) Ordering helpers (hierarchical)\n",
        "# -----------------------\n",
        "# 2) Ordering helpers (hierarchical)\n",
        "# -----------------------\n",
        "def _is_wt_group(g):\n",
        "    \"\"\"\n",
        "    Returns True if the label includes any wildtype/control alias\n",
        "    (case-insensitive, robust to punctuation and composites).\n",
        "    \"\"\"\n",
        "    u = str(g).strip().upper()\n",
        "    tokens = [t for t in re.split(r'[^A-Z0-9]+', u) if t]\n",
        "    WT_ALIASES = {\"WT\", \"WILDTYPE\", \"CONTROL\", \"CTRL\"}  # add more if needed\n",
        "    return any(t in WT_ALIASES for t in tokens)\n",
        "\n",
        "def _hier_sort_key(g):\n",
        "    lv = _x_levels(g)\n",
        "    norm = []\n",
        "    for tok in lv:\n",
        "        is_blank = 1 if _is_unassigned_token(tok) else 0\n",
        "        norm.append((is_blank, str(tok).upper()))\n",
        "    wt_present = any(_is_wt_group(tok) for tok in lv) or _is_wt_group(g)\n",
        "    wt_rank = 0 if wt_present else 1\n",
        "    # make WT/CONTROL primary\n",
        "    return (wt_rank,) + tuple(norm) + (str(g).upper(),)\n",
        "\n",
        "def _is_unassigned_token(s):\n",
        "    return (str(s).strip().upper() in {\"\", \"UNASSIGNED\", \"NONE\", \"NA\", \"N/A\"})\n",
        "\n",
        "def _x_levels(xname):\n",
        "    s = str(xname)\n",
        "    parts = [p.strip() for p in s.split(\"|\")]\n",
        "    wanted = globals().get(\"selected_group_cols_x\", None)\n",
        "    if isinstance(wanted, (list, tuple)) and wanted:\n",
        "        if len(parts) < len(wanted):\n",
        "            parts += [\"\"] * (len(wanted) - len(parts))\n",
        "        else:\n",
        "            parts = parts[:len(wanted)]\n",
        "    return parts\n",
        "\n",
        "def _order_x_groups(groups):\n",
        "    return sorted(groups, key=_hier_sort_key)\n",
        "\n",
        "def _choose_ref_group(order):\n",
        "    for g in order:\n",
        "        if _is_wt_group(g):\n",
        "            return g\n",
        "    return order[0] if order else None\n",
        "\n",
        "all_x = [g for g in sorted(long_df[\"XGroup\"].dropna().unique().tolist()) if g != \"UNASSIGNED\"] or [\"UNASSIGNED\"]\n",
        "ordered_x = _order_x_groups(all_x)\n",
        "\n",
        "# -----------------------\n",
        "# 3) Controls (left column: groups & colors)\n",
        "# -----------------------\n",
        "named_defaults = [\n",
        "    \"blue\", \"orange\", \"green\", \"red\", \"purple\",\n",
        "    \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"\n",
        "]\n",
        "\n",
        "x_checks, x_colors = {}, {}\n",
        "group_rows = []\n",
        "for i, g in enumerate(ordered_x):\n",
        "    chk = widgets.Checkbox(value=True, description=g, indent=False, layout=widgets.Layout(width=\"260px\"))\n",
        "    col = widgets.Text(value=named_defaults[i % len(named_defaults)],\n",
        "                       layout=widgets.Layout(width=\"120px\"))\n",
        "    x_checks[g] = chk\n",
        "    x_colors[g] = col\n",
        "    group_rows.append(widgets.HBox([chk, widgets.Label(\"\"), col],\n",
        "                                   layout=widgets.Layout(align_items=\"center\", height=\"28px\")))\n",
        "\n",
        "picker = widgets.VBox(group_rows, layout=widgets.Layout(gap=\"2px\"))\n",
        "\n",
        "btn_all  = widgets.Button(description=\"Select all\", layout=widgets.Layout(width=\"140px\"))\n",
        "btn_none = widgets.Button(description=\"Clear\", layout=widgets.Layout(width=\"140px\"))\n",
        "def _set_all(val):\n",
        "    for c in x_checks.values(): c.value = val\n",
        "btn_all.on_click(lambda _: _set_all(True))\n",
        "btn_none.on_click(lambda _: _set_all(False))\n",
        "\n",
        "picker_container = widgets.Box([picker],\n",
        "    layout=widgets.Layout(overflow=\"auto\", max_height=\"420px\",\n",
        "                          border=\"1px solid #ddd\", padding=\"6px\", width=\"360px\"))\n",
        "\n",
        "left_col = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Groups & Colors</b>\"),\n",
        "    widgets.HBox([btn_all, btn_none], layout=widgets.Layout(gap=\"8px\")),\n",
        "    picker_container\n",
        "], layout=widgets.Layout(width=\"380px\"))\n",
        "\n",
        "# -----------------------\n",
        "# 4) Comparison controls (right column)\n",
        "# -----------------------\n",
        "mode_radio = widgets.ToggleButtons(\n",
        "    options=[(\"Reference group\", \"ref\"), (\"Select Pairs\", \"pairs\")],\n",
        "    value=\"ref\", description=\"\", style={\"button_width\":\"150px\"},\n",
        "    layout=widgets.Layout(width=\"320px\")\n",
        ")\n",
        "\n",
        "ref_dropdown = widgets.Dropdown(\n",
        "    options=ordered_x, value=_choose_ref_group(ordered_x),\n",
        "    description=\"Reference:\", layout=widgets.Layout(width=\"320px\")\n",
        ")\n",
        "\n",
        "def _pair_label(a,b): return f\"{a}  {b}\"\n",
        "def _pair_value(a,b): return (a,b) if a <= b else (b,a)\n",
        "\n",
        "pairs_select = widgets.SelectMultiple(\n",
        "    options=[], value=[], description=\"Pairs\",\n",
        "    layout=widgets.Layout(width=\"360px\", height=\"320px\")\n",
        ")\n",
        "\n",
        "def _selected_x():\n",
        "    return _order_x_groups([g for g, cb in x_checks.items() if cb.value])\n",
        "\n",
        "def _pair_sort_key(a, b):\n",
        "    \"\"\"Rank pairs by level of first difference: later differences (within-group) rank first.\"\"\"\n",
        "    A = _x_levels(a); B = _x_levels(b)\n",
        "    L = max(len(A), len(B))\n",
        "    if len(A) < L: A += [\"\"] * (L - len(A))\n",
        "    if len(B) < L: B += [\"\"] * (L - len(B))\n",
        "    # find first index where the two labels differ; default to L if no diff\n",
        "    first_diff = next((i for i, (xa, xb) in enumerate(zip(A, B)) if xa != xb), L)\n",
        "    prefix = tuple(A[:first_diff])\n",
        "    return (-first_diff, prefix, tuple(A), tuple(B))\n",
        "\n",
        "def _update_ref_and_pairs(*_):\n",
        "    sel = _selected_x()\n",
        "    ref_dropdown.options = sel or [\"\"]\n",
        "    if sel:\n",
        "        if ref_dropdown.value not in sel:\n",
        "            ref_dropdown.value = _choose_ref_group(sel)\n",
        "    else:\n",
        "        ref_dropdown.value = None\n",
        "\n",
        "    opts = []\n",
        "    for a, b in itertools.combinations(sel, 2):\n",
        "        lbl = _pair_label(a, b)\n",
        "        val = _pair_value(a, b)\n",
        "        opts.append((lbl, val))\n",
        "    # Keep more within-family comparisons first\n",
        "    opts.sort(key=lambda kv: _pair_sort_key(*kv[1]))\n",
        "    pairs_select.options = opts\n",
        "\n",
        "for cb in x_checks.values():\n",
        "    cb.observe(_update_ref_and_pairs, names=\"value\")\n",
        "_update_ref_and_pairs()\n",
        "\n",
        "plot_btn = widgets.Button(description=\"Plot\", button_style=\"primary\",\n",
        "                          layout=widgets.Layout(width=\"160px\"))\n",
        "save_btn = widgets.Button(description=\"Save Plots\", button_style=\"success\",\n",
        "                          layout=widgets.Layout(width=\"160px\"))\n",
        "\n",
        "right_col = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Statistical comparisons</b>\"),\n",
        "    mode_radio,\n",
        "    ref_dropdown,\n",
        "    pairs_select,\n",
        "    widgets.HBox([plot_btn, save_btn], layout=widgets.Layout(gap=\"8px\"))\n",
        "], layout=widgets.Layout(width=\"360px\"))\n",
        "\n",
        "# -----------------------\n",
        "# 5) Labels for stats/legend\n",
        "# -----------------------\n",
        "def _grouping_label(which=\"X\"):\n",
        "    if which.lower().startswith(\"x\"):\n",
        "        cols = globals().get(\"selected_group_cols_x\", [])\n",
        "        default = \"XGroup\"\n",
        "    else:\n",
        "        cols = globals().get(\"selected_group_cols_hue\", [])\n",
        "        default = \"HueGroup\"\n",
        "    cols = [str(c).strip() for c in (cols or []) if str(c).strip()]\n",
        "    return \" | \".join(cols) if cols else default\n",
        "\n",
        "# -----------------------\n",
        "# 6) Stats helpers (ANOVA with Hue)\n",
        "# -----------------------\n",
        "def _fmt_p(p):\n",
        "    if not np.isfinite(p): return \"n/a\"\n",
        "    return f\"p = {p:.3f}\" if p >= 0.001 else \"p < 0.001\"\n",
        "\n",
        "def _anova_subset(df):\n",
        "    out = {\"p_x\": np.nan, \"p_h\": np.nan, \"p_int\": np.nan, \"n_h\": 0, \"ok\": False, \"err\": None}\n",
        "    d = df.dropna(subset=[\"value\",\"XGroup\"])\n",
        "    if d.empty or d[\"XGroup\"].nunique() < 2:\n",
        "        out[\"err\"] = \"Too few groups\"; return out\n",
        "    n_h = d[\"HueGroup\"].nunique(dropna=True); out[\"n_h\"] = n_h\n",
        "    try:\n",
        "        if n_h >= 2:\n",
        "            model = ols('value ~ C(XGroup) + C(HueGroup) + C(XGroup):C(HueGroup)', data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            out[\"p_x\"]   = float(an.loc['C(XGroup)','PR(>F)'])\n",
        "            out[\"p_h\"]   = float(an.loc['C(HueGroup)','PR(>F)'])\n",
        "            out[\"p_int\"] = float(an.loc['C(XGroup):C(HueGroup)','PR(>F)'])\n",
        "            out[\"ok\"] = True\n",
        "        else:\n",
        "            model = ols('value ~ C(XGroup)', data=d).fit()\n",
        "            out[\"p_x\"] = float(model.f_pvalue); out[\"ok\"] = True\n",
        "    except Exception as e:\n",
        "        out[\"err\"] = str(e)\n",
        "    return out\n",
        "\n",
        "def _stats_text(dfm, x_label, hue_label, *, mode=\"ref\", ref_group=None, pair_list=None):\n",
        "    df = dfm.dropna(subset=[\"value\"]).copy()\n",
        "    g_n = df[\"XGroup\"].nunique(dropna=True)\n",
        "    h_n = df[\"HueGroup\"].nunique(dropna=True)\n",
        "\n",
        "    if mode == \"pairs\" and pair_list:\n",
        "        lines = [\"Selected pairwise ANOVA tests:\"]\n",
        "        for a,b in pair_list:\n",
        "            sub = df[df[\"XGroup\"].isin([a,b])]\n",
        "            res = _anova_subset(sub)\n",
        "            if not res[\"ok\"]:\n",
        "                lines.append(f\"{a} vs {b}: {res['err'] or 'failed'}\"); continue\n",
        "            if res[\"n_h\"] >= 2:\n",
        "                lines.append(\n",
        "                    f\"{a} vs {b} (Two-way: {x_label}, {hue_label})  \"\n",
        "                    f\"{x_label}: {_fmt_p(res['p_x'])} | {hue_label}: {_fmt_p(res['p_h'])} | \"\n",
        "                    f\"{x_label}{hue_label}: {_fmt_p(res['p_int'])}\"\n",
        "                )\n",
        "            else:\n",
        "                lines.append(f\"{a} vs {b} (One-way {x_label}): {_fmt_p(res['p_x'])}\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def fmt(p): return _fmt_p(p)\n",
        "    if g_n == 2 and h_n <= 1:\n",
        "        g1, g2 = sorted(df[\"XGroup\"].unique())\n",
        "        v1 = df[df[\"XGroup\"] == g1][\"value\"].dropna()\n",
        "        v2 = df[df[\"XGroup\"] == g2][\"value\"].dropna()\n",
        "        if len(v1) > 1 and len(v2) > 1:\n",
        "            p = pg.ttest(v1, v2, paired=False)[\"p-val\"].values[0]\n",
        "            return f\"t-test ({x_label}): {fmt(p)}\\n{g1} vs {g2}\"\n",
        "        return \"t-test: not enough data\"\n",
        "\n",
        "    if g_n >= 2 and h_n >= 2:\n",
        "        try:\n",
        "            model = ols('value ~ C(XGroup) + C(HueGroup) + C(XGroup):C(HueGroup)', data=df).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            return (\n",
        "                \"Two-way ANOVA\\n\"\n",
        "                f\"{x_label}: {fmt(float(an.loc['C(XGroup)','PR(>F)']))}\\n\"\n",
        "                f\"{hue_label}: {fmt(float(an.loc['C(HueGroup)','PR(>F)']))}\\n\"\n",
        "                f\"{x_label}{hue_label}: {fmt(float(an.loc['C(XGroup):C(HueGroup)','PR(>F)']))}\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            return f\"ANOVA failed: {e}\"\n",
        "\n",
        "    if g_n >= 2:\n",
        "        try:\n",
        "            model = ols('value ~ C(XGroup)', data=df).fit()\n",
        "            return f\"One-way ANOVA ({x_label}): {fmt(float(model.f_pvalue))}\"\n",
        "        except Exception as e:\n",
        "            return f\"One-way ANOVA failed: {e}\"\n",
        "    return \"Too few groups for stats\"\n",
        "\n",
        "# -----------------------\n",
        "# 7) Plotting helpers\n",
        "# -----------------------\n",
        "def _p_to_stars(p):\n",
        "    if not np.isfinite(p): return \"\"\n",
        "    if p < 1e-4: return \"****\"\n",
        "    if p < 1e-3: return \"***\"\n",
        "    if p < 1e-2: return \"**\"\n",
        "    if p < 5e-2: return \"*\"\n",
        "    return \"\"\n",
        "\n",
        "def _dot_palette(hues):\n",
        "    hues = list(hues)\n",
        "    if len(hues) == 0: return {}\n",
        "    if len(hues) == 1: return {hues[0]: \"black\"}\n",
        "    if len(hues) == 2: return {hues[0]: \"white\", hues[1]: \"black\"}\n",
        "    defaults = plt.rcParams.get('axes.prop_cycle', None)\n",
        "    colors = defaults.by_key()['color'] if defaults else [\"C0\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\"]\n",
        "    return {h: colors[i % len(colors)] for i, h in enumerate(hues)}\n",
        "\n",
        "def _draw_bracket(ax, x1, x2, y, h, text):\n",
        "    ax.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=\"black\", zorder=5)\n",
        "    ax.text((x1+x2)/2, y+h, text, ha=\"center\", va=\"bottom\", fontsize=16, fontweight=\"bold\")\n",
        "\n",
        "def _order_x_groups(groups):\n",
        "    return sorted(groups, key=_hier_sort_key)\n",
        "\n",
        "def _choose_ref_group(order):\n",
        "    for g in order:\n",
        "        if _is_wt_group(g):\n",
        "            return g\n",
        "    return order[0] if order else None\n",
        "\n",
        "def _plot_metric_clean(df_metric, variable, x_color_map, *, mode=\"ref\", ref_group=None, pair_list=None, return_fig=False):\n",
        "    dfm = df_metric.copy()\n",
        "    order = _order_x_groups(dfm[\"XGroup\"].dropna().unique().tolist())\n",
        "    if not order: return None\n",
        "    if (not ref_group) or (ref_group not in order):\n",
        "        ref_group = _choose_ref_group(order)\n",
        "\n",
        "    x_label_name   = _grouping_label(\"X\")\n",
        "    hue_label_name = _grouping_label(\"Hue\")\n",
        "\n",
        "    hue_levels = [h for h in dfm[\"HueGroup\"].dropna().unique().tolist()]\n",
        "    pal_dots = _dot_palette(hue_levels)\n",
        "\n",
        "    width = max(2, 1 * len(order))\n",
        "    height = 4.0\n",
        "    fig, (ax_plot, ax_text) = plt.subplots(\n",
        "        1, 2, figsize=(width/1.2, height), gridspec_kw={'width_ratios': [3, 1]}\n",
        "    )\n",
        "\n",
        "    # Bars\n",
        "    bar_palette = [x_color_map.get(g, \"tab:blue\") for g in order]\n",
        "    sns.barplot(data=dfm, x=\"XGroup\", y=\"value\", order=order, ci=None, alpha=ALPHA, ax=ax_plot, palette=bar_palette)\n",
        "\n",
        "    # Points\n",
        "    sns.stripplot(\n",
        "        data=dfm, x=\"XGroup\", y=\"value\", order=order, hue=\"HueGroup\",\n",
        "        jitter=True, dodge=False, size=7, edgecolor=\"black\", linewidth=1,\n",
        "        palette=pal_dots, ax=ax_plot, zorder=3, alpha=ALPHA\n",
        "    )\n",
        "    if ax_plot.legend_ is not None:\n",
        "        ax_plot.legend_.remove()\n",
        "\n",
        "    plt.rcParams.update({\n",
        "      \"xtick.bottom\": True,\n",
        "      \"ytick.left\": True,\n",
        "      \"xtick.direction\": \"out\",\n",
        "      \"ytick.direction\": \"out\",\n",
        "    })\n",
        "    # Annotations\n",
        "    y_min, y_max = ax_plot.get_ylim()\n",
        "    span = (y_max - y_min) if y_max > y_min else 1.0\n",
        "    bump = 0.06 * span\n",
        "    data_max = dfm[\"value\"].max() if dfm[\"value\"].notna().any() else y_max\n",
        "\n",
        "    if mode == \"ref\" and (ref_group in order):\n",
        "        ref_vals = dfm[dfm[\"XGroup\"] == ref_group][\"value\"].dropna().to_numpy()\n",
        "        for g in order:\n",
        "            if g == ref_group: continue\n",
        "            vals = dfm[dfm[\"XGroup\"] == g][\"value\"].dropna().to_numpy()\n",
        "            if len(vals) >= 2 and len(ref_vals) >= 2:\n",
        "                try:\n",
        "                    p = float(pg.ttest(vals, ref_vals, paired=False)[\"p-val\"].values[0])\n",
        "                except Exception:\n",
        "                    p = np.nan\n",
        "                if np.isfinite(p) and p < 0.05:\n",
        "                    xloc = order.index(g)\n",
        "                    gmax = dfm[dfm[\"XGroup\"] == g][\"value\"].max()\n",
        "                    y_star = (gmax if np.isfinite(gmax) else data_max) + bump\n",
        "                    ax_plot.text(xloc, y_star, _p_to_stars(p),\n",
        "                                 ha=\"center\", va=\"bottom\", fontsize=16, fontweight=\"bold\")\n",
        "                    y_max = max(y_max, y_star + bump)\n",
        "        ax_plot.set_ylim(y_min, y_max)\n",
        "\n",
        "    elif mode == \"pairs\" and pair_list:\n",
        "        base = (dfm[\"value\"].max() if dfm[\"value\"].notna().any() else y_max) + bump\n",
        "        step = 0.12 * span\n",
        "        k = 0\n",
        "        for a,b in pair_list:\n",
        "            if (a not in order) or (b not in order):\n",
        "                continue\n",
        "            sub = dfm[dfm[\"XGroup\"].isin([a,b])].dropna(subset=[\"value\"])\n",
        "            if sub[\"XGroup\"].nunique() < 2:\n",
        "                continue\n",
        "            res = _anova_subset(sub)\n",
        "            if res[\"ok\"] and np.isfinite(res[\"p_x\"]) and (res[\"p_x\"] < 0.05):\n",
        "                x1 = order.index(a); x2 = order.index(b)\n",
        "                if x1 > x2: x1, x2 = x2, x1\n",
        "                y_here = base + k * step\n",
        "                _draw_bracket(ax_plot, x1, x2, y_here, 0.04 * span, _p_to_stars(res[\"p_x\"]))\n",
        "                y_max = max(y_max, y_here + 0.08 * span)\n",
        "                k += 1\n",
        "        ax_plot.set_ylim(y_min, y_max)\n",
        "\n",
        "    ax_plot.set_title(variable, fontsize=14)\n",
        "    ax_plot.set_xlabel(\"\")\n",
        "    ax_plot.set_ylabel(variable)\n",
        "    plt.setp(ax_plot.get_xticklabels(), rotation=45, ha='right')\n",
        "    sns.despine(ax=ax_plot)\n",
        "\n",
        "    # Right panel: stats + Hue legend\n",
        "    ax_text.axis(\"off\")\n",
        "    ax_text.text(\n",
        "        0, 1,\n",
        "        _stats_text(dfm, x_label_name, hue_label_name, mode=mode, ref_group=ref_group, pair_list=pair_list),\n",
        "        va=\"top\", ha=\"left\", fontsize=12, transform=ax_text.transAxes\n",
        "    )\n",
        "    if len(hue_levels) >= 2:\n",
        "        handles = [plt.Line2D([0],[0], marker='o', linestyle='None',\n",
        "                              markerfacecolor=pal_dots[h], markeredgecolor='black', label=str(h))\n",
        "                   for h in hue_levels]\n",
        "        ax_text.legend(handles=handles, title=hue_label_name, loc=\"upper left\", bbox_to_anchor=(0, 0.6))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig if return_fig else plt.show()\n",
        "\n",
        "# -----------------------\n",
        "# 8) Actions\n",
        "# -----------------------\n",
        "out = widgets.Output()\n",
        "\n",
        "def _selected_x_and_colors():\n",
        "    sel = _selected_x()\n",
        "    color_map = {}\n",
        "    for g in sel:\n",
        "        val = x_colors[g].value.strip()\n",
        "        color_map[g] = val if val else \"tab:blue\"\n",
        "    return sel, color_map\n",
        "\n",
        "def _current_pairs():\n",
        "    return list(pairs_select.value)\n",
        "\n",
        "def _run_plots(_=None):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        sel_x, color_map = _selected_x_and_colors()\n",
        "        if len(sel_x) < 1:\n",
        "            print(\"Select at least one X group.\"); return\n",
        "\n",
        "        mode = mode_radio.value\n",
        "        if mode == \"ref\":\n",
        "            ref = ref_dropdown.value if (ref_dropdown.value in sel_x) else _choose_ref_group(sel_x)\n",
        "            print(f\"Showing X groups: {sel_x}  |  reference for stars: {ref}\")\n",
        "        else:\n",
        "            pair_list = _current_pairs()\n",
        "            if not pair_list:\n",
        "                print(f\"Showing X groups: {sel_x}  |  no pairs selected (select at least one).\"); return\n",
        "            print(f\"Showing X groups: {sel_x}  |  pairs: {pair_list}\")\n",
        "\n",
        "        exclude = {\"PeakAccuracy_Day\",\"PeakAccuracy_Night\",\n",
        "                   \"Win-stay_Day\",\"Win-stay_Night\",\n",
        "                   \"Lose-shift_Day\",\"Lose-shift_Night\"}\n",
        "        metrics = [m for m in long_df[\"variable\"].dropna().unique() if m not in exclude]\n",
        "\n",
        "        for metric in metrics:\n",
        "            subset = long_df[(long_df[\"variable\"] == metric) & (long_df[\"XGroup\"].isin(sel_x))]\n",
        "            if subset[\"value\"].dropna().empty:\n",
        "                print(f\"Skipping {metric}  no data for selected X groups.\"); continue\n",
        "            if mode == \"ref\":\n",
        "                _plot_metric_clean(\n",
        "                    subset, metric,\n",
        "                    x_color_map={g: color_map[g] for g in sel_x if g in subset['XGroup'].unique()},\n",
        "                    mode=\"ref\", ref_group=ref\n",
        "                )\n",
        "            else:\n",
        "                _plot_metric_clean(\n",
        "                    subset, metric,\n",
        "                    x_color_map={g: color_map[g] for g in sel_x if g in subset['XGroup'].unique()},\n",
        "                    mode=\"pairs\", pair_list=_current_pairs()\n",
        "                )\n",
        "\n",
        "def _save_plots(_=None):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        sel_x, color_map = _selected_x_and_colors()\n",
        "        if len(sel_x) < 1:\n",
        "            print(\"Select at least one X group.\"); return\n",
        "\n",
        "        mode = mode_radio.value\n",
        "        ref = ref_dropdown.value if (mode == \"ref\") else None\n",
        "        pair_list = _current_pairs() if (mode == \"pairs\") else None\n",
        "        if mode == \"pairs\" and not pair_list:\n",
        "            print(\"Select at least one pair before saving.\"); return\n",
        "\n",
        "        os.makedirs(\"metric_comparisons\", exist_ok=True)\n",
        "        saved = 0\n",
        "\n",
        "        exclude = {\"PeakAccuracy_Day\",\"PeakAccuracy_Night\",\n",
        "                   \"Win-stay_Day\",\"Win-stay_Night\",\n",
        "                   \"Lose-shift_Day\",\"Lose-shift_Night\"}\n",
        "        for metric in [m for m in long_df[\"variable\"].dropna().unique() if m not in exclude]:\n",
        "            subset = long_df[(long_df[\"variable\"] == metric) & (long_df[\"XGroup\"].isin(sel_x))]\n",
        "            if subset[\"value\"].dropna().empty: continue\n",
        "            fig = _plot_metric_clean(\n",
        "                subset, metric,\n",
        "                x_color_map={g: color_map[g] for g in sel_x if g in subset['XGroup'].unique()},\n",
        "                mode=mode, ref_group=ref, pair_list=pair_list, return_fig=True\n",
        "            )\n",
        "            safe = metric.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
        "            fig.savefig(f\"metric_comparisons/{safe}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
        "            plt.close(fig); saved += 1\n",
        "\n",
        "        if saved == 0:\n",
        "            print(\"No figures to save.\"); return\n",
        "        zipname = f\"metric_comparisons_{int(time.time())}.zip\"\n",
        "        shutil.make_archive(zipname.replace(\".zip\",\"\"), 'zip', \"metric_comparisons\")\n",
        "        if colab_files is not None:\n",
        "            colab_files.download(zipname)\n",
        "        print(f\"Saved {zipname}\")\n",
        "\n",
        "plot_btn.on_click(_run_plots)\n",
        "save_btn.on_click(_save_plots)\n",
        "\n",
        "# -----------------------\n",
        "# 9) Assemble compact UI (two columns)\n",
        "# -----------------------\n",
        "def _toggle_controls(*_):\n",
        "    if mode_radio.value == \"ref\":\n",
        "        ref_dropdown.layout.display = \"\"\n",
        "        pairs_select.layout.display = \"none\"\n",
        "    else:\n",
        "        ref_dropdown.layout.display = \"none\"\n",
        "        pairs_select.layout.display = \"\"\n",
        "_toggle_controls()\n",
        "mode_radio.observe(lambda _: _toggle_controls(), names=\"value\")\n",
        "\n",
        "row = widgets.HBox(\n",
        "    [left_col, right_col],\n",
        "    layout=widgets.Layout(justify_content=\"flex-start\", align_items=\"flex-start\", gap=\"16px\", width=\"auto\")\n",
        ")\n",
        "ui = widgets.VBox([widgets.HTML(\"<h3 style='margin-bottom:6px'>FR1 Metric Comparisons</h3>\"), row, out],\n",
        "                  layout=widgets.Layout(width=\"auto\"))\n",
        "\n",
        "display(ui)\n",
        "\n",
        "# Auto-run once\n",
        "_run_plots()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Day/Night metrics\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# ---- Config ----\n",
        "# If you want to force a specific set, set FORCE_BASES to a list of base names (without _Day/_Night)\n",
        "FORCE_BASES = None  # e.g., [\"%MealPellets\",\"%GrazingPellets\",\"Pellets\",\"NumMeals\",\"AvgMealSize\",\"AvgMealDuration\",\"MealsPerHour\",\"Accuracy\"]\n",
        "DAY_TAG, NIGHT_TAG = \"_Day\", \"_Night\"\n",
        "BAR_WIDTH = 0.36\n",
        "NIGHT_ALPHA = 0.6\n",
        "DOT_SIZE = 7\n",
        "EDGE_LW_DAY = 2.0\n",
        "EDGE_LW_NIGHT = 1.0\n",
        "\n",
        "ALLOWED_DN_BASES = {\n",
        "    \"%MealPellets\",\"%GrazingPellets\",\"Pellets\",\"NumMeals\",\"AvgMealSize\",\n",
        "    \"AvgMealDuration\",\"MealsPerHour\",\"Accuracy\"\n",
        "}\n",
        "\n",
        "# ---- Helpers reused/compatible with previous cell ----\n",
        "def _safe_order(groups):\n",
        "    groups = [g for g in groups if pd.notna(g)]\n",
        "    if '_order_x_groups' in globals():\n",
        "        try:\n",
        "            return _order_x_groups(groups)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return sorted(groups, key=lambda s: str(s).upper())\n",
        "\n",
        "def _selected_x_groups():\n",
        "    if 'x_checks' in globals() and isinstance(x_checks, dict) and len(x_checks):\n",
        "        sel = [g for g, cb in x_checks.items() if getattr(cb, \"value\", False)]\n",
        "        return _safe_order(sel)\n",
        "    if 'long_df' not in globals():\n",
        "        raise RuntimeError(\"long_df not found\")\n",
        "    return _safe_order(long_df[\"XGroup\"].dropna().unique().tolist())\n",
        "\n",
        "def _color_map(x_groups):\n",
        "    if 'x_colors' in globals() and isinstance(x_colors, dict) and len(x_colors):\n",
        "        out = {}\n",
        "        for g in x_groups:\n",
        "            w = x_colors.get(g, None)\n",
        "            val = getattr(w, \"value\", None) if w is not None else None\n",
        "            out[g] = (val.strip() if isinstance(val, str) and val.strip() else \"blue\")\n",
        "        return out\n",
        "    defaults = [\"blue\",\"orange\",\"green\",\"red\",\"purple\",\"brown\",\"pink\",\"gray\",\"olive\",\"cyan\"]\n",
        "    return {g: defaults[i % len(defaults)] for i, g in enumerate(x_groups)}\n",
        "\n",
        "def _label_for(which=\"X\"):\n",
        "    if '_grouping_label' in globals():\n",
        "        return _grouping_label(which)\n",
        "    return \"XGroup\" if which.lower().startswith(\"x\") else \"HueGroup\"\n",
        "\n",
        "def _dot_palette_local(hues):\n",
        "    if '_dot_palette' in globals():\n",
        "        return _dot_palette(hues)\n",
        "    hues = list(hues)\n",
        "    if len(hues) == 0: return {}\n",
        "    if len(hues) == 1: return {hues[0]: \"black\"}\n",
        "    if len(hues) == 2: return {hues[0]: \"white\", hues[1]: \"black\"}\n",
        "    defaults = plt.rcParams.get('axes.prop_cycle', None)\n",
        "    colors = defaults.by_key()['color'] if defaults else [\"C0\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\"]\n",
        "    return {h: colors[i % len(colors)] for i, h in enumerate(hues)}\n",
        "\n",
        "# ---- Day/Night extraction helpers ----\n",
        "def _extract_dn_base(var_name):\n",
        "    \"\"\"\n",
        "    From a long_df 'variable' like 'Pellets_Day' or 'Pellets_Day_FR1', return 'Pellets'.\n",
        "    Returns None if it doesn't look like a Day/Night variable.\n",
        "    \"\"\"\n",
        "    if \"_Day\" in var_name:\n",
        "        return var_name.split(\"_Day\", 1)[0]\n",
        "    if \"_Night\" in var_name:\n",
        "        return var_name.split(\"_Night\", 1)[0]\n",
        "    return None\n",
        "\n",
        "def _ensure_daynight_view(df, base):\n",
        "    \"\"\"\n",
        "    Build a tidy DataFrame with columns: XGroup, HueGroup, DayNight ('Day'/'Night'), value\n",
        "    Works with variables that might include session suffixes (e.g., '<base>_Day_FR1').\n",
        "    \"\"\"\n",
        "    patt_day = re.compile(rf\"^{re.escape(base)}{re.escape(DAY_TAG)}(?:_.+)?$\")\n",
        "    patt_ngt = re.compile(rf\"^{re.escape(base)}{re.escape(NIGHT_TAG)}(?:_.+)?$\")\n",
        "    keep_mask = df[\"variable\"].astype(str).str.match(patt_day) | df[\"variable\"].astype(str).str.match(patt_ngt)\n",
        "    d = df.loc[keep_mask].copy()\n",
        "    if d.empty:\n",
        "        return d\n",
        "    d[\"DayNight\"] = np.where(d[\"variable\"].str.match(patt_day), \"Day\", \"Night\")\n",
        "    return d\n",
        "\n",
        "def _discover_daynight_bases(df):\n",
        "    vars_ = df[\"variable\"].astype(str).unique().tolist()\n",
        "    bases = sorted({b for v in vars_ for b in [_extract_dn_base(v)] if b})\n",
        "    # Keep only bases we actually want to plot (and that make sense for FR1)\n",
        "    bases = [b for b in bases if b in ALLOWED_DN_BASES]\n",
        "    return bases\n",
        "\n",
        "# ---- Stats text (ANOVA with Day/Night included) ----\n",
        "def _anova_daynight_text(df_dn, x_label, hue_label):\n",
        "    d = df_dn.dropna(subset=[\"value\",\"XGroup\",\"DayNight\"]).copy()\n",
        "    if d[\"XGroup\"].nunique() < 2 or d[\"DayNight\"].nunique() < 2:\n",
        "        return \"Too few groups or missing Day/Night to run ANOVA.\"\n",
        "\n",
        "    has_hue = d[\"HueGroup\"].nunique(dropna=True) >= 2\n",
        "    try:\n",
        "        if has_hue:\n",
        "            model = ols('value ~ C(XGroup) + C(DayNight) + C(HueGroup) + '\n",
        "                        'C(XGroup):C(DayNight) + C(XGroup):C(HueGroup) + '\n",
        "                        'C(DayNight):C(HueGroup) + C(XGroup):C(DayNight):C(HueGroup)',\n",
        "                        data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            def pget(term):\n",
        "                return float(an.loc[term, 'PR(>F)']) if term in an.index else np.nan\n",
        "            def fmt(p):\n",
        "                if not np.isfinite(p): return \"n/a\"\n",
        "                return f\"p = {p:.3f}\" if p >= 0.001 else \"p < 0.001\"\n",
        "            return (\n",
        "                \"Three-way ANOVA (XGroup, Day/Night, Hue)\\n\"\n",
        "                f\"{x_label}: {fmt(pget('C(XGroup)'))}\\n\"\n",
        "                f\"Day/Night: {fmt(pget('C(DayNight)'))}\\n\"\n",
        "                f\"{hue_label}: {fmt(pget('C(HueGroup)'))}\\n\"\n",
        "                f\"{x_label}Day/Night: {fmt(pget('C(XGroup):C(DayNight)'))}\\n\"\n",
        "                f\"{x_label}{hue_label}: {fmt(pget('C(XGroup):C(HueGroup)'))}\\n\"\n",
        "                f\"Day/Night{hue_label}: {fmt(pget('C(DayNight):C(HueGroup)'))}\\n\"\n",
        "                f\"{x_label}Day/Night{hue_label}: {fmt(pget('C(XGroup):C(DayNight):C(HueGroup)'))}\"\n",
        "            )\n",
        "        else:\n",
        "            model = ols('value ~ C(XGroup) + C(DayNight) + C(XGroup):C(DayNight)', data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            def fmt(p):\n",
        "                if not np.isfinite(p): return \"n/a\"\n",
        "                return f\"p = {p:.3f}\" if p >= 0.001 else \"p < 0.001\"\n",
        "            return (\n",
        "                \"Two-way ANOVA (XGroup, Day/Night)\\n\"\n",
        "                f\"{_label_for('X')}: {fmt(float(an.loc['C(XGroup)','PR(>F)']))}\\n\"\n",
        "                f\"Day/Night: {fmt(float(an.loc['C(DayNight)','PR(>F)']))}\\n\"\n",
        "                f\"{_label_for('X')}Day/Night: {fmt(float(an.loc['C(XGroup):C(DayNight)','PR(>F)']))}\"\n",
        "            )\n",
        "    except Exception as e:\n",
        "        return f\"ANOVA failed: {e}\"\n",
        "\n",
        "# ---- Main plotting for a single base ----\n",
        "def _plot_day_night_for_base(base, x_groups, color_map):\n",
        "    df_dn = _ensure_daynight_view(long_df, base)\n",
        "    df_dn = df_dn[df_dn[\"XGroup\"].isin(x_groups)].copy()\n",
        "    if df_dn.empty:\n",
        "        print(f\"Skipping {base}: no Day/Night data for selected groups.\")\n",
        "        return\n",
        "\n",
        "    x_label = _label_for(\"X\")\n",
        "    hue_label = _label_for(\"Hue\")\n",
        "\n",
        "    hue_levels = [h for h in df_dn[\"HueGroup\"].dropna().unique().tolist()]\n",
        "    pal_dots = _dot_palette_local(hue_levels)\n",
        "\n",
        "    means = (df_dn.dropna(subset=[\"value\"])\n",
        "                  .groupby([\"XGroup\",\"DayNight\"], as_index=False)[\"value\"]\n",
        "                  .mean().rename(columns={\"value\":\"mean\"}))\n",
        "    grid = (means.set_index([\"XGroup\",\"DayNight\"])[\"mean\"]\n",
        "                 .unstack(\"DayNight\")\n",
        "                 .reindex(index=x_groups, columns=[\"Day\",\"Night\"]))\n",
        "\n",
        "    width = max(4, 1.2 * len(x_groups))\n",
        "    fig, (ax, ax_txt) = plt.subplots(1, 2, figsize=(width, 4.6), gridspec_kw={'width_ratios': [3, 1]})\n",
        "\n",
        "    x = np.arange(len(x_groups))\n",
        "    off = BAR_WIDTH/2.0 + 0.02\n",
        "    pos_day = x - off\n",
        "    pos_night = x + off\n",
        "\n",
        "    # Night bars (filled)\n",
        "    for i, g in enumerate(x_groups):\n",
        "        val = grid.loc[g, \"Night\"] if (\"Night\" in grid.columns) else np.nan\n",
        "        if pd.notna(val):\n",
        "            ax.bar(pos_night[i], val, width=BAR_WIDTH, color=color_map[g],\n",
        "                   alpha=NIGHT_ALPHA, edgecolor=\"black\", linewidth=EDGE_LW_NIGHT, zorder=2)\n",
        "\n",
        "    # Day bars (outline)\n",
        "    for i, g in enumerate(x_groups):\n",
        "        val = grid.loc[g, \"Day\"] if (\"Day\" in grid.columns) else np.nan\n",
        "        if pd.notna(val):\n",
        "            ax.bar(pos_day[i], val, width=BAR_WIDTH, facecolor=(0,0,0,0),\n",
        "                   edgecolor=color_map[g], linewidth=EDGE_LW_DAY, zorder=3)\n",
        "\n",
        "    # Overlay individual dots by HueGroup\n",
        "    rng = np.random.default_rng(42)\n",
        "    jitter = lambda n: (rng.normal(0, 0.02, size=n))\n",
        "\n",
        "    # Day dots (outline markers)\n",
        "    sdf = df_dn[df_dn[\"DayNight\"] == \"Day\"].dropna(subset=[\"value\"])\n",
        "    for g in x_groups:\n",
        "        sub = sdf[sdf[\"XGroup\"] == g]\n",
        "        if sub.empty: continue\n",
        "        px = pos_day[x_groups.index(g)]\n",
        "        for h in sub[\"HueGroup\"].unique():\n",
        "            hh = sub[sub[\"HueGroup\"] == h]\n",
        "            if hh.empty: continue\n",
        "            ax.scatter(np.full(len(hh), px) + jitter(len(hh)), hh[\"value\"],\n",
        "                       s=DOT_SIZE**2/2, facecolors=pal_dots.get(h, \"black\"),\n",
        "                       edgecolors=\"black\", linewidths=0.6, alpha=NIGHT_ALPHA, zorder=4)\n",
        "\n",
        "    # Night dots (filled markers)\n",
        "    sdf = df_dn[df_dn[\"DayNight\"] == \"Night\"].dropna(subset=[\"value\"])\n",
        "    for g in x_groups:\n",
        "        sub = sdf[sdf[\"XGroup\"] == g]\n",
        "        if sub.empty: continue\n",
        "        px = pos_night[x_groups.index(g)]\n",
        "        for h in sub[\"HueGroup\"] == sub[\"HueGroup\"]:\n",
        "            pass  # to keep structure clear\n",
        "        for h in sub[\"HueGroup\"].unique():\n",
        "            hh = sub[sub[\"HueGroup\"] == h]\n",
        "            if hh.empty: continue\n",
        "            ax.scatter(np.full(len(hh), px) + jitter(len(hh)), hh[\"value\"],\n",
        "                       s=DOT_SIZE**2/2, facecolors=pal_dots.get(h, \"black\"),\n",
        "                       edgecolors=\"black\", linewidths=0.6, alpha=NIGHT_ALPHA, zorder=4)\n",
        "\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(x_groups, rotation=45, ha=\"right\")\n",
        "    ax.set_ylabel(base)\n",
        "    ax.set_title(f\"{base}\")\n",
        "    sns.despine(ax=ax)\n",
        "\n",
        "    ax_txt.axis(\"off\")\n",
        "    ax_txt.text(0, 1, _anova_daynight_text(df_dn, x_label, hue_label),\n",
        "                va=\"top\", ha=\"left\", fontsize=12, transform=ax_txt.transAxes)\n",
        "\n",
        "    if len(hue_levels) >= 2:\n",
        "        handles = [plt.Line2D([0],[0], marker='o', linestyle='None',\n",
        "                              markerfacecolor=pal_dots[h], markeredgecolor='black',\n",
        "                              label=str(h)) for h in hue_levels]\n",
        "        ax_txt.legend(handles=handles, title=hue_label,\n",
        "                      loc=\"upper left\", bbox_to_anchor=(0, 0.3),\n",
        "                      frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---- Run ----\n",
        "if 'long_df' not in globals():\n",
        "    raise RuntimeError(\"This cell expects long_df from the previous cell.\")\n",
        "\n",
        "Xsel = _selected_x_groups()\n",
        "if not Xsel:\n",
        "    print(\"No X groups selected or available.\")\n",
        "else:\n",
        "    cmap = _color_map(Xsel)\n",
        "    if FORCE_BASES is not None:\n",
        "        BASES = [b for b in FORCE_BASES if b in ALLOWED_DN_BASES]\n",
        "    else:\n",
        "        BASES = _discover_daynight_bases(long_df)\n",
        "    if not BASES:\n",
        "        print(\"No Day/Night metrics found in long_df.\")\n",
        "    else:\n",
        "        for base in BASES:\n",
        "            _plot_day_night_for_base(base, Xsel, cmap)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NWeeNCOgstiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfhfE3F8k6Jl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Interpellet intervals\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Collect IPI from file_to_df\n",
        "if \"file_to_df\" not in globals():\n",
        "    if \"loaded_files\" in globals() and \"feds\" in globals():\n",
        "        file_to_df = {Path(str(fname)).name: df for fname, df in zip(loaded_files, feds)}\n",
        "    else:\n",
        "        raise RuntimeError(\"Missing `file_to_df`.\")\n",
        "\n",
        "rows = []\n",
        "for fname, df in file_to_df.items():\n",
        "    if df is None or df.empty:\n",
        "        continue\n",
        "    if \"Event\" in df.columns:\n",
        "        df = df[df[\"Event\"].isin([\"Left\",\"Right\",\"Pellet\"])].copy()\n",
        "    if \"InterPelletInterval\" not in df.columns:\n",
        "        continue\n",
        "    vals = pd.to_numeric(df[\"InterPelletInterval\"], errors=\"coerce\").dropna()\n",
        "    vals = vals[vals > 0]\n",
        "    if vals.empty:\n",
        "        continue\n",
        "    base = os.path.basename(str(fname))\n",
        "    rows.extend({\"filename\": base, \"IPI_s\": float(v)} for v in vals)\n",
        "\n",
        "ipi = pd.DataFrame(rows)\n",
        "if ipi.empty:\n",
        "    raise RuntimeError(\"No InterPelletInterval values found.\")\n",
        "\n",
        "# 2) Merge XGroup from files_to_group_both (ignore Hue)\n",
        "if \"files_to_group_both\" not in globals() or files_to_group_both is None or files_to_group_both.empty:\n",
        "    raise RuntimeError(\"Missing `files_to_group_both`. Build Groups first.\")\n",
        "\n",
        "grp = files_to_group_both.copy()\n",
        "src_col = \"filename\" if \"filename\" in grp.columns else (\"File\" if \"File\" in grp.columns else None)\n",
        "if src_col is None:\n",
        "    raise RuntimeError(\"`files_to_group_both` needs 'filename' or 'File'.\")\n",
        "grp[\"filename\"] = grp[src_col].astype(str).map(os.path.basename)\n",
        "ipi = ipi.merge(grp[[\"filename\",\"XGroup\"]].drop_duplicates(), on=\"filename\", how=\"left\")\n",
        "ipi[\"XGroup\"] = ipi[\"XGroup\"].fillna(\"UNASSIGNED\")\n",
        "\n",
        "# 3) Choose groups: all checked in widget, else all present\n",
        "def _selected_xgroups():\n",
        "    if 'x_checks' in globals() and isinstance(x_checks, dict) and len(x_checks):\n",
        "        return [g for g, cb in x_checks.items() if getattr(cb, \"value\", False)]\n",
        "    return sorted(ipi[\"XGroup\"].dropna().unique().tolist())\n",
        "\n",
        "chosen = _selected_xgroups()\n",
        "if not chosen:\n",
        "    raise RuntimeError(\"No XGroups selected/found.\")\n",
        "ipi2 = ipi[ipi[\"XGroup\"].isin(chosen)].copy()\n",
        "\n",
        "# 4) KDE in log10 space (correct for log-x plotting)\n",
        "ipi2 = ipi2[(ipi2[\"IPI_s\"] > 0) & np.isfinite(ipi2[\"IPI_s\"])]\n",
        "ipi2[\"log10_IPI\"] = np.log10(ipi2[\"IPI_s\"])\n",
        "\n",
        "# --- Match order & colors from the main plotting cell ---\n",
        "\n",
        "_present = ipi2[\"XGroup\"].dropna().unique().tolist()\n",
        "\n",
        "# ORDER: prefer the global ordered_x (WT/CONTROL-first); otherwise reproduce rule locally\n",
        "if \"ordered_x\" in globals():\n",
        "    group_order = [g for g in ordered_x if g in _present] + [g for g in _present if g not in ordered_x]\n",
        "else:\n",
        "    import re\n",
        "    def _is_wt(g):\n",
        "        toks = [t for t in re.split(r'[^A-Z0-9]+', str(g).upper()) if t]\n",
        "        return any(t in {\"WT\", \"WILDTYPE\", \"CONTROL\", \"CTRL\"} for t in toks)\n",
        "    def _x_levels(g): return [p.strip() for p in str(g).split(\"|\")]\n",
        "    def _key(g):\n",
        "        lv = _x_levels(g)\n",
        "        wt_rank = 0 if any(_is_wt(tok) for tok in lv) or _is_wt(g) else 1\n",
        "        blanks = [(1 if s.strip().upper() in {\"\",\"UNASSIGNED\",\"NONE\",\"NA\",\"N/A\"} else 0, s.upper()) for s in lv]\n",
        "        return (wt_rank, *blanks, str(g).upper())\n",
        "    group_order = sorted(_present, key=_key)\n",
        "\n",
        "# COLORS: pull from x_colors (widgets) so bars/lines/KDE match\n",
        "palette_map = None\n",
        "if \"x_colors\" in globals() and isinstance(x_colors, dict) and x_colors:\n",
        "    def _col(g):\n",
        "        try:\n",
        "            v = x_colors[g].value\n",
        "            return v.strip() if isinstance(v, str) and v.strip() else \"tab:blue\"\n",
        "        except Exception:\n",
        "            return \"tab:blue\"\n",
        "    palette_map = {g: _col(g) for g in group_order}\n",
        "\n",
        "# ---- PLOT ONLY (no metric computation) ----\n",
        "sns.set_style(\"white\")\n",
        "plt.figure(figsize=(9, 5))\n",
        "sns.set_style(\"white\")\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# convert seconds to minutes and compute log10 in minutes\n",
        "ipi2[\"IPI_min\"] = ipi2[\"IPI_s\"] / 60.0\n",
        "ipi2 = ipi2[(ipi2[\"IPI_min\"] > 0) & np.isfinite(ipi2[\"IPI_min\"])]\n",
        "ipi2[\"log10_IPI_min\"] = np.log10(ipi2[\"IPI_min\"])\n",
        "\n",
        "# plot KDE in log10(minutes)\n",
        "ax = sns.kdeplot(\n",
        "    data=ipi2,\n",
        "    x=\"log10_IPI_min\",\n",
        "    hue=\"XGroup\",\n",
        "    hue_order=group_order,\n",
        "    palette=palette_map,\n",
        "    fill=False,\n",
        "    common_norm=False,\n",
        "    cut=0,\n",
        "    bw_adjust=0.9,\n",
        "    gridsize=512,\n",
        "    linewidth=2\n",
        ")\n",
        "\n",
        "# Nice decade ticks based on data range (in log10 minutes)\n",
        "lo = np.floor(ipi2[\"log10_IPI_min\"].min())\n",
        "hi = np.ceil(ipi2[\"log10_IPI_min\"].max())\n",
        "ticks_log = np.arange(lo, hi + 1)\n",
        "ax.set_xticks(ticks_log)\n",
        "\n",
        "def _min_label(t):\n",
        "    v = 10.0 ** float(t)  # minutes\n",
        "    # format: show 2 decimals if <1 min, otherwise integer minutes\n",
        "    return f\"{v:.2f}\" if v < 1 else f\"{int(round(v))}\"\n",
        "\n",
        "ax.set_xticklabels([_min_label(t) for t in ticks_log])\n",
        "ax.set_xlabel(\"Interpellet Interval (min)\")\n",
        "ax.set_ylabel(\"Density\")\n",
        "ax.set_title(\"\")\n",
        "leg = ax.get_legend()\n",
        "if leg:\n",
        "    leg.set_title(\"\")\n",
        "    leg.set_frame_on(False)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Embed TrueType fonts so text stays editable in Illustrator/Inkscape\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['pdf.fonttype'] = 42\n",
        "mpl.rcParams['ps.fonttype'] = 42\n",
        "\n",
        "outdir = \"figures\"\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "from datetime import datetime\n",
        "fname = f\"interpellet_histogram_{datetime.now():%Y-%m-%d}.pdf\"\n",
        "fig = ax.get_figure()\n",
        "fig.savefig(\n",
        "    os.path.join(outdir, fname),\n",
        "    format=\"pdf\",\n",
        "    bbox_inches=\"tight\",\n",
        "    transparent=True,\n",
        ")\n",
        "files.download(os.path.join(outdir, fname))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DKm0nsGv5f0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Cluster FR1 by heatmap & PCA (X/Hue from Group UI; generic labels/markers)\n",
        "\n",
        "import os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as mcolors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ---------- Pick FR1 source ----------\n",
        "if 'FR1metrics_merged' in globals() and FR1metrics_merged is not None and not FR1metrics_merged.empty:\n",
        "    FR1_src = FR1metrics_merged.copy()\n",
        "elif 'FR1metrics_csv' in globals() and FR1metrics_csv is not None and not FR1metrics_csv.empty:\n",
        "    FR1_src = FR1metrics_csv.copy()\n",
        "elif 'FR1metrics' in globals() and FR1metrics is not None and not FR1metrics.empty:\n",
        "    FR1_src = FR1metrics.copy()\n",
        "else:\n",
        "    raise RuntimeError(\"No FR1 table found. Expect FR1metrics_merged, FR1metrics_csv, FR1metrics, or FR1_metrics.\")\n",
        "\n",
        "# Ensure we have a File column (fallback from filename if needed)\n",
        "if 'File' not in FR1_src.columns:\n",
        "    if 'filename' in FR1_src.columns:\n",
        "        FR1_src = FR1_src.copy()\n",
        "        FR1_src['File'] = FR1_src['filename'].astype(str)\n",
        "    else:\n",
        "        raise RuntimeError(\"FR1 table must include 'File' or 'filename' column.\")\n",
        "\n",
        "# ---------- Groups from the Group UI ----------\n",
        "def _basename(p): return os.path.basename(str(p))\n",
        "\n",
        "if 'files_to_group_both' in globals() and files_to_group_both is not None and not files_to_group_both.empty:\n",
        "    grp_map = files_to_group_both.copy()\n",
        "    src_col = 'filename' if 'filename' in grp_map.columns else 'File'\n",
        "    grp_map[\"File_base\"] = grp_map[src_col].astype(str).apply(_basename)\n",
        "    grp_map = grp_map[[\"File_base\",\"XGroup\",\"HueGroup\"]]\n",
        "else:\n",
        "    raise RuntimeError(\"No groups found. Run the 'Group for plotting' widget (two-column) and click 'Build Groups'.\")\n",
        "# Ensure both tables have a File_base column built from the filename/File\n",
        "FR1_src = FR1_src.copy()\n",
        "src_col_fr1 = 'filename' if 'filename' in FR1_src.columns else 'File'\n",
        "FR1_src['File_base'] = FR1_src[src_col_fr1].astype(str).apply(_basename)\n",
        "\n",
        "# grp_map already has File_base from earlier code\n",
        "# Merge groups onto FR1; each file should map to at most one (XGroup, HueGroup)\n",
        "ldf = (\n",
        "    FR1_src.merge(\n",
        "        grp_map, on='File_base', how='left', validate='m:1'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Helpful checks\n",
        "if {'XGroup','HueGroup'}.issubset(ldf.columns) and ldf[['XGroup','HueGroup']].isna().all().all():\n",
        "    raise RuntimeError(\n",
        "        \"Group merge completed but all XGroup/HueGroup are NaN. \"\n",
        "        \"Check that the basename in FR1 matches the Group UI table.\"\n",
        "    )\n",
        "# ---------- STRICT METRIC SELECTION (whitelist patterns) ----------\n",
        "# Base metric names (covers base, _Day/_Night, and any extra suffix like _FR1)\n",
        "_METRIC_BASES = [\n",
        "    \"Pellets\",\"Left_Poke\",\"Right_Poke\",\"Total_Pokes\",\"Accuracy\",\"PokesPerPellet\",\n",
        "    \"RetrievalTime\",\"InterPelletInterval\",\"PokeTime\",\n",
        "    \"%MealPellets\",\"%GrazingPellets\",\"NumMeals\",\"AvgMealSize\",\"AvgMealDuration\",\n",
        "    \"RecordingHours\",\"MealsPerHour\",\"Daily_Pellets\",\"Left Poke with Pellet\",\n",
        "]\n",
        "\n",
        "def _metric_regex_from_bases(bases):\n",
        "    safes = [re.escape(b) for b in bases]\n",
        "    # Match base; optionally _Day/_Night; optionally any further suffix (e.g., _FR1)\n",
        "    return re.compile(rf\"^(?:{'|'.join(safes)})(?:_(?:Day|Night))?(?:_.+)?$\")\n",
        "\n",
        "_METRIC_RX = _metric_regex_from_bases(_METRIC_BASES)\n",
        "\n",
        "# Identify metric columns by name pattern only (ignore dtype), exclude ids explicitly\n",
        "ID_LIKE = {\n",
        "    \"File\",\"filename\",\"Mouse_ID\",\"Strain\",\"Sex\",\"Genotype\",\"Session_type\",\n",
        "    \"File_base\",\"XGroup\",\"HueGroup\"\n",
        "}\n",
        "metric_columns = [c for c in ldf.columns if isinstance(c, str) and _METRIC_RX.match(c) and c not in ID_LIKE]\n",
        "if not metric_columns:\n",
        "    raise RuntimeError(\"No metric columns matched the whitelist patterns. Check column names or base lists.\")\n",
        "\n",
        "# Coerce metric columns to numeric safely\n",
        "for c in metric_columns:\n",
        "    ldf[c] = pd.to_numeric(ldf[c], errors=\"coerce\")\n",
        "\n",
        "# ---------- Build long & wide ----------\n",
        "id_keep = [c for c in [\"File\",\"filename\",\"Mouse_ID\",\"Strain\",\"Sex\",\"Genotype\",\"Session_type\",\"XGroup\",\"HueGroup\"] if c in ldf.columns]\n",
        "\n",
        "long_df = ldf.melt(\n",
        "    id_vars=id_keep,\n",
        "    value_vars=metric_columns,\n",
        "    var_name=\"metric\",\n",
        "    value_name=\"value\"\n",
        ").copy()\n",
        "\n",
        "# Wide (one row per file/animal), average duplicates\n",
        "wide_index = [c for c in [\"File\",\"filename\",\"Mouse_ID\",\"Strain\",\"Sex\",\"Genotype\",\"XGroup\",\"HueGroup\"] if c in long_df.columns]\n",
        "wide_metrics = (\n",
        "    long_df.pivot_table(\n",
        "        index=wide_index,\n",
        "        columns=\"metric\",\n",
        "        values=\"value\",\n",
        "        aggfunc=\"mean\",\n",
        "        observed=True\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# The metric columns are exactly those we whitelisted\n",
        "metric_columns = [c for c in wide_metrics.columns if c not in wide_index]\n",
        "\n",
        "# ---------- Group means by XGroup (for heatmap & PCA feature set) ----------\n",
        "if (\"XGroup\" not in wide_metrics.columns) or (\"HueGroup\" not in wide_metrics.columns):\n",
        "    raise RuntimeError(\"XGroup and HueGroup are required to cluster on both.\")\n",
        "\n",
        "# Order rows as a stable product of XGroup then HueGroup\n",
        "_x_order = sorted(wide_metrics[\"XGroup\"].astype(str).unique())\n",
        "_h_order = sorted(wide_metrics[\"HueGroup\"].astype(str).unique())\n",
        "\n",
        "group_means = (\n",
        "    wide_metrics\n",
        "    .assign(XGroup=wide_metrics[\"XGroup\"].astype(str),\n",
        "            HueGroup=wide_metrics[\"HueGroup\"].astype(str))\n",
        "    .groupby([\"XGroup\", \"HueGroup\"], dropna=False)[metric_columns]\n",
        "    .mean()\n",
        "    .reindex(pd.MultiIndex.from_product([_x_order, _h_order],\n",
        "                                        names=[\"XGroup\",\"HueGroup\"]))\n",
        ")\n",
        "\n",
        "# Build a readable index like \"X | Hue\" for the heatmap rows\n",
        "group_means.index = [f\"{x} | {h}\" for x, h in group_means.index]\n",
        "\n",
        "# ---------- Heatmap (minmax per column, with numeric annotations) ----------\n",
        "def _fmt_cell(x):\n",
        "    if pd.isna(x): return \"\"\n",
        "    ax = abs(float(x))\n",
        "    return f\"{x:.0f}\" if ax >= 100 else f\"{x:.1f}\" if ax >= 10 else f\"{x:.2f}\"\n",
        "\n",
        "annot_data = group_means.applymap(_fmt_cell)\n",
        "\n",
        "heatmap_scaled = group_means.copy()\n",
        "for col in heatmap_scaled.columns:\n",
        "    col_min, col_max = heatmap_scaled[col].min(), heatmap_scaled[col].max()\n",
        "    if pd.isna(col_min) or pd.isna(col_max):\n",
        "        heatmap_scaled[col] = 0.0\n",
        "    elif col_max == col_min:\n",
        "        heatmap_scaled[col] = 0.5  # constant column  mid tone\n",
        "    else:\n",
        "        heatmap_scaled[col] = (heatmap_scaled[col] - col_min) / (col_max - col_min)\n",
        "\n",
        "heatmap_scaled = heatmap_scaled.sort_index()\n",
        "annot_data = annot_data.loc[heatmap_scaled.index]\n",
        "\n",
        "# ---------- PCA (same metric set) ----------\n",
        "pca_features = metric_columns[:]  # same set used in heatmap\n",
        "mouse_data = wide_metrics.dropna(subset=pca_features).copy()\n",
        "\n",
        "# Labels: prefer Mouse_ID, else filename (basename), else File\n",
        "if \"Mouse_ID\" in mouse_data.columns and mouse_data[\"Mouse_ID\"].notna().any():\n",
        "    labels = mouse_data[\"Mouse_ID\"].astype(str)\n",
        "elif \"filename\" in mouse_data.columns and mouse_data[\"filename\"].notna().any():\n",
        "    labels = mouse_data[\"filename\"].astype(str).apply(lambda p: os.path.basename(str(p)))\n",
        "else:\n",
        "    labels = mouse_data[\"File\"].astype(str)\n",
        "mouse_data[\"Label\"] = labels\n",
        "\n",
        "# Ensure grouping columns exist as strings\n",
        "for col, default in [(\"XGroup\", \"UNASSIGNED\"), (\"HueGroup\", \"ALL\")]:\n",
        "    if col in mouse_data.columns:\n",
        "        mouse_data[col] = mouse_data[col].astype(str)\n",
        "    else:\n",
        "        mouse_data[col] = default\n",
        "\n",
        "# Standardize metrics and run PCA\n",
        "X = StandardScaler().fit_transform(mouse_data[pca_features].values)\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(X)\n",
        "\n",
        "pca_df = pd.DataFrame(pca_result, columns=[\"PC1\", \"PC2\"])\n",
        "pca_df[\"Label\"]    = mouse_data[\"Label\"].values\n",
        "pca_df[\"XGroup\"]   = mouse_data[\"XGroup\"].values\n",
        "pca_df[\"HueGroup\"] = mouse_data[\"HueGroup\"].values\n",
        "\n",
        "# Loadings (top 8 by |PC1|)\n",
        "loadings = pd.DataFrame(pca.components_.T, index=pca_features, columns=[\"PC1\", \"PC2\"])\n",
        "top8_features = loadings.reindex(loadings[\"PC1\"].abs().sort_values(ascending=False).head(8).index)\n",
        "loadings_melted = top8_features[[\"PC1\", \"PC2\"]].reset_index().melt(id_vars=\"index\", var_name=\"PC\", value_name=\"Loading\")\n",
        "\n",
        "# ---------- Plot (13 grid) ----------\n",
        "fig = plt.figure(figsize=(18, 12), constrained_layout=True)\n",
        "gs = gridspec.GridSpec(\n",
        "    2, 2, figure=fig,\n",
        "    height_ratios=[0.8, 1.0],\n",
        "    width_ratios=[1.0, 0.8],\n",
        "    hspace=0.1, wspace=0.1\n",
        ")\n",
        "\n",
        "# Top: heatmap spans both columns\n",
        "ax_heat    = fig.add_subplot(gs[0, :])\n",
        "# Bottom: scatter (left) and bar (right)\n",
        "ax_scatter = fig.add_subplot(gs[1, 0])\n",
        "ax_bar     = fig.add_subplot(gs[1, 1])\n",
        "\n",
        "# 1) Heatmap of XGroup means\n",
        "sns.heatmap(\n",
        "    heatmap_scaled,\n",
        "    ax=ax_heat,\n",
        "    annot=annot_data.values,\n",
        "    fmt=\"\",\n",
        "    cmap=\"Blues\",\n",
        "    linewidths=0.5,\n",
        "    linecolor='gray',\n",
        "    alpha=0.5,\n",
        "    cbar=True\n",
        ")\n",
        "ax_heat.tick_params(axis='x', rotation=70)\n",
        "ax_heat.tick_params(axis='y', rotation=0)\n",
        "ax_heat.set_title(\"\", fontsize=16, color=\"darkblue\")\n",
        "ax_heat.set_xlabel(\"\"); ax_heat.set_ylabel(\"\")\n",
        "\n",
        "# 2) PCA: color by XGroup; marker encodes HueGroup\n",
        "unique_x = sorted(pca_df[\"XGroup\"].unique())\n",
        "\n",
        "# color map for XGroup\n",
        "x_colors = {}\n",
        "if len(unique_x) >= 1: x_colors[unique_x[0]] = \"dodgerblue\"\n",
        "if len(unique_x) >= 2: x_colors[unique_x[1]] = \"red\"\n",
        "if len(unique_x) > 2:\n",
        "    cmap = cm.get_cmap(\"tab20\", len(unique_x) - 2)\n",
        "    for i, grp in enumerate(unique_x[2:]):\n",
        "        x_colors[grp] = mcolors.to_hex(cmap(i))\n",
        "\n",
        "# marker selection for HueGroup\n",
        "all_hues = sorted([h for h in pca_df[\"HueGroup\"].unique()])\n",
        "if len(all_hues) == 0:\n",
        "    all_hues = [\"ALL\"]\n",
        "if len(all_hues) == 1:\n",
        "    hue_to_marker = {all_hues[0]: (\"o\", \"filled\")}\n",
        "elif len(all_hues) == 2:\n",
        "    hue_to_marker = {all_hues[0]: (\"o\", \"hollow\"), all_hues[1]: (\"o\", \"filled\")}\n",
        "else:\n",
        "    marker_cycle = [\"o\", \"s\", \"^\", \"D\", \"P\", \"X\", \"*\", \"v\", \"<\", \">\"]\n",
        "    hue_to_marker = {h: (marker_cycle[i % len(marker_cycle)], \"filled\") for i, h in enumerate(all_hues)}\n",
        "\n",
        "# draw points: loop XGroup (color), then HueGroup (marker style)\n",
        "for xg in unique_x:\n",
        "    sub_x = pca_df[pca_df[\"XGroup\"] == xg]\n",
        "    for hg in all_hues:\n",
        "        sub = sub_x[sub_x[\"HueGroup\"] == hg]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        marker, fill = hue_to_marker[hg]\n",
        "        if fill == \"hollow\":\n",
        "            ax_scatter.scatter(\n",
        "                sub[\"PC1\"], sub[\"PC2\"],\n",
        "                edgecolors=x_colors.get(xg, \"black\"), facecolors=\"none\",\n",
        "                s=110, linewidth=1.2, marker=marker, alpha=0.85,\n",
        "                label=f\"{xg} | {hg}\"\n",
        "            )\n",
        "        else:  # filled\n",
        "            ax_scatter.scatter(\n",
        "                sub[\"PC1\"], sub[\"PC2\"],\n",
        "                color=x_colors.get(xg, \"black\"),\n",
        "                s=110, linewidth=0.8, marker=marker, alpha=0.85,\n",
        "                label=f\"{xg} | {hg}\"\n",
        "            )\n",
        "\n",
        "# point labels\n",
        "for _, row in pca_df.iterrows():\n",
        "    ax_scatter.text(\n",
        "        row[\"PC1\"] + 0.05, row[\"PC2\"] + 0.12, str(row[\"Label\"]),\n",
        "        fontsize=9, color=\"gray\", alpha=0.6, ha=\"center\", va=\"bottom\"\n",
        "    )\n",
        "\n",
        "ax_scatter.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
        "ax_scatter.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
        "\n",
        "# legend (dedup + sorted)\n",
        "handles, labels = ax_scatter.get_legend_handles_labels()\n",
        "pairs = sorted({(lab, h) for lab, h in zip(labels, handles)}, key=lambda x: x[0])\n",
        "if pairs:\n",
        "    sorted_labels, sorted_handles = zip(*pairs)\n",
        "    ax_scatter.legend(sorted_handles, sorted_labels, frameon=True, title=\"XGroup | HueGroup\", fontsize=9)\n",
        "\n",
        "# 3) Loadings barplot\n",
        "sns.barplot(\n",
        "    data=loadings_melted, y=\"index\", x=\"Loading\",\n",
        "    hue=\"PC\", hue_order=[\"PC1\", \"PC2\"],\n",
        "    ax=ax_bar, palette=[\"purple\", \"orange\"], alpha=0.5\n",
        ")\n",
        "ax_bar.set_xlabel(\"Loading Weight\")\n",
        "ax_bar.set_title(\"Top 8 PC Loadings (metrics only)\", fontsize=14)\n",
        "ax_bar.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
        "ax_bar.set_ylabel(\"\")\n",
        "ax_bar.legend(title=\"\", frameon=False, fontsize=9)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}