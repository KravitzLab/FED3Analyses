{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/FED3Analyses/blob/PsygeneL0-to-L1-QC/PsygeneBEAML0_to_L1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ByYgNRzBXRg"
      },
      "source": [
        "# This is a notebook for quality control and renaming BEAM files from L0 to L1 for Psygene.  \n",
        "<br>\n",
        "<img src=\"https://github.com/KravitzLab/KreedLabWiki/blob/main/images/ChatGPT%20Image%20Apr%2020,%202025,%2004_05_24%20PM.png?raw=true\" width=\"300\" />\n",
        "\n",
        "Updated: 07 28 25\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Cbn8lmxKhJsY"
      },
      "outputs": [],
      "source": [
        "# @title Import libraries\n",
        "# Install Required Packages (if missing) ---\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "required_packages = {\n",
        "    \"ipywidgets\": \"ipywidgets\",\n",
        "    \"openpyxl\": \"openpyxl\"\n",
        "}\n",
        "\n",
        "for pkg, name in required_packages.items():\n",
        "    if importlib.util.find_spec(pkg) is None:\n",
        "        print(f\"Installing {name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", name])\n",
        "\n",
        "# ---Imports ---\n",
        "import os, re, zipfile, shutil, warnings, io\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "from scipy.optimize import curve_fit\n",
        "from statsmodels.formula.api import ols\n",
        "import statsmodels.api as sm\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from google.colab import files, output\n",
        "\n",
        "# ---  Enable custom widgets & suppress warnings ---\n",
        "output.enable_custom_widget_manager()\n",
        "warnings.filterwarnings('ignore')  # ‚ùó Use with caution\n",
        "\n",
        "print(\" All packages ready and environment set up.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHGcTfSd82q0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Upload BEAM L0 files and Key\n",
        "\n",
        "\n",
        "# --- Upload Files ---\n",
        "print(\"Upload your ZIP file containing CSVs\")\n",
        "zip_upload = files.upload()\n",
        "\n",
        "print(\"Upload your XLSX key file\")\n",
        "xlsx_upload = files.upload()\n",
        "\n",
        "zip_filename = list(zip_upload.keys())[0]\n",
        "xlsx_filename = list(xlsx_upload.keys())[0]\n",
        "key_df = pd.read_excel(xlsx_filename)\n",
        "\n",
        "# --- Step 3: Extract, match by BEAM within window, and group ---\n",
        "\n",
        "import re, io, zipfile\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "MATCH_WINDOW_DAYS = 15  # change if needed\n",
        "\n",
        "# 1) Normalize the key\n",
        "key_df = key_df.copy()\n",
        "\n",
        "def normalize_beam_value(x):\n",
        "    if pd.isna(x):\n",
        "        return None\n",
        "    m = re.search(r\"(\\d{1,4})\", str(x))\n",
        "    return m.group(1).zfill(3) if m else None\n",
        "\n",
        "if \"BEAM\" not in key_df.columns:\n",
        "    raise ValueError(\"Key file is missing a 'BEAM' column.\")\n",
        "\n",
        "key_df[\"BEAM_norm\"] = key_df[\"BEAM\"].apply(normalize_beam_value)\n",
        "\n",
        "# Parse/normalize FED_StartDate\n",
        "if \"FED_StartDate\" not in key_df.columns:\n",
        "    raise ValueError(\"Key file is missing a 'FED_StartDate' column.\")\n",
        "key_df[\"FED_StartDate_parsed\"] = pd.to_datetime(key_df[\"FED_StartDate\"], errors=\"coerce\")\n",
        "\n",
        "# 2) Helpers for filename parsing (supports several patterns)\n",
        "def parse_beam_and_date_from_name(name: str):\n",
        "    \"\"\"\n",
        "    Return (beam_id_3digit, file_date_dateobj) or (None, None) if no match.\n",
        "    Tries multiple filename patterns.\n",
        "    \"\"\"\n",
        "    base = Path(name).name\n",
        "    patterns = [\n",
        "        (r\"BEAM(\\d{3})_(\\d{10})\\.csv\", \"%Y%m%d%H\"),      # BEAM023_2025030800.csv\n",
        "        (r\"BEAM(\\d{1,4})_(\\d{8})\\.csv\", \"%Y%m%d\"),       # BEAM23_20250308.csv\n",
        "        (r\"BEAM\\s*(\\d{1,4}).*?(\\d{4}-\\d{2}-\\d{2})\", \"%Y-%m-%d\"),  # BEAM 23 ... 2025-03-08.csv\n",
        "        (r\"(\\d{8})_BEAM(\\d{1,4})\\.csv\", \"%Y%m%d\"),       # 20250308_BEAM23.csv\n",
        "    ]\n",
        "    for pat, fmt in patterns:\n",
        "        m = re.search(pat, base, flags=re.IGNORECASE)\n",
        "        if not m:\n",
        "            continue\n",
        "        # determine which group is beam/date based on pattern orientation\n",
        "        if pat.startswith(r\"(\\d{8})_BEAM\"):\n",
        "            date_raw, beam_raw = m.group(1), m.group(2)\n",
        "        else:\n",
        "            beam_raw, date_raw = m.group(1), m.group(2)\n",
        "        try:\n",
        "            beam_id = str(int(beam_raw)).zfill(3)\n",
        "            file_dt = datetime.strptime(date_raw, fmt).date()  # date-only\n",
        "            return beam_id, file_dt\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None, None\n",
        "\n",
        "# 3) Extract/scan the ZIP, match to key within window\n",
        "beam_data = defaultdict(list)   # for the big BEAM_data table\n",
        "raw_beam_files = {}             # filename-> DataFrame (unique per match)\n",
        "unmatched_reasons = []          # collect diagnostics\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    for file in zip_ref.namelist():\n",
        "        if not file.lower().endswith(\".csv\"):\n",
        "            continue\n",
        "\n",
        "        beam_id, file_date = parse_beam_and_date_from_name(file)\n",
        "        if beam_id is None or file_date is None:\n",
        "            unmatched_reasons.append((file, \"Filename not parsed\"))\n",
        "            continue\n",
        "\n",
        "        # load CSV\n",
        "        with zip_ref.open(file) as f:\n",
        "            df = pd.read_csv(f)\n",
        "        df[\"BEAM\"] = beam_id\n",
        "        df[\"file_date\"] = pd.to_datetime(file_date)\n",
        "\n",
        "        # For the big table later\n",
        "        df[\"source_file\"] = file\n",
        "        beam_data[int(beam_id)].append(df.copy())\n",
        "\n",
        "        # All possible key matches for this BEAM\n",
        "        key_matches = key_df[key_df[\"BEAM_norm\"] == beam_id].copy()\n",
        "        key_matches = key_matches.dropna(subset=[\"FED_StartDate_parsed\"])\n",
        "\n",
        "        if key_matches.empty:\n",
        "            unmatched_reasons.append((file, f\"No BEAM {beam_id} found in key\"))\n",
        "            continue\n",
        "\n",
        "        # Date-only comparison, allow multiple cohort matches\n",
        "        matched_any = False\n",
        "        for _, row in key_matches.iterrows():\n",
        "            start_date = pd.to_datetime(row[\"FED_StartDate_parsed\"]).date()\n",
        "            if start_date is None:\n",
        "                continue\n",
        "            if abs((file_date - start_date).days) <= MATCH_WINDOW_DAYS:\n",
        "                df_match = df.copy()\n",
        "                # Attach cohort columns if present\n",
        "                if \"Mouse_ID\" in row:\n",
        "                    df_match[\"Mouse_ID\"] = row[\"Mouse_ID\"]\n",
        "                if \"Cohort\" in row:\n",
        "                    df_match[\"Cohort\"] = row[\"Cohort\"]\n",
        "\n",
        "                # Store under a unique key to avoid overwriting if multiple matches\n",
        "                unique_key = f\"{file}__{row.get('Mouse_ID', 'unknown')}\"\n",
        "                raw_beam_files[unique_key] = df_match\n",
        "                matched_any = True\n",
        "\n",
        "        if not matched_any:\n",
        "            # Report nearest distance for debugging\n",
        "            dists = (key_matches[\"FED_StartDate_parsed\"].dt.date.apply(\n",
        "                lambda d: abs((file_date - d).days)))\n",
        "            nearest = int(dists.min()) if len(dists) else None\n",
        "            unmatched_reasons.append((file, f\"No key date within {MATCH_WINDOW_DAYS} days (nearest={nearest})\"))\n",
        "\n",
        "# 4) Build BEAM_data and merged outputs (unchanged behavior, but robust to multiples)\n",
        "# Combine all matched dfs\n",
        "if len(raw_beam_files) == 0:\n",
        "    raise SystemExit(\"No files matched within the window. See diagnostics printed above.\")\n",
        "\n",
        "all_matched_dfs = list(raw_beam_files.values())\n",
        "BEAM_data = pd.concat(all_matched_dfs, ignore_index=True)\n",
        "\n",
        "# Group by (BEAM, Mouse_ID) and concatenate\n",
        "beam_grouped = defaultdict(list)\n",
        "for fname, df in raw_beam_files.items():\n",
        "    beam_id = df[\"BEAM\"].iloc[0]\n",
        "    mouse_id = df[\"Mouse_ID\"].iloc[0] if \"Mouse_ID\" in df.columns else \"unknown\"\n",
        "    group_key = (beam_id, mouse_id)\n",
        "    beam_grouped[group_key].append(df)\n",
        "\n",
        "merged_beam_files = {}\n",
        "for (beam_id, mouse_id), dfs in beam_grouped.items():\n",
        "    merged_df = pd.concat(dfs, ignore_index=True)\n",
        "    # Convert datetime column if present\n",
        "    if \"datetime\" in merged_df.columns:\n",
        "        merged_df[\"datetime\"] = pd.to_datetime(merged_df[\"datetime\"], errors=\"coerce\")\n",
        "        merged_df.sort_values(\"datetime\", inplace=True)\n",
        "    fname = f\"BEAM{str(beam_id).zfill(3)}_{mouse_id}.csv\"\n",
        "    merged_beam_files[fname] = merged_df\n",
        "\n",
        "# Also update BEAM_data index if 'datetime' exists\n",
        "if \"datetime\" in BEAM_data.columns:\n",
        "    BEAM_data[\"datetime\"] = pd.to_datetime(BEAM_data[\"datetime\"], errors=\"coerce\")\n",
        "    BEAM_data.set_index(\"datetime\", inplace=True)\n",
        "\n",
        "# 5) Reporting\n",
        "print(f\"Combined {len(all_matched_dfs)} matched files into {len(merged_beam_files)} unique device+mouse merged files.\")\n",
        "print(f\"Final BEAM_data shape: {BEAM_data.shape}\")\n",
        "if unmatched_reasons:\n",
        "    print(\"\\nUnmatched/diagnostic reasons (first 25 shown):\")\n",
        "    for fn, reason in unmatched_reasons[:25]:\n",
        "        print(f\"  - {fn}: {reason}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vm8CrGu-gQ9x"
      },
      "outputs": [],
      "source": [
        "# @title Plot all files\n",
        "# Get the number of unique Animal_IDs\n",
        "num_animals = BEAM_data[\"Mouse_ID\"].nunique()\n",
        "\n",
        "# Generate a color palette with enough distinct colors\n",
        "palette = sns.color_palette(\"husl\", num_animals)\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "sns.lineplot(\n",
        "    data=BEAM_data,\n",
        "    x=\"datetime\",\n",
        "    y=\"activity_percent\",\n",
        "    hue=\"Mouse_ID\",\n",
        "    palette=palette,\n",
        "    alpha=0.7,\n",
        "    linewidth=0.8,\n",
        ")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Activity (%)\")\n",
        "plt.title(\"Activity Over Time\")\n",
        "sns.despine()\n",
        "plt.legend().remove()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03hxDpRZAbBY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Plot individual mice\n",
        "\n",
        "\n",
        "unique_subjects = list(BEAM_data['Mouse_ID'].dropna().unique())\n",
        "subject_indices = {i: subj for i, subj in enumerate(unique_subjects)}\n",
        "\n",
        "plot_output = widgets.Output()\n",
        "\n",
        "def plot_subject(i):\n",
        "    subject = subject_indices[i]\n",
        "    df = BEAM_data[BEAM_data['Mouse_ID'] == subject].copy()\n",
        "    df = df.reset_index()\n",
        "\n",
        "    with plot_output:\n",
        "        plot_output.clear_output(wait=True)\n",
        "        if df.empty:\n",
        "            print(f\"No data for subject: {subject}\")\n",
        "            return\n",
        "\n",
        "        # Compute duration string\n",
        "        duration = df['datetime'].max() - df['datetime'].min()\n",
        "        days = duration.days\n",
        "        hours = duration.seconds // 3600\n",
        "        duration_str = f\"{days} days, {hours} hours\"\n",
        "\n",
        "        # Compute average activity\n",
        "        avg_activity = df[\"activity_percent\"].dropna().mean() * 100\n",
        "        avg_str = f\"{avg_activity:.1f}%\"\n",
        "\n",
        "        # Plot\n",
        "        fig, ax = plt.subplots(figsize=(10, 3))\n",
        "        sns.lineplot(data=df, x=\"datetime\", y=\"activity_percent\", alpha=0.5, ax=ax)\n",
        "        ax.set_ylabel(\"Activity\")\n",
        "        ax.set_xlabel(\"Time\")\n",
        "        ax.set_title(f\"Mouse_ID: {subject}    Duration: {duration_str}    Avg Activity: {avg_str}\")\n",
        "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d\\n%H:%M'))\n",
        "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "        sns.despine()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "slider = widgets.IntSlider(\n",
        "    min=0,\n",
        "    max=len(unique_subjects) - 1,\n",
        "    step=1,\n",
        "    description=\"Subject Index:\",\n",
        "    layout=widgets.Layout(width='500px'),\n",
        "    style={'handle_color': '#444'}\n",
        ")\n",
        "\n",
        "subject_label = widgets.HTML(\n",
        "    value=f\"<b>Subject:</b> {subject_indices[0]}\",\n",
        "    layout=widgets.Layout(margin='0 0 10px 0')\n",
        ")\n",
        "\n",
        "def on_slider_change(change):\n",
        "    i = change['new']\n",
        "    subject_label.value = f\"<b>Subject:</b> {subject_indices[i]}\"\n",
        "    plot_subject(i)\n",
        "\n",
        "slider.observe(on_slider_change, names='value')\n",
        "\n",
        "ui = widgets.VBox([subject_label, slider, plot_output])\n",
        "display(ui)\n",
        "plot_subject(0)\n",
        "\n",
        "# Compute average activity and lux per subject\n",
        "activity_summary = (\n",
        "    BEAM_data.groupby(\"Mouse_ID\")[[\"activity_percent\", \"lux\"]]\n",
        "    .mean()\n",
        "    .dropna()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Convert activity to percentage and round both columns\n",
        "activity_summary[\"Avg_Activity (%)\"] = (activity_summary[\"activity_percent\"] * 100).round(1)\n",
        "activity_summary[\"Avg_Lux\"] = activity_summary[\"lux\"].round(1)\n",
        "\n",
        "# Keep only necessary columns and sort\n",
        "activity_summary = activity_summary[[\"Mouse_ID\", \"Avg_Activity (%)\", \"Avg_Lux\"]]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7EyYGaw2_6Gl"
      },
      "outputs": [],
      "source": [
        "# @title QC and manual file inclution\n",
        "\n",
        "MIN_AVG_ACTIVITY = 5.0  # %\n",
        "MAX_ZERO_DURATION = timedelta(hours=8)\n",
        "MAX_GAP_DURATION = timedelta(hours=2)\n",
        "\n",
        "passed_files = []\n",
        "flagged_files = {}\n",
        "print(f\"Running QC on {len(merged_beam_files)} merged files...\")\n",
        "def has_long_zero_percent(df, threshold=MAX_ZERO_DURATION):\n",
        "    zero_mask = df[\"activity_percent\"] == 0\n",
        "    if zero_mask.sum() == 0:\n",
        "        return False\n",
        "    df = df.copy()\n",
        "    df[\"gap\"] = zero_mask.astype(int)\n",
        "    df[\"block\"] = (df[\"gap\"].diff(1) != 0).cumsum() * df[\"gap\"]\n",
        "    for _, group in df[df[\"block\"] > 0].groupby(\"block\"):\n",
        "        duration = group.index[-1] - group.index[0]\n",
        "        if duration >= threshold:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "flagged_files = {}\n",
        "passed_files = []\n",
        "\n",
        "for fname, df in merged_beam_files.items():\n",
        "    df = df.copy()\n",
        "\n",
        "    if \"datetime\" not in df.columns:\n",
        "        if df.index.name == \"datetime\":\n",
        "            df = df.reset_index()\n",
        "        else:\n",
        "            raise KeyError(f\"'datetime' column not found in file: {fname}\")\n",
        "\n",
        "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
        "    df.set_index(\"datetime\", inplace=True)\n",
        "    df.sort_index(inplace=True)\n",
        "\n",
        "    reasons = []\n",
        "\n",
        "    avg_pct = df[\"activity_percent\"].mean() * 100\n",
        "    if avg_pct < MIN_AVG_ACTIVITY:\n",
        "        reasons.append(f\"Low avg activity ({avg_pct:.2f}%)\")\n",
        "\n",
        "    gap = df.index.to_series().diff().max()\n",
        "    if gap > MAX_GAP_DURATION:\n",
        "        gap_minutes = round(gap.total_seconds() / 60)\n",
        "        reasons.append(f\"Data gap >2hr (max: {gap_minutes} min)\")\n",
        "\n",
        "    if has_long_zero_percent(df):\n",
        "        reasons.append(\"Zero activity >8hrs\")\n",
        "\n",
        "    if reasons:\n",
        "        flagged_files[fname] = reasons\n",
        "    else:\n",
        "        passed_files.append(fname)\n",
        "\n",
        "# üëÅÔ∏è‚Äçüó®Ô∏è Manual Review with Slider\n",
        "flagged_fnames = list(flagged_files.keys())\n",
        "approved_files = passed_files.copy()\n",
        "override_files = []\n",
        "rejected_files = []\n",
        "inclusion_map = {}\n",
        "decision_log = {}\n",
        "\n",
        "if flagged_fnames:\n",
        "    print(f\"\\n{len(flagged_fnames)} files flagged. Launching manual review...\")\n",
        "\n",
        "    plot_output = widgets.Output()\n",
        "    button_output = widgets.Output()\n",
        "    decision_log_output = widgets.Output()\n",
        "\n",
        "    def update_plot(change=None):\n",
        "      fname = flagged_fnames[slider.value]\n",
        "      df = merged_beam_files[fname].copy()\n",
        "      df = df.reset_index() if \"datetime\" not in df.columns else df.copy()\n",
        "      df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
        "\n",
        "      with plot_output:\n",
        "          clear_output(wait=True)\n",
        "          plt.figure(figsize=(10, 4))\n",
        "          plt.plot(df[\"datetime\"], df[\"activity_percent\"] * 100, label=\"Activity %\")\n",
        "          plt.title(fname)\n",
        "          plt.ylabel(\"Activity %\")\n",
        "          plt.xlabel(\"Time\")\n",
        "          plt.grid(True)\n",
        "          plt.xticks(rotation=45)\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n",
        "\n",
        "      with button_output:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"\\nReviewing file {slider.value + 1} of {len(flagged_fnames)}: {fname}\")\n",
        "            print(\"Reason(s):\", \" | \".join(flagged_files[fname]))\n",
        "            status = inclusion_map.get(fname, None)\n",
        "            print(\"Current decision:\",\n",
        "                  \"Included\" if status else \"Excluded\" if status is False else \"Undecided\")\n",
        "\n",
        "            include_btn = widgets.Button(description=\"Include\", button_style='success')\n",
        "            exclude_btn = widgets.Button(description=\"Exclude\", button_style='danger')\n",
        "\n",
        "            def handle_decision(decision):\n",
        "                if fname in approved_files: approved_files.remove(fname)\n",
        "                if fname in override_files: override_files.remove(fname)\n",
        "                if fname in rejected_files: rejected_files.remove(fname)\n",
        "\n",
        "                inclusion_map[fname] = decision\n",
        "                if decision:\n",
        "                    approved_files.append(fname)\n",
        "                    override_files.append(fname)\n",
        "                else:\n",
        "                    rejected_files.append(fname)\n",
        "\n",
        "                log_decision(fname, decision)\n",
        "                if slider.value < slider.max:\n",
        "                    slider.value += 1\n",
        "                else:\n",
        "                    print(\"Review complete.\")\n",
        "\n",
        "            include_btn.on_click(lambda _: handle_decision(True))\n",
        "            exclude_btn.on_click(lambda _: handle_decision(False))\n",
        "            display(widgets.HBox([include_btn, exclude_btn]))\n",
        "\n",
        "    def log_decision(fname, decision):\n",
        "        decision_log[fname] = f\"{'Included' if decision else 'Excluded'}: {fname}\"\n",
        "        with decision_log_output:\n",
        "            clear_output(wait=True)\n",
        "            for line in decision_log.values():\n",
        "                print(line)\n",
        "\n",
        "    slider = widgets.IntSlider(min=0, max=len(flagged_fnames) - 1, step=1, description=\"File\")\n",
        "    slider.observe(update_plot, names='value')\n",
        "\n",
        "    display(slider, plot_output, button_output, widgets.Label(\"Decision log:\"), decision_log_output)\n",
        "    update_plot()\n",
        "else:\n",
        "    print(\"No files flagged. All passed QC.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xLLyJGYE6R5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Renaming approved files and updating Key\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, shutil, zipfile, re\n",
        "import pandas as pd\n",
        "\n",
        "# Optional: Colab download helper\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "except Exception:\n",
        "    colab_files = None\n",
        "\n",
        "# ---------- Helper: normalize BEAM to 3-digit digits-only string ----------\n",
        "def normalize_beam_value(x):\n",
        "    \"\"\"\n",
        "    Return a 3-digit, digits-only BEAM string (e.g., 12 -> '012', 'BEAM 12' -> '012').\n",
        "    Returns None for missing/unparseable values.\n",
        "    \"\"\"\n",
        "    if pd.isna(x):\n",
        "        return None\n",
        "    s = str(x)\n",
        "    m = re.search(r\"(\\d+)\", s)\n",
        "    if not m:\n",
        "        return None\n",
        "    d = m.group(1)              # digits only\n",
        "    d = d.lstrip(\"0\") or \"0\"    # keep '0' if all zeros\n",
        "    return d.zfill(3)\n",
        "\n",
        "# ---------- Clean and prepare output directory ----------\n",
        "renamed_dir = \"renamed_files\"\n",
        "if os.path.exists(renamed_dir):\n",
        "    shutil.rmtree(renamed_dir)\n",
        "os.makedirs(renamed_dir)\n",
        "\n",
        "flagged_log = []\n",
        "skipped_files = []\n",
        "\n",
        "# ---------- KEY NORMALIZATION ----------\n",
        "key_df = key_df.copy()\n",
        "key_df[\"BEAM_norm\"] = key_df[\"BEAM\"].apply(normalize_beam_value)\n",
        "key_df = key_df.dropna(subset=[\"BEAM_norm\"])  # drop rows without a usable BEAM\n",
        "key_index = key_df.set_index(\"BEAM_norm\")\n",
        "\n",
        "# Create column to store review notes\n",
        "session_col = \"BEAM_EX\"\n",
        "if session_col not in key_df.columns:\n",
        "    key_df[session_col] = pd.NA\n",
        "\n",
        "# ---------- Rename only QC-approved merged files ----------\n",
        "for fname, df in merged_beam_files.items():\n",
        "    if fname not in approved_files:\n",
        "        continue  # skip excluded or unreviewed files\n",
        "\n",
        "    try:\n",
        "        beam_id_norm = normalize_beam_value(df[\"BEAM\"].iloc[0])\n",
        "        if beam_id_norm is None:\n",
        "            raise ValueError(\"BEAM missing/unparseable\")\n",
        "\n",
        "        mouse_id = str(df[\"Mouse_ID\"].iloc[0])\n",
        "\n",
        "        # file_date may be per-row; coerce and take earliest valid date\n",
        "        file_dates = pd.to_datetime(df[\"file_date\"], errors=\"coerce\")\n",
        "        if file_dates.notna().any():\n",
        "            date_fmt = file_dates.min().strftime(\"%Y%m%d\")\n",
        "        else:\n",
        "            raise ValueError(\"file_date missing/unparseable\")\n",
        "\n",
        "    except Exception as e:\n",
        "        skipped_files.append((fname, f\"Metadata extraction error: {e}\"))\n",
        "        continue\n",
        "\n",
        "    if beam_id_norm not in key_index.index:\n",
        "        skipped_files.append((fname, f\"BEAM ID not found in key (searched='{beam_id_norm}')\"))\n",
        "        continue\n",
        "\n",
        "    # If you also want BEAM in the new filename, use:\n",
        "    # new_fname = f\"{mouse_id}_BEAM{beam_id_norm}_{date_fmt}.csv\"\n",
        "    new_fname = f\"{mouse_id}_BEAM_{date_fmt}.csv\"\n",
        "\n",
        "    df.to_csv(os.path.join(renamed_dir, new_fname), index=False)\n",
        "\n",
        "# ---------- Update flagged notes in key (use normalized BEAM) ----------\n",
        "for fname, reason in flagged_files.items():\n",
        "    if fname not in merged_beam_files:\n",
        "        continue\n",
        "    df = merged_beam_files[fname]\n",
        "    beam_id_norm = normalize_beam_value(df[\"BEAM\"].iloc[0])\n",
        "    if beam_id_norm is None:\n",
        "        continue\n",
        "    key_df.loc[key_df[\"BEAM_norm\"] == beam_id_norm, session_col] = \" | \".join(reason)\n",
        "    flagged_log.append((beam_id_norm, \" | \".join(reason)))\n",
        "\n",
        "# ---------- Identify Animal_IDs with no matching file ----------\n",
        "no_file_log = []\n",
        "key_modified = False\n",
        "\n",
        "merged_keys = {\n",
        "    (normalize_beam_value(df[\"BEAM\"].iloc[0]), str(df[\"Mouse_ID\"].iloc[0]))\n",
        "    for df in merged_beam_files.values()\n",
        "}\n",
        "\n",
        "for _, row in key_df.iterrows():\n",
        "    beam_id_norm = row[\"BEAM_norm\"]\n",
        "    mouse_id = str(row[\"Mouse_ID\"])\n",
        "    if (beam_id_norm, mouse_id) not in merged_keys:\n",
        "        condition = (key_df[\"BEAM_norm\"] == beam_id_norm) & (key_df[\"Mouse_ID\"] == mouse_id)\n",
        "        key_df.loc[condition, session_col] = \"no file\"\n",
        "        no_file_log.append((beam_id_norm, mouse_id))\n",
        "        key_modified = True\n",
        "\n",
        "# ---------- Extract base names from uploaded files (robust) ----------\n",
        "zip_base = os.path.splitext(os.path.basename(str(zip_filename))) if 'zip_filename' in globals() else (\"output\", \".zip\")\n",
        "zip_base = zip_base[0]\n",
        "key_base = os.path.splitext(os.path.basename(str(xlsx_filename))) if 'xlsx_filename' in globals() else (\"key\", \".xlsx\")\n",
        "key_base = key_base[0]\n",
        "\n",
        "# ---------- Save updated key (keeps BEAM_norm; you can drop original BEAM if you want) ----------\n",
        "key_out_path = f\"{key_base}_updated.xlsx\"\n",
        "key_df.to_excel(key_out_path, index=False)\n",
        "\n",
        "# ---------- Zip renamed files ----------\n",
        "zip_out_path = f\"{zip_base.replace('_L0', '_L1')}.zip\"\n",
        "with zipfile.ZipFile(zip_out_path, \"w\") as zipf:\n",
        "    for f in os.listdir(renamed_dir):\n",
        "        zipf.write(os.path.join(renamed_dir, f), arcname=f)\n",
        "\n",
        "# ---------- Summary report ----------\n",
        "print(f\"\\n{len(no_file_log)} Mouse_IDs in key had no merged BEAM file:\")\n",
        "for beam, mouse in no_file_log:\n",
        "    print(f\"  - BEAM {beam}, Mouse {mouse}\")\n",
        "\n",
        "print(f\"\\nRenamed {len(os.listdir(renamed_dir))} approved merged files.\")\n",
        "print(f\"Skipped {len(skipped_files)} files:\")\n",
        "for fn, reason in skipped_files:\n",
        "    print(f\"  - {fn}: {reason}\")\n",
        "print(f\"Flagged {len(flagged_log)} files updated in key.\")\n",
        "\n",
        "# ---------- Apply Excel styling (red/orange fills) ----------\n",
        "# ---------- Apply Excel styling (strict rules) ----------\n",
        "try:\n",
        "    from openpyxl import load_workbook\n",
        "    from openpyxl.styles import PatternFill\n",
        "\n",
        "    # Build helper to normalize BEAM\n",
        "    def _norm_beam(x):\n",
        "        if pd.isna(x): return None\n",
        "        m = re.search(r\"(\\d+)\", str(x))\n",
        "        if not m: return None\n",
        "        d = m.group(1)\n",
        "        d = d.lstrip(\"0\") or \"0\"\n",
        "        return d.zfill(3)\n",
        "\n",
        "    # Sets for logic\n",
        "    merged_keys = {\n",
        "        (_norm_beam(df[\"BEAM\"].iloc[0]), str(df[\"Mouse_ID\"].iloc[0]))\n",
        "        for df in merged_beam_files.values()\n",
        "    }\n",
        "\n",
        "    approved_pairs = set()\n",
        "    if isinstance(approved_files, (set, list, tuple)):\n",
        "        for fname in approved_files:\n",
        "            if fname in merged_beam_files:\n",
        "                dfm = merged_beam_files[fname]\n",
        "                approved_pairs.add((_norm_beam(dfm[\"BEAM\"].iloc[0]), str(dfm[\"Mouse_ID\"].iloc[0])))\n",
        "\n",
        "    flagged_pairs = set()\n",
        "    if isinstance(flagged_files, dict):\n",
        "        for fname in flagged_files.keys():\n",
        "            if fname in merged_beam_files:\n",
        "                dfm = merged_beam_files[fname]\n",
        "                flagged_pairs.add((_norm_beam(dfm[\"BEAM\"].iloc[0]), str(dfm[\"Mouse_ID\"].iloc[0])))\n",
        "\n",
        "    wb = load_workbook(key_out_path)\n",
        "    ws = wb.active  # first sheet\n",
        "\n",
        "    # Header map\n",
        "    header_to_col = {}\n",
        "    for col in range(1, ws.max_column + 1):\n",
        "        val = ws.cell(row=1, column=col).value\n",
        "        if val is not None:\n",
        "            header_to_col[str(val)] = col\n",
        "\n",
        "    # Ensure target column exists\n",
        "    def ensure_column(col_name: str) -> int:\n",
        "        if col_name in header_to_col and header_to_col[col_name]:\n",
        "            return header_to_col[col_name]\n",
        "        new_idx = ws.max_column + 1\n",
        "        ws.cell(row=1, column=new_idx).value = col_name\n",
        "        header_to_col[col_name] = new_idx\n",
        "        return new_idx\n",
        "\n",
        "    red_fill    = PatternFill(fill_type=\"solid\", start_color=\"FFFFC7CE\", end_color=\"FFFFC7CE\")\n",
        "    orange_fill = PatternFill(fill_type=\"solid\", start_color=\"FFFFE5B2\", end_color=\"FFFFE5B2\")\n",
        "\n",
        "    respect_existing_fill = True\n",
        "    def has_custom_fill(cell):\n",
        "        pt = getattr(cell.fill, \"patternType\", None)\n",
        "        if pt and pt != \"none\":\n",
        "            rgb = getattr(cell.fill.start_color, \"rgb\", None)\n",
        "            return rgb not in (None, \"00000000\")\n",
        "        return False\n",
        "\n",
        "    # We‚Äôll style BEAM_EX only when a tag applies; otherwise no fill.\n",
        "    col_idx = ensure_column(session_col)\n",
        "    key_iter = key_df.reset_index(drop=True)\n",
        "\n",
        "    for i, row in key_iter.iterrows():\n",
        "        excel_row = i + 2  # header row is 1\n",
        "        pair = (row.get(\"BEAM_norm\", None), str(row.get(\"Mouse_ID\", \"\")))\n",
        "\n",
        "        # Decide tag per your rules\n",
        "        if pair not in merged_keys:\n",
        "            tag = \"no_file\"              # RED\n",
        "        elif pair in flagged_pairs and pair in approved_pairs:\n",
        "            tag = \"include\"              # ORANGE (failed QC but manually included)\n",
        "        elif pair in flagged_pairs and pair not in approved_pairs:\n",
        "            tag = \"reject\"               # RED (manually rejected)\n",
        "        else:\n",
        "            tag = None                   # no color\n",
        "\n",
        "        # Keep the existing text value in BEAM_EX\n",
        "        val = row.get(session_col, pd.NA)\n",
        "        cell = ws.cell(row=excel_row, column=col_idx)\n",
        "        cell.value = \"\" if pd.isna(val) else str(val)\n",
        "\n",
        "        # Apply fill only if a tag exists; otherwise, leave as-is (no color)\n",
        "        if tag and not (respect_existing_fill and has_custom_fill(cell)):\n",
        "            if tag in (\"reject\", \"no_file\"):\n",
        "                cell.fill = red_fill\n",
        "            elif tag == \"include\":\n",
        "                cell.fill = orange_fill\n",
        "        # If tag is None, do not touch cell.fill\n",
        "\n",
        "    wb.save(key_out_path)\n",
        "    print(f\"\\nStyled workbook saved to: {os.path.abspath(key_out_path)}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n(Styling skipped) Could not style Excel: {e}\")\n",
        "\n",
        "# ---------- Downloads (Colab) ----------\n",
        "if colab_files is not None:\n",
        "    try:\n",
        "        colab_files.download(key_out_path)\n",
        "        colab_files.download(zip_out_path)\n",
        "    except Exception as e:\n",
        "        print(f\"(Download hint) Could not auto-download: {e}\\nFiles saved as:\\n - {key_out_path}\\n - {zip_out_path}\")\n",
        "else:\n",
        "    print(f\"(Local) Files saved as:\\n - {key_out_path}\\n - {zip_out_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}