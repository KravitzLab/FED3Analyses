{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/FED3Analyses/blob/main/FED3_Bandit_V1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This notebook analyzes FED3 Bandit data\n",
        "<br>\n",
        "<img src=\"https://fed3bandit.readthedocs.io/en/latest/_static/fed3bandit_logo1.svg\" width=\"200\" />\n",
        "\n",
        "\n",
        "Authors: Chantelle Murrell and Sebastian Alves<br>\n",
        "Updated: 12-01-25  \n",
        "Version 1.0.1"
      ],
      "metadata": {
        "id": "ru9_Mj2OpjhT"
      },
      "id": "ru9_Mj2OpjhT"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install libraries and import them {\"run\":\"auto\"}\n",
        "\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Packages to ensure are installed (add others here if you like)\n",
        "packages = {\n",
        "    \"fed3\": \"git+https://github.com/earnestt1234/fed3.git\",\n",
        "    \"fed3bandit\": \"fed3bandit\",\n",
        "    \"pingouin\": \"pingouin\",\n",
        "    \"ipydatagrid\": \"ipydatagrid\",\n",
        "    \"openpyxl\": \"openpyxl\",\n",
        "}\n",
        "\n",
        "for name, source in packages.items():\n",
        "    if importlib.util.find_spec(name) is None:\n",
        "        print(f\"Installing {name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", source])\n",
        "\n",
        "# ----------------------------\n",
        "# Imports\n",
        "# ----------------------------\n",
        "# Standard library\n",
        "import copy\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tempfile\n",
        "import threading\n",
        "import time\n",
        "import warnings\n",
        "import zipfile\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "from os.path import basename, splitext\n",
        "\n",
        "# Third-party\n",
        "from ipydatagrid import DataGrid, TextRenderer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "import fed3\n",
        "import fed3.plot as fplot\n",
        "import fed3bandit as f3b\n",
        "from scipy.stats import f_oneway\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.gridspec as gridspec\n",
        "from google.colab import files\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration\n",
        "# ----------------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.rcParams.update({'font.size': 12, 'figure.autolayout': True})\n",
        "plt.rcParams['figure.figsize'] = [6, 4]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "print(\"Packages installed and imports ready.\")\n"
      ],
      "metadata": {
        "id": "c_vNg7dvVfCh",
        "cellView": "form"
      },
      "id": "c_vNg7dvVfCh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload files\n",
        "\n",
        "# Reset caches to avoid duplicates if you re-run this cell\n",
        "feds, loaded_files, session_types = [], [], []\n",
        "\n",
        "def extract_session_type(csv_path, fallback=\"Unknown\"):\n",
        "    \"\"\"Read 'Session_Type ' or variants; return first non-empty value.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, sep=None, engine='python', dtype=str)\n",
        "        df.columns = [c.strip() for c in df.columns]\n",
        "        lower = {c.casefold(): c for c in df.columns}\n",
        "        for cand in [\"session_type\", \"session type\", \"sessiontype\", \"session\"]:\n",
        "            if cand in lower:\n",
        "                col = lower[cand]\n",
        "                vals = df[col].dropna().astype(str).str.strip()\n",
        "                vals = vals[vals.ne(\"\")]\n",
        "                if not vals.empty:\n",
        "                    return vals.iloc[0]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return fallback\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for name, data in uploaded.items():\n",
        "    if name.lower().endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(io.BytesIO(data)) as zf:\n",
        "            for zi in zf.infolist():\n",
        "                if not zi.filename.lower().endswith(\".csv\"):\n",
        "                    continue\n",
        "                file_data = zf.read(zi)\n",
        "                if len(file_data) <= 1024:\n",
        "                    continue\n",
        "                with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".csv\", delete=False) as tmp:\n",
        "                    tmp.write(file_data); tmp_path = tmp.name\n",
        "                try:\n",
        "                    session_type = extract_session_type(tmp_path)\n",
        "                    df = fed3.load(tmp_path)\n",
        "                    df.name = os.path.basename(zi.filename)\n",
        "                    df.attrs = {\"Session_type\": session_type}\n",
        "                    feds.append(df)\n",
        "                    loaded_files.append(os.path.basename(zi.filename))\n",
        "                    session_types.append(session_type)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {zi.filename}: {e}\")\n",
        "                finally:\n",
        "                    os.remove(tmp_path)\n",
        "    elif name.lower().endswith(\".csv\"):\n",
        "        if len(data) <= 1024:\n",
        "            continue\n",
        "        with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".csv\", delete=False) as tmp:\n",
        "            tmp.write(data); tmp_path = tmp.name\n",
        "        try:\n",
        "            session_type = extract_session_type(tmp_path)\n",
        "            df = fed3.load(tmp_path)\n",
        "            df.name = os.path.basename(name)\n",
        "            df.attrs = {\"Session_type\": session_type}\n",
        "            feds.append(df)\n",
        "            loaded_files.append(os.path.basename(name))\n",
        "            session_types.append(session_type)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {name}: {e}\")\n",
        "        finally:\n",
        "            os.remove(tmp_path)\n",
        "\n",
        "print(f\"Loaded {len(loaded_files)} files. Session types captured for all.\")\n",
        "# Optional quick plot\n",
        "if feds:\n",
        "    try:\n",
        "        fed3.as_aligned(feds, alignment=\"datetime\", inplace=True)\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        fplot.line(feds, y='pellets'); plt.legend().remove(); plt.tight_layout(); plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Plotting skipped: {e}\")"
      ],
      "metadata": {
        "id": "_3hqwt-oejwc",
        "cellView": "form"
      },
      "id": "_3hqwt-oejwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Build Key\n",
        "\n",
        "\n",
        "import os, glob, io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ipydatagrid import DataGrid, TextRenderer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from google.colab import files as colab_files\n",
        "from google.colab import output as colab_output\n",
        "\n",
        "# Require that the file-upload cell has already populated these:\n",
        "assert 'loaded_files' in globals() and 'session_types' in globals(), \\\n",
        "    \"Run the 'Upload FED3 files' cell first.\"\n",
        "\n",
        "colab_output.enable_custom_widget_manager()\n",
        "\n",
        "# ---------------------------\n",
        "# Base: bare-bones Key_Df from loaded data\n",
        "# ---------------------------\n",
        "def _make_base_key_df():\n",
        "    return pd.DataFrame({\"filename\": loaded_files, \"Session_type\": session_types})\n",
        "\n",
        "def _file_base(s):\n",
        "    return os.path.splitext(os.path.basename(str(s)))[0].strip()\n",
        "\n",
        "def _norm_base_lower(s):\n",
        "    return _file_base(s).lower()\n",
        "\n",
        "# ---------------------------\n",
        "# Key scanner: detect Mouse_ID or filename columns\n",
        "# ---------------------------\n",
        "def _scan_key_columns(df):\n",
        "    \"\"\"\n",
        "    Returns dict:\n",
        "      {\n",
        "        'has_mouse': bool,\n",
        "        'has_filename': bool,\n",
        "        'filename_col': 'filename'|'File'|None,\n",
        "        'msg': str\n",
        "      }\n",
        "    Accepts keys that have either Mouse_ID or a filename column (filename/File).\n",
        "    \"\"\"\n",
        "    info = {'has_mouse': False, 'has_filename': False, 'filename_col': None, 'msg': ''}\n",
        "    try:\n",
        "        cols = [str(c).strip() for c in df.columns]\n",
        "        has_mouse = 'Mouse_ID' in cols\n",
        "        fname_col = 'filename' if 'filename' in cols else ('File' if 'File' in cols else None)\n",
        "        info.update({\n",
        "            'has_mouse': has_mouse,\n",
        "            'has_filename': fname_col is not None,\n",
        "            'filename_col': fname_col\n",
        "        })\n",
        "        if has_mouse:\n",
        "            info['msg'] = \"'Mouse_ID' found.\"\n",
        "        elif fname_col:\n",
        "            info['msg'] = f\"'{fname_col}' found; will match on filename.\"\n",
        "        else:\n",
        "            info['msg'] = \"Neither 'Mouse_ID' nor 'filename'/'File' found in provided key.\"\n",
        "    except Exception as e:\n",
        "        info['msg'] = f\"Error while checking key: {e}\"\n",
        "    return info\n",
        "\n",
        "# ---------------------------\n",
        "# Read uploaded key (CSV/XLSX), accept Mouse_ID or filename\n",
        "# ---------------------------\n",
        "def _read_key_from_upload(name, content_bytes):\n",
        "    \"\"\"Return (df_or_None, message). Reads CSV/XLSX bytes from Colab upload.\"\"\"\n",
        "    ext = name.lower().rsplit('.', 1)[-1] if '.' in name else ''\n",
        "    try:\n",
        "        bio = io.BytesIO(content_bytes)\n",
        "        if ext == 'xlsx':\n",
        "            xls = pd.ExcelFile(bio, engine='openpyxl')\n",
        "            frames = [pd.read_excel(xls, sheet_name=s) for s in xls.sheet_names]\n",
        "            key_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        elif ext == 'csv':\n",
        "            key_df = pd.read_csv(bio, sep=None, engine='python')\n",
        "        else:\n",
        "            return None, f\"Unsupported key type .{ext}\"\n",
        "\n",
        "        key_df = key_df.copy()\n",
        "        key_df.columns = [str(c).strip() for c in key_df.columns]\n",
        "        scan = _scan_key_columns(key_df)\n",
        "        if not (scan['has_mouse'] or scan['has_filename']):\n",
        "            return None, scan['msg']\n",
        "\n",
        "        # Normalize types/columns we might use later\n",
        "        if scan['has_mouse']:\n",
        "            globals()['KEY_MATCH_MODE'] = 'mouse_id'\n",
        "            key_df['Mouse_ID'] = key_df['Mouse_ID'].astype(str).str.strip()\n",
        "\n",
        "        if scan['has_filename']:\n",
        "            globals()['KEY_MATCH_MODE'] = 'filename'\n",
        "            fcol = scan['filename_col']\n",
        "            key_df[fcol] = key_df[fcol].astype(str).str.strip()\n",
        "            key_df['_key_file_base_lower'] = key_df[fcol].map(_norm_base_lower)\n",
        "        else:\n",
        "            globals()['KEY_MATCH_MODE'] = None\n",
        "\n",
        "        # Persist a deterministic copy on disk for reproducibility\n",
        "        fixed_path = f\"_uploaded_key.{ext}\"\n",
        "        with open(fixed_path, \"wb\") as f:\n",
        "            f.write(content_bytes)\n",
        "        globals()['uploaded_key_path'] = fixed_path\n",
        "\n",
        "        return key_df, f\"Key loaded from upload ({name}) and saved to {fixed_path}. {scan['msg']}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error reading uploaded key: {e}\"\n",
        "\n",
        "# ---------------------------\n",
        "# Matching filename <-> Mouse_ID\n",
        "# ---------------------------\n",
        "def _match_mouse_id_to_filenames(filenames, key_df):\n",
        "    \"\"\"Return DataFrame: filename, Mouse_ID, match_status based on Mouse_ID substring in filename.\"\"\"\n",
        "    base_names_lower = [_norm_base_lower(f) for f in filenames]\n",
        "    mouse_ids = (\n",
        "        key_df['Mouse_ID']\n",
        "        .dropna().astype(str).map(str.strip)\n",
        "        .replace({'': np.nan}).dropna().unique().tolist()\n",
        "    )\n",
        "    rows = []\n",
        "    for fname, base in zip(filenames, base_names_lower):\n",
        "        hits = [mid for mid in mouse_ids if str(mid).lower() in base]\n",
        "        if len(hits) == 1:\n",
        "            rows.append({\"filename\": fname, \"Mouse_ID\": hits[0], \"match_status\": \"Matched (Mouse_ID in filename)\"})\n",
        "        elif len(hits) > 1:\n",
        "            longest = max(len(str(h)) for h in hits)\n",
        "            best = [h for h in hits if len(str(h)) == longest]\n",
        "            if len(best) == 1:\n",
        "                rows.append({\"filename\": fname, \"Mouse_ID\": best[0], \"match_status\": \"Matched (longest Mouse_ID token)\"})\n",
        "            else:\n",
        "                rows.append({\"filename\": fname, \"Mouse_ID\": None, \"match_status\": f\"Ambiguous Mouse_ID: {hits}\"})\n",
        "        else:\n",
        "            rows.append({\"filename\": fname, \"Mouse_ID\": None, \"match_status\": \"Mouse_ID not found in filename\"})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------------------------\n",
        "# Build/Rematch function\n",
        "# ---------------------------\n",
        "status_box = widgets.Output()\n",
        "Key_Df = _make_base_key_df()  # start bare-bones\n",
        "\n",
        "def _collapse_key_suffixes(df, keep_filename_from_base=True):\n",
        "    \"\"\"\n",
        "    Remove duplicate columns created by merge suffixes.\n",
        "    - If both 'col' and 'col_key' exist, keep the key version (rename col_key -> col, drop base col).\n",
        "    - Special-case: if keep_filename_from_base and base == 'filename',\n",
        "      drop 'filename_key' and keep the base 'filename'.\n",
        "    \"\"\"\n",
        "    cols = list(df.columns)\n",
        "    rename_map = {}\n",
        "    drop_cols = []\n",
        "\n",
        "    for col in cols:\n",
        "        if col.endswith('_key'):\n",
        "            base = col[:-4]\n",
        "            if keep_filename_from_base and base == 'filename':\n",
        "                # keep the 'filename' from the bare-bones (actual loaded_files)\n",
        "                drop_cols.append(col)\n",
        "            elif base in df.columns:\n",
        "                # key version wins: drop base col, rename *_key -> base\n",
        "                drop_cols.append(base)\n",
        "                rename_map[col] = base\n",
        "            else:\n",
        "                # no base column; just strip \"_key\"\n",
        "                rename_map[col] = base\n",
        "\n",
        "    df = df.drop(columns=drop_cols, errors='ignore').rename(columns=rename_map)\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_or_rematch_key_df(key_df=None, msg_hint=\"\"):\n",
        "    \"\"\"\n",
        "    If key_df provided and valid:\n",
        "      - If a filename column exists (filename/File), always use filename-based merge.\n",
        "        - If Mouse_ID is also present, it is preserved from the key file.\n",
        "      - Else, if only Mouse_ID exists, use substring-based Mouse_ID matching.\n",
        "    Else: keep bare-bones.\n",
        "\n",
        "    When a key is used, any duplicate column names between the bare-bones\n",
        "    and the key are resolved so that the key's values win (except 'filename').\n",
        "    \"\"\"\n",
        "    global Key_Df\n",
        "    files_df = _make_base_key_df().copy()\n",
        "    files_df['_file_base_lower'] = files_df['filename'].map(_norm_base_lower)\n",
        "\n",
        "    if key_df is None:\n",
        "        # Pure bare-bones Key_Df\n",
        "        Key_Df = files_df.drop(columns=['_file_base_lower']).copy()\n",
        "        Key_Df[\"match_status\"] = \"No key\"\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Key status: No key provided; showing bare-bones Key_Df.\")\n",
        "        return\n",
        "\n",
        "    # Identify key capabilities\n",
        "    scan = _scan_key_columns(key_df)\n",
        "    kd = key_df.copy()\n",
        "\n",
        "    # Helper: make a unique version of the key for whichever join we use\n",
        "    def _dedup(df, subset_cols):\n",
        "        dup_counts = df[subset_cols].astype(str).agg('|'.join, axis=1).value_counts()\n",
        "        n_dups = int((dup_counts > 1).sum())\n",
        "        if n_dups:\n",
        "            with status_box:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Note: {n_dups} duplicate key(s) on {subset_cols}; taking the first occurrence.\")\n",
        "        return df.drop_duplicates(subset=subset_cols, keep=\"first\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # 1) Prefer filename-based route whenever filename column exists\n",
        "    #    (whether or not Mouse_ID is present)\n",
        "    # ---------------------------\n",
        "    if scan['has_filename']:\n",
        "        fcol = scan['filename_col']\n",
        "\n",
        "        # Normalize filename column in key\n",
        "        kd[fcol] = kd[fcol].astype(str).str.strip()\n",
        "        kd['_key_file_base_lower'] = kd[fcol].map(_norm_base_lower)\n",
        "\n",
        "        # One row per normalized filename in the key\n",
        "        key_unique = _dedup(kd, ['_key_file_base_lower'])\n",
        "\n",
        "        # Merge by normalized basename (case-insensitive, extension-stripped)\n",
        "        Key_Df = (\n",
        "            files_df\n",
        "            .merge(\n",
        "                key_unique,\n",
        "                left_on=\"_file_base_lower\",\n",
        "                right_on=\"_key_file_base_lower\",\n",
        "                how=\"left\",\n",
        "                suffixes=(\"\", \"_key\")\n",
        "            )\n",
        "            .drop(columns=['_file_base_lower', '_key_file_base_lower'])\n",
        "        )\n",
        "\n",
        "        # Clean up duplicate columns so key overwrites bare-bones where appropriate\n",
        "        Key_Df = _collapse_key_suffixes(Key_Df, keep_filename_from_base=True)\n",
        "\n",
        "        # match_status for filename-based matching\n",
        "        Key_Df[\"match_status\"] = np.where(\n",
        "            Key_Df[fcol].notna(),\n",
        "            \"Matched (filename)\",\n",
        "            \"Filename not found in key\"\n",
        "        )\n",
        "\n",
        "    # ---------------------------\n",
        "    # 2) If no filename column but Mouse_ID exists, use substring route\n",
        "    # ---------------------------\n",
        "    elif scan['has_mouse']:\n",
        "        # Mouse_ID route (fallback when no filename column exists)\n",
        "        kd['Mouse_ID'] = kd['Mouse_ID'].astype(str).str.strip()\n",
        "        key_unique = _dedup(kd, ['Mouse_ID'])\n",
        "\n",
        "        matched = _match_mouse_id_to_filenames(\n",
        "            files_df['filename'].tolist(),\n",
        "            key_unique\n",
        "        )[[\"filename\", \"Mouse_ID\", \"match_status\"]]\n",
        "\n",
        "        Key_Df = (\n",
        "            files_df\n",
        "            .merge(matched, on=\"filename\", how=\"left\")\n",
        "            .merge(key_unique, on=\"Mouse_ID\", how=\"left\", suffixes=(\"\", \"_key\"))\n",
        "            .drop(columns=['_file_base_lower'])\n",
        "        )\n",
        "\n",
        "        # Clean up duplicate columns (e.g., Mouse_ID vs Mouse_ID_key)\n",
        "        Key_Df = _collapse_key_suffixes(Key_Df, keep_filename_from_base=True)\n",
        "\n",
        "    # ---------------------------\n",
        "    # 3) Neither filename nor Mouse_ID in key\n",
        "    # ---------------------------\n",
        "    else:\n",
        "        # Neither route available (shouldn't happen due to earlier check)\n",
        "        Key_Df = files_df.drop(columns=['_file_base_lower']).copy()\n",
        "        Key_Df[\"match_status\"] = \"Key missing Mouse_ID and filename columns\"\n",
        "\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        if msg_hint:\n",
        "            print(msg_hint)\n",
        "        print(f\"Merged key columns into Key_Df ({len(Key_Df)} rows, {len(Key_Df.columns)} cols).\")\n",
        "\n",
        "# ---------------------------\n",
        "# Grid UI\n",
        "# ---------------------------\n",
        "def make_grid(df: pd.DataFrame):\n",
        "    g = DataGrid(\n",
        "        df,\n",
        "        editable=True,\n",
        "        selection_mode='cell',\n",
        "        layout={'height': '420px'},\n",
        "        base_row_size=28,\n",
        "        base_column_size=120,\n",
        "    )\n",
        "    g.default_renderer = TextRenderer(text_wrap=True)\n",
        "    return g\n",
        "\n",
        "def rebuild_grid(msg=\"\"):\n",
        "    global grid, ui\n",
        "    df = Key_Df.copy().reset_index(drop=True)\n",
        "    new_grid = make_grid(df)\n",
        "    ui.children = (upload_row, new_grid, controls, status_box)\n",
        "    grid = new_grid\n",
        "    with status_box:\n",
        "        if msg:\n",
        "            print(msg)\n",
        "        print(f\"Grid now shows Key_Df ({len(df)} rows, {len(df.columns)} cols)\")\n",
        "\n",
        "# ---------------------------\n",
        "# Colab-native upload button ONLY (no path UI)\n",
        "# ---------------------------\n",
        "upload_btn = widgets.Button(description=\"Upload\", button_style=\"primary\", layout=widgets.Layout(width=\"120px\"))\n",
        "reset_btn = widgets.Button(description=\"Reset Key\", button_style=\"warning\", layout=widgets.Layout(width=\"120px\"))\n",
        "download_button = widgets.Button(description='Download', button_style='success', layout=widgets.Layout(width=\"120px\"))\n",
        "\n",
        "def on_colab_upload(_):\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Opening Colab upload dialog...\")\n",
        "    uploaded = colab_files.upload()  # opens the native Colab picker\n",
        "    if not uploaded:\n",
        "        with status_box:\n",
        "            print(\"No file selected.\")\n",
        "        return\n",
        "    name, content = next(iter(uploaded.items()))\n",
        "    key_df, msg = _read_key_from_upload(name, content)\n",
        "    if key_df is None:\n",
        "        build_or_rematch_key_df(None)\n",
        "        rebuild_grid(f\"Key status: {msg}\")\n",
        "    else:\n",
        "        build_or_rematch_key_df(key_df, msg_hint=f\"Key status: {msg}\")\n",
        "        rebuild_grid()\n",
        "\n",
        "def _load_saved_key_from_disk():\n",
        "    \"\"\"Returns (key_df_or_None, message) from uploaded_key_path if present/valid.\"\"\"\n",
        "    key_path = globals().get('uploaded_key_path', None)\n",
        "    if not (key_path and os.path.exists(key_path)):\n",
        "        return None, \"No saved key on disk to reload.\"\n",
        "    try:\n",
        "        ext = key_path.lower().rsplit('.', 1)[-1] if '.' in key_path else ''\n",
        "        if ext == 'xlsx':\n",
        "            xls = pd.ExcelFile(key_path, engine='openpyxl')\n",
        "            frames = [pd.read_excel(xls, sheet_name=s) for s in xls.sheet_names]\n",
        "            key_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        elif ext == 'csv':\n",
        "            key_df = pd.read_csv(key_path, sep=None, engine='python')\n",
        "        else:\n",
        "            return None, f\"Unsupported key type .{ext}\"\n",
        "\n",
        "        key_df = key_df.copy()\n",
        "        key_df.columns = [str(c).strip() for c in key_df.columns]\n",
        "        scan = _scan_key_columns(key_df)\n",
        "        if not (scan['has_mouse'] or scan['has_filename']):\n",
        "            return None, scan['msg']\n",
        "\n",
        "        if scan['has_mouse']:\n",
        "            key_df['Mouse_ID'] = key_df['Mouse_ID'].astype(str).str.strip()\n",
        "        if scan['has_filename']:\n",
        "            fcol = scan['filename_col']\n",
        "            key_df[fcol] = key_df[fcol].astype(str).str.strip()\n",
        "            key_df['_key_file_base_lower'] = key_df[fcol].map(_norm_base_lower)\n",
        "\n",
        "        return key_df, f\"Key reloaded from {os.path.basename(key_path)}. {scan['msg']}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error reading saved uploaded key: {e}\"\n",
        "\n",
        "def on_reset(_):\n",
        "    \"\"\"\n",
        "    HARD RESET: forget any saved key and show bare-bones Key_Df.\n",
        "    Deletes _uploaded_key.(xlsx|csv) if present and clears uploaded_key_path.\n",
        "    \"\"\"\n",
        "    # 1) Forget path in memory\n",
        "    globals().pop('uploaded_key_path', None)\n",
        "\n",
        "    # 2) Remove any persisted key files from disk\n",
        "    import os\n",
        "    for ext in (\"xlsx\", \"csv\"):\n",
        "        try:\n",
        "            os.remove(f\"_uploaded_key.{ext}\")\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    # 3) Show bare-bones\n",
        "    build_or_rematch_key_df(None)\n",
        "    rebuild_grid(\"Reset: cleared saved key; showing bare-bones Key_Df.\")\n",
        "\n",
        "def download_df(_):\n",
        "    global Key_Df\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Saving latest Key_Df as XLSX ...\")\n",
        "    try:\n",
        "        path = \"/content/Key_Df.xlsx\"\n",
        "        Key_Df.to_excel(path, index=False, engine='openpyxl')\n",
        "        colab_files.download(path)\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Saved and downloading Key_Df.xlsx ...\")\n",
        "    except Exception as e:\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Error while saving/downloading: {e}\")\n",
        "\n",
        "upload_btn.on_click(on_colab_upload)\n",
        "reset_btn.on_click(on_reset)\n",
        "download_button.on_click(download_df)\n",
        "\n",
        "upload_row = widgets.HBox([\n",
        "    widgets.HTML(\"<b>Optional key:</b>\"),\n",
        "    upload_btn,\n",
        "    reset_btn,\n",
        "    download_button\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# Edit / rematch / download controls\n",
        "# ---------------------------\n",
        "new_col_name    = widgets.Text(placeholder='Enter new column name', description='New Col:')\n",
        "add_col_button  = widgets.Button(description='Add Column', button_style='info')\n",
        "apply_button    = widgets.Button(description='Apply Changes', button_style='primary', layout=widgets.Layout(width=\"120px\"))\n",
        "\n",
        "def add_column(_):\n",
        "    global Key_Df\n",
        "    col = new_col_name.value.strip()\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        if not col:\n",
        "            print(\"Please enter a column name.\"); return\n",
        "        if col in Key_Df.columns:\n",
        "            print(f\"Column '{col}' already exists.\"); return\n",
        "        Key_Df[col] = \"\"\n",
        "        print(f\"Added column '{col}' to Key_Df.\")\n",
        "    rebuild_grid()\n",
        "\n",
        "def apply_edits(_):\n",
        "    global Key_Df\n",
        "    try:\n",
        "        Key_Df = grid.data.copy().reset_index(drop=True)\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Applied grid edits to Key_Df ({len(Key_Df)} rows, {len(Key_Df.columns)} cols).\")\n",
        "    except Exception as e:\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Error applying edits: {e}\")\n",
        "\n",
        "add_col_button.on_click(add_column)\n",
        "apply_button.on_click(apply_edits)\n",
        "\n",
        "controls = widgets.HBox([new_col_name, add_col_button, apply_button])\n",
        "\n",
        "# ---------------------------\n",
        "# Initialize UI\n",
        "# ---------------------------\n",
        "Key_Df = _make_base_key_df()\n",
        "Key_Df[\"match_status\"] = \"No key\"\n",
        "\n",
        "grid = make_grid(Key_Df.copy().reset_index(drop=True))\n",
        "ui = widgets.VBox([upload_row, grid, controls, status_box])\n",
        "display(ui)\n"
      ],
      "metadata": {
        "id": "ZwexsQq-8sh-",
        "cellView": "form"
      },
      "id": "ZwexsQq-8sh-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot individual files\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# optional Colab downloader\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "except Exception:\n",
        "    colab_files = None\n",
        "\n",
        "files_list = feds\n",
        "assert isinstance(files_list, list) and len(files_list) > 0, \"No FED3 files loaded.\"\n",
        "assert 'Key_Df' in globals() and isinstance(Key_Df, pd.DataFrame), \"Build/rematch Key_Df first.\"\n",
        "\n",
        "# metadata_df = copy of Key_Df\n",
        "metadata_df = Key_Df.copy().reset_index(drop=True)\n",
        "if 'filename' in metadata_df.columns:\n",
        "    metadata_df['filename'] = metadata_df['filename'].astype(str).map(os.path.basename)\n",
        "\n",
        "def _coerce_numeric_col(df, col, clip_upper=None, na_map=None):\n",
        "    if col not in df.columns:\n",
        "        return\n",
        "    s = df[col]\n",
        "    if na_map:\n",
        "        s = s.replace(na_map)\n",
        "    s = pd.to_numeric(s, errors='coerce')\n",
        "    if clip_upper is not None:\n",
        "        s.loc[s > clip_upper] = np.nan\n",
        "    df[col] = s\n",
        "\n",
        "def _plot_file_core(file_index):\n",
        "    df = files_list[file_index].copy()\n",
        "    full_name = getattr(df, 'name', f\"File_{file_index}\")\n",
        "    file_basename = os.path.basename(str(full_name))\n",
        "\n",
        "    # Preserve original index once\n",
        "    if \"Original_Timestamp\" not in df.columns:\n",
        "        df[\"Original_Timestamp\"] = df.index\n",
        "\n",
        "    # Attach metadata by filename (matching already done upstream)\n",
        "    meta_row = None\n",
        "    if 'filename' in metadata_df.columns:\n",
        "        mr = metadata_df.loc[metadata_df['filename'] == file_basename]\n",
        "        if not mr.empty:\n",
        "            meta_row = mr.iloc[0]\n",
        "    if meta_row is not None:\n",
        "        for col in meta_row.index:\n",
        "            if col == 'filename':\n",
        "                continue\n",
        "            if col not in df.columns:\n",
        "                df[col] = meta_row[col]\n",
        "            else:\n",
        "                if pd.isna(df[col]).all() and pd.notna(meta_row[col]):\n",
        "                    df[col] = meta_row[col]\n",
        "\n",
        "    # Time + cleanup\n",
        "    try:\n",
        "        df['timestamp'] = pd.to_datetime(df.index)\n",
        "    except Exception:\n",
        "        df['timestamp'] = np.arange(len(df))\n",
        "\n",
        "    _coerce_numeric_col(df, 'Poke_Time', clip_upper=2)\n",
        "    _coerce_numeric_col(df, 'Retrieval_Time', na_map={\"Timed_out\": np.nan})\n",
        "\n",
        "    if len(df) == 0:\n",
        "        print(f\"[!] Empty dataframe for {file_basename}. Skipping plot.\")\n",
        "        return None, None\n",
        "\n",
        "    # Behavioral traces (needs fed3bandit as f3b)\n",
        "    true_left = f3b.true_probs(df, offset=5)[0]\n",
        "    mouse_left = f3b.binned_paction(df, window=10)\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 3))\n",
        "    ax.plot(np.arange(len(true_left)), true_left, color=\"black\", linewidth=2, alpha=0.5)\n",
        "\n",
        "    color = \"dodgerblue\"\n",
        "    if 'Sex' in df.columns and pd.notna(df['Sex']).any():\n",
        "        try:\n",
        "            color = \"red\" if str(df['Sex'].iloc[0]).strip().lower().startswith(\"f\") else \"dodgerblue\"\n",
        "        except Exception:\n",
        "            pass\n",
        "    ax.plot(np.arange(len(mouse_left)), mouse_left, color=color, linewidth=3, alpha=0.7)\n",
        "\n",
        "    ax.set_ylabel(\"P(Left)\")\n",
        "    ax.set_xlabel(\"Trial\")\n",
        "    ax.set_yticks([1, 0.5, 0])\n",
        "\n",
        "    # Title = Mouse_ID (fallback to filename)\n",
        "    title_text = str(meta_row['Mouse_ID']) if (meta_row is not None and 'Mouse_ID' in meta_row and pd.notna(meta_row['Mouse_ID'])) else file_basename\n",
        "    ax.set_title(title_text)\n",
        "\n",
        "    sns.despine()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # suggest a base filename for saving\n",
        "    safe_title = \"\".join(c if c.isalnum() or c in (\"-\", \"_\") else \"_\" for c in title_text)\n",
        "    suggested = f\"{safe_title}_pLeft\"\n",
        "    return fig, suggested\n",
        "\n",
        "# ----- UI -----\n",
        "N = len(files_list)\n",
        "idx_slider = widgets.IntSlider(min=0, max=max(0, N-1), step=1, value=0, description='File', continuous_update=True)\n",
        "status_lbl = widgets.HTML()\n",
        "out = widgets.Output()\n",
        "\n",
        "# save controls\n",
        "fmt_dd   = widgets.Dropdown(options=[\"pdf\", \"png\", \"svg\"], value=\"pdf\", description=\"Format:\", layout=widgets.Layout(width=\"180px\"))\n",
        "save_btn = widgets.Button(description=\"Save current plot\", button_style=\"success\", layout=widgets.Layout(width=\"200px\"))\n",
        "\n",
        "_last_fig = None\n",
        "_last_name = \"plot\"\n",
        "\n",
        "def _status(idx):\n",
        "    name = getattr(files_list[idx], 'name', f\"File_{idx}\")\n",
        "    return f\"Index: <b>{idx}</b> &nbsp;|&nbsp; File: <code>{os.path.basename(str(name))}</code>\"\n",
        "\n",
        "def _render(*_):\n",
        "    global _last_fig, _last_name\n",
        "    idx = int(idx_slider.value)\n",
        "    status_lbl.value = _status(idx)\n",
        "    out.clear_output()\n",
        "    with out:\n",
        "        fig, suggested = _plot_file_core(idx)\n",
        "        _last_fig = fig\n",
        "        _last_name = suggested or \"plot\"\n",
        "\n",
        "def _save_current(_):\n",
        "    if _last_fig is None:\n",
        "        with out:\n",
        "            print(\"No plot to save yet.\");\n",
        "        return\n",
        "    fmt = fmt_dd.value\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    outdir = \"figures\"\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    path = os.path.join(outdir, f\"{_last_name}_{ts}.{fmt}\")\n",
        "    _last_fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
        "    with out:\n",
        "        print(f\"Saved: {path}\")\n",
        "    if colab_files is not None:\n",
        "        colab_files.download(path)\n",
        "\n",
        "save_btn.on_click(_save_current)\n",
        "idx_slider.observe(_render, names='value')\n",
        "\n",
        "controls = widgets.HBox([idx_slider, fmt_dd, save_btn], layout=widgets.Layout(gap=\"10px\"))\n",
        "display(widgets.VBox([controls, status_lbl, out]))\n",
        "\n",
        "# Initial draw\n",
        "_render()"
      ],
      "metadata": {
        "id": "qeZfA9tXWngI",
        "cellView": "form"
      },
      "id": "qeZfA9tXWngI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Analyze Bandit metrics\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from IPython.display import FileLink\n",
        "import tqdm\n",
        "\n",
        "assert 'metadata_df' in globals() and isinstance(metadata_df, pd.DataFrame), \\\n",
        "    \"metadata_df not found. Build/rematch Key_Df and set metadata_df = Key_Df.copy() first.\"\n",
        "\n",
        "# ---------- Small helpers ----------\n",
        "def _basename(pathlike) -> str:\n",
        "    s = str(pathlike).replace(\"\\\\\", \"/\")\n",
        "    return s.split(\"/\")[-1]\n",
        "\n",
        "def _file_base_lower(pathlike):\n",
        "    return os.path.splitext(os.path.basename(str(pathlike)))[0].lower()\n",
        "\n",
        "def _get_timestamp_series(df, ts_col=\"MM:DD:YYYY hh:mm:ss\"):\n",
        "    \"\"\"Return a pandas Series of timestamps for df (never an Index).\"\"\"\n",
        "    if ts_col in df.columns:\n",
        "        ts = pd.to_datetime(df[ts_col], format=\"%m:%d:%Y %H:%M:%S\", errors=\"coerce\")\n",
        "        return pd.Series(ts, index=df.index)\n",
        "    for cand in [\"DateTime\", \"Datetime\", \"Timestamp\", \"timestamp\", \"datetime\"]:\n",
        "        if cand in df.columns:\n",
        "            ts = pd.to_datetime(df[cand], errors=\"coerce\")\n",
        "            return pd.Series(ts, index=df.index)\n",
        "    idx = df.index\n",
        "    if isinstance(idx, pd.DatetimeIndex):\n",
        "        return pd.Series(idx, index=df.index)\n",
        "    return pd.to_datetime(pd.Series(idx, index=df.index), errors=\"coerce\")\n",
        "\n",
        "def _split_day_night(df, ts_col=\"MM:DD:YYYY hh:mm:ss\"):\n",
        "    \"\"\"Day: 07:00â€“19:00; Night: otherwise.\"\"\"\n",
        "    ts = _get_timestamp_series(df, ts_col=ts_col)\n",
        "    valid = ts.notna()\n",
        "    hrs = ts.dt.hour\n",
        "    day_mask = valid & (hrs >= 7) & (hrs < 19)\n",
        "    night_mask = valid & ~day_mask\n",
        "    return df.loc[day_mask], df.loc[night_mask]\n",
        "\n",
        "def compute_withinbout_lose_shift(c_df, max_gap_s=120):\n",
        "    try:\n",
        "        if \"Event\" not in c_df.columns or len(c_df) < 2:\n",
        "            return np.nan\n",
        "        events = c_df[\"Event\"].to_numpy()\n",
        "        times = _get_timestamp_series(c_df).to_numpy()\n",
        "        total = shifted = 0\n",
        "        for i in range(len(events) - 1):\n",
        "            curr_evt, next_evt = events[i], events[i + 1]\n",
        "            if curr_evt not in (\"Left\", \"Right\"):\n",
        "                continue\n",
        "            dt_s = (times[i + 1] - times[i]) / np.timedelta64(1, \"s\")\n",
        "            if np.isnan(dt_s) or dt_s > max_gap_s:\n",
        "                continue\n",
        "            if next_evt == \"Pellet\":\n",
        "                continue\n",
        "            if next_evt in (\"Left\", \"Right\"):\n",
        "                total += 1\n",
        "                if next_evt != curr_evt:\n",
        "                    shifted += 1\n",
        "        return (shifted / total) if total > 0 else np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def compute_withinbout_win_stay(c_df, max_gap_s=120):\n",
        "    try:\n",
        "        if \"Event\" not in c_df.columns or len(c_df) < 3:\n",
        "            return np.nan\n",
        "        events = c_df[\"Event\"].to_numpy()\n",
        "        times = _get_timestamp_series(c_df).to_numpy()\n",
        "        pellet_idx = [i for i in range(1, len(events) - 1) if events[i] == \"Pellet\"]\n",
        "        total = same = 0\n",
        "        for i in pellet_idx:\n",
        "            prev_event, next_event = events[i - 1], events[i + 1]\n",
        "            dt_s = (times[i + 1] - times[i]) / np.timedelta64(1, \"s\")\n",
        "            if not np.isnan(dt_s) and dt_s <= max_gap_s:\n",
        "                if prev_event in (\"Left\", \"Right\") and next_event in (\"Left\", \"Right\"):\n",
        "                    total += 1\n",
        "                    if next_event == prev_event:\n",
        "                        same += 1\n",
        "        return (same / total) if total > 0 else np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def compute_peak_accuracy(c_df):\n",
        "    \"\"\"Pre-/around-reversal accuracy using f3b.reversal_peh on the subset.\"\"\"\n",
        "    try:\n",
        "        rev_avg = f3b.reversal_peh(c_df, (-10, 10), True)\n",
        "        if len(rev_avg) == 0:\n",
        "            return np.nan\n",
        "        return float(np.mean(rev_avg[:10])) if len(rev_avg) >= 10 else float(np.mean(rev_avg))\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def estimate_daily_pellets(c_df):\n",
        "    ts = _get_timestamp_series(c_df)\n",
        "    valid_ts = ts.dropna()\n",
        "    if valid_ts.size < 2:\n",
        "        return np.nan\n",
        "    duration_hours = (valid_ts.max() - valid_ts.min()).total_seconds() / 3600.0\n",
        "    if duration_hours <= 0:\n",
        "        return np.nan\n",
        "\n",
        "    pellet_events = np.nan\n",
        "    if \"Pellet_Count\" in c_df.columns and c_df[\"Pellet_Count\"].notna().any():\n",
        "        pc = pd.to_numeric(c_df[\"Pellet_Count\"], errors=\"coerce\")\n",
        "        if pc.notna().any():\n",
        "            diffs = pc.diff().fillna(0).clip(lower=0)\n",
        "            pellet_events = float(diffs.sum())\n",
        "            if pellet_events == 0 and pc.iloc[-1] >= pc.iloc[0]:\n",
        "                pellet_events = float(pc.iloc[-1] - pc.iloc[0])\n",
        "    if (pd.isna(pellet_events)) and (\"Event\" in c_df.columns):\n",
        "        pellet_events = float((c_df[\"Event\"] == \"Pellet\").sum())\n",
        "\n",
        "    if pd.isna(pellet_events):\n",
        "        return np.nan\n",
        "    return (pellet_events / duration_hours) * 24.0\n",
        "\n",
        "# ---------- Sessions ----------\n",
        "def _get_sessions():\n",
        "    if 'feds' in globals() and isinstance(feds, (list, tuple)) and len(feds) > 0:\n",
        "        return list(feds)\n",
        "    raise RuntimeError(\"No FED3 sessions found. Expecting a non-empty 'feds' list.\")\n",
        "\n",
        "_sessions = _get_sessions()\n",
        "all_indices = list(range(len(_sessions)))\n",
        "\n",
        "# ---------- Normalize metadata_df ----------\n",
        "md = metadata_df.copy()\n",
        "md['filename'] = md['filename'].astype(str).map(_basename)\n",
        "if 'Mouse_ID' in md.columns:\n",
        "    md['Mouse_ID'] = md['Mouse_ID'].astype(str).str.strip()\n",
        "else:\n",
        "    md['Mouse_ID'] = np.nan  # ensure column exists\n",
        "\n",
        "# ---------- Compute metrics for ALL files ----------\n",
        "rows = []\n",
        "for idx in tqdm.tqdm(all_indices):\n",
        "    c_df = _sessions[idx]\n",
        "    file_name = _basename(getattr(c_df, \"name\", f\"File_{idx}\"))\n",
        "\n",
        "    try:\n",
        "        pre_acc_all = compute_peak_accuracy(c_df)\n",
        "        clean_retrieval_time = pd.to_numeric(c_df.get(\"Retrieval_Time\", pd.Series(dtype=float)), errors=\"coerce\")\n",
        "        clean_poke_time = pd.to_numeric(c_df.get(\"Poke_Time\", pd.Series(dtype=float)), errors=\"coerce\")\n",
        "        clean_poke_time = clean_poke_time[clean_poke_time > 0]\n",
        "\n",
        "        row = {\n",
        "            \"filename\": file_name,\n",
        "            \"PeakAccuracy\": pre_acc_all,\n",
        "            \"Total_pellets\": f3b.count_pellets(c_df),\n",
        "            \"Total_pokes\": f3b.count_pokes(c_df),\n",
        "            \"PokesPerPellet\": f3b.pokes_per_pellet(c_df),\n",
        "            \"RetrievalTime\": clean_retrieval_time.median() if not clean_retrieval_time.empty else np.nan,\n",
        "            \"PokeTime\": clean_poke_time.median() if not clean_poke_time.empty else np.nan,\n",
        "            \"Win-stay\": compute_withinbout_win_stay(c_df),\n",
        "            \"Lose-shift\": compute_withinbout_lose_shift(c_df),\n",
        "            \"daily pellets\": estimate_daily_pellets(c_df),\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to compute OVERALL metrics for {file_name} (idx {idx}): {e}\")\n",
        "        continue\n",
        "\n",
        "    # Day/Night splits\n",
        "    try:\n",
        "        day_df, night_df = _split_day_night(c_df, ts_col=\"MM:DD:YYYY hh:mm:ss\")\n",
        "        row.update({\n",
        "            \"PeakAccuracy_Day\": compute_peak_accuracy(day_df),\n",
        "            \"PeakAccuracy_Night\": compute_peak_accuracy(night_df),\n",
        "            \"Win-stay_Day\": compute_withinbout_win_stay(day_df),\n",
        "            \"Win-stay_Night\": compute_withinbout_win_stay(night_df),\n",
        "            \"Lose-shift_Day\": compute_withinbout_lose_shift(day_df),\n",
        "            \"Lose-shift_Night\": compute_withinbout_lose_shift(night_df),\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to compute DAY/NIGHT metrics for {file_name} (idx {idx}): {e}\")\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "Banditmetrics = pd.DataFrame(rows)\n",
        "if Banditmetrics.empty:\n",
        "    display(HTML(\"<b style='color:#b00'>No files to analyze.</b>\"))\n",
        "    raise SystemExit\n",
        "\n",
        "# ---------- Attach Mouse_ID (primary) from metadata_df, then merge all metadata ----------\n",
        "# Merging priority:\n",
        "# 1) Map filename -> Mouse_ID from metadata_df\n",
        "mouse_map = md.set_index('filename')['Mouse_ID']\n",
        "Banditmetrics['Mouse_ID'] = Banditmetrics['filename'].map(mouse_map)\n",
        "\n",
        "# 2) Fallback: if still missing Mouse_ID, try substring match on metadata Mouse_IDs\n",
        "if Banditmetrics['Mouse_ID'].isna().any():\n",
        "    known_ids = md['Mouse_ID'].dropna().unique().tolist()\n",
        "    for i, row in Banditmetrics.loc[Banditmetrics['Mouse_ID'].isna()].iterrows():\n",
        "        base = _file_base_lower(row['filename'])\n",
        "        hits = [mid for mid in known_ids if str(mid).lower() in base]\n",
        "        if len(hits) == 1:\n",
        "            Banditmetrics.at[i, 'Mouse_ID'] = hits[0]\n",
        "        elif len(hits) > 1:\n",
        "            longest = max(len(str(h)) for h in hits)\n",
        "            best = [h for h in hits if len(str(h)) == longest]\n",
        "            if len(best) == 1:\n",
        "                Banditmetrics.at[i, 'Mouse_ID'] = best[0]\n",
        "\n",
        "# 3) Merge metadata: prefer Mouse_ID, fallback to filename if no Mouse_ID match\n",
        "md_mouse_unique = md.drop_duplicates(subset=['Mouse_ID'], keep='first')\n",
        "bm = Banditmetrics.merge(md_mouse_unique, on='Mouse_ID', how='left', suffixes=('', '_md'))\n",
        "\n",
        "needs_fallback = bm['Mouse_ID'].isna() | bm.filter(regex='_md$').isna().all(axis=1)\n",
        "if needs_fallback.any():\n",
        "    md_file_unique = md.drop_duplicates(subset=['filename'], keep='first')\n",
        "    bm_fb = Banditmetrics.loc[needs_fallback].merge(\n",
        "        md_file_unique, on='filename', how='left', suffixes=('', '_md')\n",
        "    )\n",
        "    bm = pd.concat([bm.loc[~needs_fallback], bm_fb], ignore_index=True)\n",
        "\n",
        "# ---------- Session-type suffixing ----------\n",
        "metric_cols = [\n",
        "    \"Win-stay\", \"Lose-shift\", \"PeakAccuracy\", \"Total_pellets\", \"Total_pokes\",\n",
        "    \"PokesPerPellet\", \"RetrievalTime\", \"PokeTime\", \"daily pellets\",\n",
        "    \"Win-stay_Day\", \"Win-stay_Night\", \"Lose-shift_Day\", \"Lose-shift_Night\",\n",
        "    \"PeakAccuracy_Day\", \"PeakAccuracy_Night\",\n",
        "]\n",
        "\n",
        "# Prefer metadata_df's Session_type; fallback to df.attrs or \"Unknown\"\n",
        "if 'Session_type' in bm.columns:\n",
        "    session_series = bm['Session_type'].astype(str).str.strip()\n",
        "else:\n",
        "    sess_map = {\n",
        "        _basename(getattr(_sessions[i], \"name\", f\"File_{i}\")):\n",
        "        (_sessions[i].attrs.get(\"Session_type\") or \"Unknown\")\n",
        "        for i in range(len(_sessions))\n",
        "    }\n",
        "    session_series = bm[\"filename\"].map(sess_map).fillna(\"Unknown\").astype(str)\n",
        "\n",
        "session_series = session_series.str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "bm[\"_Session_type_for_csv\"] = session_series\n",
        "\n",
        "def with_session_suffix_for_csv(df, file_col=\"File\", metrics=metric_cols, session_col=\"_Session_type_for_csv\"):\n",
        "    df = df.copy()\n",
        "    for m in metrics:\n",
        "        if m not in df.columns:\n",
        "            continue\n",
        "        for sess in df[session_col].dropna().unique():\n",
        "            mask = df[session_col] == sess\n",
        "            col_name = f\"{m}_{sess}\"\n",
        "            if col_name not in df.columns:\n",
        "                df[col_name] = np.nan\n",
        "            df.loc[mask, col_name] = df.loc[mask, m]\n",
        "        df.drop(columns=[m], inplace=True)\n",
        "    return df.drop(columns=[session_col])\n",
        "\n",
        "Banditmetrics_merged = bm.copy()  # optional to inspect in notebook\n",
        "Banditmetrics_csv = with_session_suffix_for_csv(Banditmetrics_merged)\n",
        "\n",
        "# ---------- Minimalist CSV: ID column + Session-typeâ€“suffixed metrics only ----------\n",
        "\n",
        "def _norm_name(s: str) -> str:\n",
        "    return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n",
        "\n",
        "# Decide whether the primary ID is Mouse_ID or filename\n",
        "match_mode = globals().get('KEY_MATCH_MODE', None)\n",
        "\n",
        "if match_mode == 'filename':\n",
        "    id_col = 'filename'\n",
        "elif match_mode == 'mouse_id':\n",
        "    id_col = 'Mouse_ID'\n",
        "else:\n",
        "    # Fallback if KEY_MATCH_MODE wasn't set for some reason\n",
        "    if 'Mouse_ID' in Banditmetrics_csv.columns and Banditmetrics_csv['Mouse_ID'].notna().any():\n",
        "        id_col = 'Mouse_ID'\n",
        "    elif 'filename' in Banditmetrics_csv.columns:\n",
        "        id_col = 'filename'\n",
        "    else:\n",
        "        raise ValueError(\"Neither 'Mouse_ID' nor 'filename' found in Banditmetrics_csv.\")\n",
        "\n",
        "# Metric columns: any Session_typeâ€“suffixed metric (e.g., Win-stay_FR1, PeakAccuracy_Probe)\n",
        "def _metric_match(col: str) -> bool:\n",
        "    # Keep any column that is a suffixed metric from metric_cols\n",
        "    return any(col.startswith(base + \"_\") for base in metric_cols)\n",
        "\n",
        "metric_keep = [c for c in Banditmetrics_csv.columns if _metric_match(c)]\n",
        "\n",
        "if not metric_keep:\n",
        "    raise RuntimeError(\"No session-suffixed metric columns matched; check 'metric_cols' and your column names.\")\n",
        "\n",
        "# Final order: ID column first, then all metric columns\n",
        "cols_out = [id_col] + [c for c in metric_keep if c != id_col]\n",
        "Banditmetrics_csv = Banditmetrics_csv.loc[:, cols_out]\n",
        "\n",
        "# ---------- Save CSV (new naming: strain_strainnum_task_L3.csv) ----------\n",
        "# Use Gene / Gene_ID / Session_type from merged metadata\n",
        "example = Banditmetrics_merged.iloc[0]\n",
        "\n",
        "# Strain name (Gene or Strain)\n",
        "strain_name = str(\n",
        "    example.get(\"Gene\", example.get(\"Strain\", \"NA\"))\n",
        ").replace(\" \", \"_\")\n",
        "\n",
        "# Strain number (Gene_ID or Strain_ID), zero-padded to 3 digits\n",
        "strain_num_raw = example.get(\"Gene_ID\", example.get(\"Strain_ID\", \"NA\"))\n",
        "try:\n",
        "    strain_num = f\"{int(strain_num_raw):03d}\"\n",
        "except Exception:\n",
        "    strain_num = str(strain_num_raw).zfill(3)\n",
        "\n",
        "# Task (Session_type)\n",
        "task_name = str(example.get(\"Session_type\", \"Unknown\")).replace(\" \", \"_\")\n",
        "\n",
        "fname = f\"{strain_name}_{strain_num}_{task_name}_L3.csv\"\n",
        "\n",
        "Banditmetrics_csv.to_csv(fname, index=False)\n",
        "display(HTML(f\"<b>âœ“ Saved metrics CSV to:</b> <code>{fname}</code>\"))\n",
        "try:\n",
        "    display(FileLink(fname))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------- Download button ----------\n",
        "btn = widgets.Button(\n",
        "    description=f\"Download {os.path.basename(fname)}\",\n",
        "    icon=\"download\",\n",
        "    tooltip=\"Click to download the metrics CSV\",\n",
        "    layout=widgets.Layout(width=\"auto\"),\n",
        ")\n",
        "status = widgets.HTML()\n",
        "\n",
        "def _on_click(b):\n",
        "    clear_output(wait=True)\n",
        "    display(btn, status)\n",
        "    if not os.path.exists(fname):\n",
        "        status.value = f\"<b style='color:#b00'>File not found:</b> {fname}\"\n",
        "        return\n",
        "    try:\n",
        "        from google.colab import files as gfiles\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(fname)}</code>â€¦\"\n",
        "        gfiles.download(fname)\n",
        "    except Exception:\n",
        "        status.value = (\n",
        "            \"Not running in Colab. File saved locally at:<br>\"\n",
        "            f\"<code>{fname}</code><br>\"\n",
        "            \"Use the link above to open it.\"\n",
        "        )\n",
        "\n",
        "display(btn, status)\n",
        "btn.on_click(_on_click)\n"
      ],
      "metadata": {
        "id": "gTvKLp-fXgZk",
        "collapsed": true,
        "cellView": "form"
      },
      "id": "gTvKLp-fXgZk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Group for plotting\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# --- sanity ---\n",
        "if 'metadata_df' not in globals() or metadata_df is None or metadata_df.empty:\n",
        "    raise RuntimeError(\"metadata_df is missing or empty. Build metadata_df (copy of Key_Df) first.\")\n",
        "\n",
        "EXCLUDE_LOWER = {\"match_status\"}   # everything else is allowed\n",
        "\n",
        "def _build_file_column(df):\n",
        "    if \"filename\" in df.columns:\n",
        "        return df[\"filename\"].apply(lambda p: os.path.basename(str(p)))\n",
        "    if \"FED3_from_file\" in df.columns and \"Date_from_file\" in df.columns:\n",
        "        return \"FED\" + df[\"FED3_from_file\"].astype(str) + \"_\" + df[\"Date_from_file\"].astype(str)\n",
        "    if \"FED3_from_file\" in df.columns:\n",
        "        return \"FED\" + df[\"FED3_from_file\"].astype(str)\n",
        "    return df.index.astype(str)\n",
        "\n",
        "def _norm_val(x):\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s.lower() in {\"nan\", \"none\"}:\n",
        "        return \"UNK\"\n",
        "    return s.upper()\n",
        "\n",
        "def _build_group_row(row, ordered_cols):\n",
        "    if not ordered_cols:\n",
        "        return \"ALL\"\n",
        "    return \" | \".join(_norm_val(row[c]) for c in ordered_cols)\n",
        "\n",
        "def build_mapping(ordered_cols):\n",
        "    _meta = metadata_df.copy()\n",
        "    _meta[\"filename\"] = _build_file_column(_meta)\n",
        "    _meta[\"Group\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols), axis=1)\n",
        "    mapping = (\n",
        "        _meta[[\"filename\", \"Group\"]]\n",
        "        .dropna(subset=[\"filename\"])\n",
        "        .drop_duplicates()\n",
        "        .sort_values([\"Group\", \"filename\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    return mapping\n",
        "\n",
        "def _unique_keep_order(seq):\n",
        "    seen = set(); out = []\n",
        "    for x in seq:\n",
        "        if x not in seen:\n",
        "            seen.add(x); out.append(x)\n",
        "    return out\n",
        "\n",
        "# ---------- UI (fixed sizes + grid) ----------\n",
        "PX_W = \"260px\"   # list box width\n",
        "PX_H = \"160px\"   # list box height\n",
        "BTN_W = \"160px\"  # button column width\n",
        "HDR_H = \"28px\"   # header cell height (consistent across all headers)\n",
        "\n",
        "title = widgets.HTML(\"<h3>Select columns to group by for X and Hue, then reorder X to set hierarchy</h3>\")\n",
        "\n",
        "all_cols = sorted((c for c in metadata_df.columns if str(c).lower() not in EXCLUDE_LOWER), key=str.lower)\n",
        "\n",
        "def header(text):\n",
        "    # Normalize header height/margins so they align perfectly in the grid row\n",
        "    return widgets.HTML(\n",
        "        f\"<div style='height:{HDR_H};display:flex;align-items:flex-end;'>\"\n",
        "        f\"<h4 style=\\\"margin:0;\\\">{text}</h4></div>\"\n",
        "    )\n",
        "\n",
        "# Headers (row 1 of grid)\n",
        "available_hdr = header(\"Available\")\n",
        "actions_hdr   = header(\"Actions\")\n",
        "x_hdr         = header(\"X grouping\")\n",
        "hue_hdr       = header(\"Hue grouping\")\n",
        "\n",
        "# Widgets (row 2 of grid)\n",
        "available = widgets.SelectMultiple(\n",
        "    options=all_cols, value=tuple(), rows=14,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "right_x = widgets.Select(\n",
        "    options=[], value=None, rows=8,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "right_hue = widgets.Select(\n",
        "    options=[], value=None, rows=8,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Buttons\n",
        "btn_add_x    = widgets.Button(description=\"Add to X â–¶\", button_style='primary', layout=widgets.Layout(width=BTN_W))\n",
        "btn_add_hue  = widgets.Button(description=\"Add to Hue â–¶\",button_style='primary', layout=widgets.Layout(width=BTN_W))\n",
        "btn_clear    = widgets.Button(description=\"Clear\", button_style='danger', layout=widgets.Layout(width=BTN_W))\n",
        "btn_up       = widgets.Button(description=\"â†‘ Up (X only)\", layout=widgets.Layout(width=BTN_W))\n",
        "btn_down     = widgets.Button(description=\"â†“ Down (X only)\", layout=widgets.Layout(width=BTN_W))\n",
        "btn_build    = widgets.Button(description=\"Build Groups\", button_style='success', layout=widgets.Layout(width=\"160px\"))\n",
        "\n",
        "controls_col = widgets.VBox(\n",
        "    [btn_add_x, btn_add_hue, btn_clear, btn_up, btn_down],\n",
        "    layout=widgets.Layout(\n",
        "        align_items=\"center\",\n",
        "        width=BTN_W, min_width=BTN_W, max_width=BTN_W,\n",
        "        height=PX_H, min_height=PX_H, max_height=PX_H,\n",
        "        flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "btn_build = widgets.Button(description=\"Build Groups\", button_style='success', layout=widgets.Layout(width=\"160px\"))\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- Callbacks ---\n",
        "def on_add_x(_):\n",
        "    sel = list(available.value)\n",
        "    if not sel: return\n",
        "    new_opts = _unique_keep_order(list(right_x.options) + sel)\n",
        "    right_x.value = None\n",
        "    right_x.options = new_opts\n",
        "    right_x.value = new_opts[-1] if new_opts else None\n",
        "\n",
        "def on_add_hue(_):\n",
        "    sel = list(available.value)\n",
        "    if not sel: return\n",
        "    new_opts = _unique_keep_order(list(right_hue.options) + sel)\n",
        "    right_hue.value = None\n",
        "    right_hue.options = new_opts\n",
        "    right_hue.value = new_opts[-1] if new_opts else None\n",
        "\n",
        "def on_clear(_):\n",
        "    right_x.value = None; right_x.options = []\n",
        "    right_hue.value = None; right_hue.options = []\n",
        "\n",
        "def on_up(_):\n",
        "    item = right_x.value\n",
        "    if item is None: return\n",
        "    opts = list(right_x.options)\n",
        "    i = opts.index(item)\n",
        "    if i > 0:\n",
        "        opts[i-1], opts[i] = opts[i], opts[i-1]\n",
        "        right_x.value = None; right_x.options = opts; right_x.value = item\n",
        "\n",
        "def on_down(_):\n",
        "    item = right_x.value\n",
        "    if item is None: return\n",
        "    opts = list(right_x.options)\n",
        "    i = opts.index(item)\n",
        "    if i < len(opts) - 1:\n",
        "        opts[i+1], opts[i] = opts[i], opts[i+1]\n",
        "        right_x.value = None; right_x.options = opts; right_x.value = item\n",
        "\n",
        "def on_build(_):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        ordered_cols_x = list(right_x.options)\n",
        "        ordered_cols_hue = list(right_hue.options)\n",
        "\n",
        "        mapping_x = build_mapping(ordered_cols_x)\n",
        "        mapping_hue = build_mapping(ordered_cols_hue)\n",
        "\n",
        "        _meta = metadata_df.copy()\n",
        "        _meta[\"filename\"] = _build_file_column(_meta)\n",
        "        _meta[\"XGroup\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols_x), axis=1)\n",
        "        _meta[\"HueGroup\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols_hue), axis=1)\n",
        "        mapping_both = (\n",
        "            _meta[[\"filename\", \"XGroup\", \"HueGroup\"]]\n",
        "            .dropna(subset=[\"filename\"])\n",
        "            .drop_duplicates()\n",
        "            .sort_values([\"XGroup\", \"HueGroup\", \"filename\"])\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        globals()['files_to_group_x'] = mapping_x.copy()\n",
        "        globals()['files_to_group_hue'] = mapping_hue.copy()\n",
        "        globals()['files_to_group_both'] = mapping_both.copy()\n",
        "        globals()['selected_group_cols_x'] = ordered_cols_x.copy()\n",
        "        globals()['selected_group_cols_hue'] = ordered_cols_hue.copy()\n",
        "\n",
        "        print(\"X-axis grouping (hierarchy):\", ordered_cols_x if ordered_cols_x else [\"ALL\"])\n",
        "        print(f\"Total unique files (X map): {mapping_x['filename'].nunique()}\")\n",
        "        display(widgets.HTML(\"<b>X-group summary</b>\"))\n",
        "        display((mapping_x.groupby(\"Group\", dropna=False)[\"filename\"]\n",
        "                 .nunique().sort_values(ascending=False)\n",
        "                 .rename(\"UniqueFiles\").to_frame()))\n",
        "\n",
        "        print(\"\\nHue grouping:\", ordered_cols_hue if ordered_cols_hue else [\"ALL\"])\n",
        "        print(f\"Total unique files (Hue map): {mapping_hue['filename'].nunique()}\")\n",
        "        display(widgets.HTML(\"<b>Hue-group summary</b>\"))\n",
        "        display((mapping_hue.groupby(\"Group\", dropna=False)[\"filename\"]\n",
        "                 .nunique().sort_values(ascending=False)\n",
        "                 .rename(\"UniqueFiles\").to_frame()))\n",
        "        print(\"\\nCombined mapping available as `files_to_group_both` (filename, XGroup, HueGroup)\")\n",
        "\n",
        "# Wire up\n",
        "btn_add_x.on_click(on_add_x)\n",
        "btn_add_hue.on_click(on_add_hue)\n",
        "btn_clear.on_click(on_clear)\n",
        "btn_up.on_click(on_up)\n",
        "btn_down.on_click(on_down)\n",
        "btn_build.on_click(on_build)\n",
        "\n",
        "# ----- Grid layout -----\n",
        "grid = widgets.GridBox(\n",
        "    children=[\n",
        "        available_hdr, actions_hdr, x_hdr, hue_hdr,     # row 1: headers\n",
        "        available,     controls_col, right_x, right_hue # row 2: widgets\n",
        "    ],\n",
        "    layout=widgets.Layout(\n",
        "        grid_template_columns=f\"{PX_W} {BTN_W} {PX_W} {PX_W}\",\n",
        "        grid_template_rows=\"auto auto\",\n",
        "        grid_gap=\"6px 16px\",\n",
        "        align_items=\"flex-start\",\n",
        "        justify_items=\"flex-start\",\n",
        "        width=\"100%\"\n",
        "    )\n",
        ")\n",
        "\n",
        "ui = widgets.VBox([title, grid, widgets.HBox([btn_build]), output])\n",
        "display(ui)"
      ],
      "metadata": {
        "id": "1fdlX7wlE1R6",
        "cellView": "form"
      },
      "id": "1fdlX7wlE1R6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot metrics!\n",
        "\n",
        "import os, time, shutil, re, itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Stats\n",
        "import pingouin as pg\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Optional Colab download\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "except Exception:\n",
        "    colab_files = None\n",
        "\n",
        "ALPHA = 0.6  # apply to both bars and dots\n",
        "\n",
        "# -----------------------\n",
        "# 0) Preconditions & source\n",
        "# -----------------------\n",
        "if 'Banditmetrics_merged' in globals() and Banditmetrics_merged is not None and not Banditmetrics_merged.empty:\n",
        "    bm = Banditmetrics_merged.copy()\n",
        "elif 'Banditmetrics' in globals() and Banditmetrics is not None and not Banditmetrics.empty:\n",
        "    bm = Banditmetrics.copy()\n",
        "elif 'Banditmetrics_csv' in globals() and Banditmetrics_csv is not None and not Banditmetrics_csv.empty:\n",
        "    bm = Banditmetrics_csv.copy()\n",
        "else:\n",
        "    raise RuntimeError(\"No metrics table found. Run the metrics cell first.\")\n",
        "\n",
        "if \"filename\" not in bm.columns:\n",
        "    if \"File\" in bm.columns:\n",
        "        bm[\"filename\"] = bm[\"File\"].astype(str)\n",
        "    else:\n",
        "        raise RuntimeError(\"Metrics table must include a 'filename' column.\")\n",
        "\n",
        "# -----------------------\n",
        "# Merge in XGroup/HueGroup from grouping widget\n",
        "# -----------------------\n",
        "def _basename_col(s):\n",
        "    return os.path.basename(str(s))\n",
        "\n",
        "def _src_name(df):\n",
        "    if \"filename\" in df.columns:\n",
        "        return \"filename\"\n",
        "    return None\n",
        "\n",
        "if (\"XGroup\" not in bm.columns) or (\"HueGroup\" not in bm.columns):\n",
        "    if 'files_to_group_both' in globals() and files_to_group_both is not None and not files_to_group_both.empty:\n",
        "        m = files_to_group_both.copy()\n",
        "        m_src = _src_name(m)\n",
        "        if m_src is None:\n",
        "            raise RuntimeError(\"Grouping table must include 'filename' (or legacy 'File').\")\n",
        "        m[\"file_base\"]  = m[m_src].apply(_basename_col)\n",
        "        bm[\"file_base\"] = bm[\"filename\"].apply(_basename_col)\n",
        "        bm = bm.merge(m[[\"file_base\",\"XGroup\",\"HueGroup\"]], on=\"file_base\", how=\"left\").drop(columns=[\"file_base\"])\n",
        "        bm[\"XGroup\"]   = bm[\"XGroup\"].fillna(\"UNASSIGNED\")\n",
        "        bm[\"HueGroup\"] = bm[\"HueGroup\"].fillna(\"UNASSIGNED\")\n",
        "    elif 'files_to_group' in globals() and files_to_group is not None and not files_to_group.empty:\n",
        "        m = files_to_group.copy()\n",
        "        m_src = _src_name(m)\n",
        "        if m_src is None:\n",
        "            raise RuntimeError(\"Grouping table must include 'filename' (or legacy 'File').\")\n",
        "        m[\"file_base\"]  = m[m_src].apply(_basename_col)\n",
        "        bm[\"file_base\"] = bm[\"filename\"].apply(_basename_col)\n",
        "        bm = bm.merge(m[[\"file_base\",\"Group\"]], on=\"file_base\", how=\"left\").drop(columns=[\"file_base\"])\n",
        "        bm[\"Group\"] = m[\"Group\"].fillna(\"UNASSIGNED\")\n",
        "        bm[\"XGroup\"] = bm[\"Group\"]\n",
        "        bm[\"HueGroup\"] = \"ALL\"\n",
        "    else:\n",
        "        raise RuntimeError(\"Missing X/Hue mapping. Run the grouping widget (Build Groups) first.\")\n",
        "\n",
        "# -----------------------\n",
        "# 1) Melt to long format\n",
        "# -----------------------\n",
        "base_metric_names = [\n",
        "    \"Win-stay\",\"Lose-shift\",\"PeakAccuracy\",\n",
        "    \"Total_pellets\",\"Total_pokes\",\"PokesPerPellet\",\n",
        "    \"RetrievalTime\",\"PokeTime\",\"daily pellets\",\n",
        "    \"Win-stay_Day\",\"Win-stay_Night\",\"Lose-shift_Day\",\"Lose-shift_Night\",\n",
        "    \"PeakAccuracy_Day\",\"PeakAccuracy_Night\",\n",
        "]\n",
        "\n",
        "metric_cols = []\n",
        "for c in bm.columns:\n",
        "    if pd.api.types.is_numeric_dtype(bm[c]):\n",
        "        for base in base_metric_names:\n",
        "            if c == base or c.startswith(base + \"_\"):\n",
        "                metric_cols.append(c); break\n",
        "seen = set(); metric_cols = [c for c in metric_cols if not (c in seen or seen.add(c))]\n",
        "if not metric_cols:\n",
        "    raise RuntimeError(\"No numeric metric columns found among expected Bandit metrics.\")\n",
        "\n",
        "candidate_id_vars = [\"Genotype\",\"Sex\",\"Strain\",\"Start_Date\",\"flename\",\"Mouse_ID\",\"Session_type\",\"XGroup\",\"HueGroup\"]\n",
        "id_vars = [c for c in candidate_id_vars if c in bm.columns]\n",
        "for need in [\"XGroup\",\"HueGroup\",\"filename\"]:\n",
        "    if need not in id_vars: id_vars.append(need)\n",
        "\n",
        "long_df = pd.melt(\n",
        "    bm,\n",
        "    id_vars=id_vars,\n",
        "    value_vars=metric_cols,\n",
        "    var_name=\"variable\",\n",
        "    value_name=\"value\"\n",
        ")\n",
        "bm.head()\n",
        "\n",
        "# -----------------------\n",
        "# 2) Ordering helpers\n",
        "# -----------------------\n",
        "\n",
        "# Define hue order priority\n",
        "HUE_PRIORITY = [\"Female\", \"Male\", \"F\", \"M\", \"Day\", \"Night\", \"Light\", \"Dark\", \"ALL\", \"UNASSIGNED\"]\n",
        "\n",
        "def _is_wt_group(g):\n",
        "    u = str(g).strip().upper()\n",
        "    tokens = [t for t in re.split(r'[^A-Z0-9]+', u) if t]\n",
        "    WT_ALIASES = {\"WT\", \"WILDTYPE\", \"CONTROL\", \"CTRL\"}\n",
        "    return any(t in WT_ALIASES for t in tokens)\n",
        "\n",
        "def _hier_sort_key(g):\n",
        "    lv = _x_levels(g)\n",
        "    norm = []\n",
        "    for tok in lv:\n",
        "        is_blank = 1 if _is_unassigned_token(tok) else 0\n",
        "        norm.append((is_blank, str(tok).upper()))\n",
        "    wt_present = any(_is_wt_group(tok) for tok in lv) or _is_wt_group(g)\n",
        "    wt_rank = 0 if wt_present else 1\n",
        "    return (wt_rank,) + tuple(norm) + (str(g).upper(),)\n",
        "\n",
        "def _is_unassigned_token(s):\n",
        "    return (str(s).strip().upper() in {\"\", \"UNASSIGNED\", \"NONE\", \"NA\", \"N/A\"})\n",
        "\n",
        "def _x_levels(xname):\n",
        "    s = str(xname)\n",
        "    parts = [p.strip() for p in s.split(\"|\")]\n",
        "    wanted = globals().get(\"selected_group_cols_x\", None)\n",
        "    if isinstance(wanted, (list, tuple)) and wanted:\n",
        "        if len(parts) < len(wanted):\n",
        "            parts += [\"\"] * (len(wanted) - len(parts))\n",
        "        else:\n",
        "            parts = parts[:len(wanted)]\n",
        "    return parts\n",
        "\n",
        "def _order_x_groups(groups):\n",
        "    return sorted(groups, key=_hier_sort_key)\n",
        "\n",
        "def _choose_ref_group(order):\n",
        "    for g in order:\n",
        "        if _is_wt_group(g):\n",
        "            return g\n",
        "    return order[0] if order else None\n",
        "\n",
        "def _order_hue_groups(hues):\n",
        "    hp = globals().get(\"HUE_PRIORITY\", [\"Female\", \"Male\", \"F\", \"M\", \"ALL\", \"UNASSIGNED\"])\n",
        "    hp_lower = [p.lower() for p in hp]\n",
        "    def _prio(h):\n",
        "        u = str(h).strip()\n",
        "        try:\n",
        "            return (0, hp_lower.index(u.lower()), u.upper())\n",
        "        except ValueError:\n",
        "            return (1, u.upper())\n",
        "    return sorted([h for h in hues if h is not None], key=_prio)\n",
        "\n",
        "# -----------------------\n",
        "# 3) Controls (left column: groups & colors)\n",
        "# -----------------------\n",
        "named_defaults = [\n",
        "    \"dodgerblue\", \"red\", \"green\", \"orange\", \"purple\",\n",
        "    \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
        "\n",
        "# Build the ordered list of XGroup levels\n",
        "all_x_groups = long_df[\"XGroup\"].dropna().unique().tolist()\n",
        "if not all_x_groups:\n",
        "    raise RuntimeError(\"No XGroup values found â€“ check that the grouping step ran correctly.\")\n",
        "\n",
        "ordered_x = _order_x_groups(all_x_groups)\n",
        "\n",
        "x_checks, x_colors = {}, {}\n",
        "group_rows = []\n",
        "for i, g in enumerate(ordered_x):\n",
        "    chk = widgets.Checkbox(value=True, description=g, indent=False, layout=widgets.Layout(width=\"260px\"))\n",
        "    col = widgets.Text(value=named_defaults[i % len(named_defaults)],\n",
        "                       layout=widgets.Layout(width=\"120px\"))\n",
        "    x_checks[g] = chk\n",
        "    x_colors[g] = col\n",
        "    # more compact row\n",
        "    group_rows.append(widgets.HBox([chk, widgets.Label(\"\"), col],\n",
        "                                   layout=widgets.Layout(align_items=\"center\", height=\"28px\")))\n",
        "\n",
        "picker = widgets.VBox(group_rows, layout=widgets.Layout(gap=\"2px\"))\n",
        "\n",
        "btn_all  = widgets.Button(description=\"Select all\", layout=widgets.Layout(width=\"140px\"))\n",
        "btn_none = widgets.Button(description=\"Clear\", layout=widgets.Layout(width=\"140px\"))\n",
        "def _set_all(val):\n",
        "    for c in x_checks.values(): c.value = val\n",
        "btn_all.on_click(lambda _: _set_all(True))\n",
        "btn_none.on_click(lambda _: _set_all(False))\n",
        "\n",
        "# scrollable container for the (possibly long) group list\n",
        "picker_container = widgets.Box([picker],\n",
        "    layout=widgets.Layout(overflow=\"auto\", max_height=\"420px\",\n",
        "                          border=\"1px solid #ddd\", padding=\"6px\", width=\"360px\"))\n",
        "\n",
        "left_col = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Groups & Colors</b>\"),\n",
        "    widgets.HBox([btn_all, btn_none], layout=widgets.Layout(gap=\"8px\")),\n",
        "    picker_container\n",
        "], layout=widgets.Layout(width=\"380px\"))\n",
        "\n",
        "# -----------------------\n",
        "# 4) Comparison controls (right column)\n",
        "# -----------------------\n",
        "mode_radio = widgets.ToggleButtons(\n",
        "    options=[(\"Reference group\", \"ref\"), (\"Select Pairs\", \"pairs\")],\n",
        "    value=\"ref\", description=\"\", style={\"button_width\":\"150px\"},\n",
        "    layout=widgets.Layout(width=\"320px\")\n",
        ")\n",
        "\n",
        "ref_dropdown = widgets.Dropdown(\n",
        "    options=ordered_x, value=_choose_ref_group(ordered_x),\n",
        "    description=\"Reference:\", layout=widgets.Layout(width=\"320px\")\n",
        ")\n",
        "\n",
        "def _pair_label(a,b): return f\"{a} âŸ· {b}\"\n",
        "def _pair_value(a,b): return (a,b) if a <= b else (b,a)\n",
        "\n",
        "pairs_select = widgets.SelectMultiple(\n",
        "    options=[], value=[], description=\"Pairs\",\n",
        "    layout=widgets.Layout(width=\"360px\", height=\"320px\")\n",
        ")\n",
        "\n",
        "def _selected_x():\n",
        "    return _order_x_groups([g for g, cb in x_checks.items() if cb.value])\n",
        "\n",
        "def _pair_sort_key(a, b):\n",
        "    \"\"\"Rank pairs by level of first difference: later differences (within-group) rank first.\"\"\"\n",
        "    A = _x_levels(a); B = _x_levels(b)\n",
        "    L = max(len(A), len(B))\n",
        "    if len(A) < L: A += [\"\"] * (L - len(A))\n",
        "    if len(B) < L: B += [\"\"] * (L - len(B))\n",
        "    first_diff = next((i for i, (x, y) in enumerate(zip(A, B)) if x != y), L)\n",
        "    prefix = tuple(A[:first_diff])\n",
        "    return (-first_diff, prefix, tuple(A), tuple(B))\n",
        "\n",
        "def _update_ref_and_pairs(*_):\n",
        "    sel = _selected_x()\n",
        "    ref_dropdown.options = sel or [\"â€”\"]\n",
        "    if sel:\n",
        "        if ref_dropdown.value not in sel:\n",
        "            ref_dropdown.value = _choose_ref_group(sel)\n",
        "    else:\n",
        "        ref_dropdown.value = None\n",
        "\n",
        "    opts = []\n",
        "    for a, b in itertools.combinations(sel, 2):\n",
        "        lbl = _pair_label(a, b)\n",
        "        val = _pair_value(a, b)\n",
        "        opts.append((lbl, val))\n",
        "    opts.sort(key=lambda kv: _pair_sort_key(*kv[1]))\n",
        "    pairs_select.options = opts\n",
        "\n",
        "for cb in x_checks.values():\n",
        "    cb.observe(_update_ref_and_pairs, names=\"value\")\n",
        "_update_ref_and_pairs()\n",
        "\n",
        "# action buttons\n",
        "plot_btn = widgets.Button(description=\"Plot\", button_style=\"primary\",\n",
        "                          layout=widgets.Layout(width=\"160px\"))\n",
        "save_btn = widgets.Button(description=\"Save Plots\", button_style=\"success\",\n",
        "                          layout=widgets.Layout(width=\"160px\"))\n",
        "\n",
        "# Right column layout\n",
        "right_col = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Statistical comparisons</b>\"),\n",
        "    mode_radio,\n",
        "    ref_dropdown,\n",
        "    pairs_select,\n",
        "    widgets.HBox([plot_btn, save_btn], layout=widgets.Layout(gap=\"8px\"))\n",
        "], layout=widgets.Layout(width=\"360px\"))\n",
        "\n",
        "# -----------------------\n",
        "# 5) Labels for stats/legend\n",
        "# -----------------------\n",
        "def _grouping_label(which=\"X\"):\n",
        "    if which.lower().startswith(\"x\"):\n",
        "        cols = globals().get(\"selected_group_cols_x\", [])\n",
        "        default = \"XGroup\"\n",
        "    else:\n",
        "        cols = globals().get(\"selected_group_cols_hue\", [])\n",
        "        default = \"HueGroup\"\n",
        "    cols = [str(c).strip() for c in (cols or []) if str(c).strip()]\n",
        "    return \" | \".join(cols) if cols else default\n",
        "\n",
        "# -----------------------\n",
        "# 6) Stats helpers (ANOVA with Hue)\n",
        "# -----------------------\n",
        "def _fmt_p(p):\n",
        "    if not np.isfinite(p): return \"n/a\"\n",
        "    return f\"p = {p:.3f}\" if p >= 0.001 else \"p < 0.001\"\n",
        "\n",
        "def _anova_subset(df):\n",
        "    \"\"\"\n",
        "    If >=2 Hue levels: two-way ANOVA (XGroup, HueGroup, interaction)\n",
        "    Else: one-way ANOVA (XGroup).\n",
        "    \"\"\"\n",
        "    out = {\"p_x\": np.nan, \"p_h\": np.nan, \"p_int\": np.nan, \"n_h\": 0, \"ok\": False, \"err\": None}\n",
        "    d = df.dropna(subset=[\"value\",\"XGroup\"])\n",
        "    if d.empty or d[\"XGroup\"].nunique() < 2:\n",
        "        out[\"err\"] = \"Too few groups\"; return out\n",
        "    n_h = d[\"HueGroup\"].nunique(dropna=True); out[\"n_h\"] = n_h\n",
        "    try:\n",
        "        if n_h >= 2:\n",
        "            model = ols('value ~ C(XGroup) + C(HueGroup) + C(XGroup):C(HueGroup)', data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            out[\"p_x\"]   = float(an.loc['C(XGroup)','PR(>F)'])\n",
        "            out[\"p_h\"]   = float(an.loc['C(HueGroup)','PR(>F)'])\n",
        "            out[\"p_int\"] = float(an.loc['C(XGroup):C(HueGroup)','PR(>F)'])\n",
        "            out[\"ok\"] = True\n",
        "        else:\n",
        "            model = ols('value ~ C(XGroup)', data=d).fit()\n",
        "            out[\"p_x\"] = float(model.f_pvalue); out[\"ok\"] = True\n",
        "    except Exception as e:\n",
        "        out[\"err\"] = str(e)\n",
        "    return out\n",
        "\n",
        "def _stats_text(dfm, x_label, hue_label, *, mode=\"ref\", ref_group=None, pair_list=None):\n",
        "    df = dfm.dropna(subset=[\"value\"]).copy()\n",
        "    g_n = df[\"XGroup\"].nunique(dropna=True)\n",
        "    h_n = df[\"HueGroup\"].nunique(dropna=True)\n",
        "\n",
        "    if mode == \"pairs\" and pair_list:\n",
        "        lines = [\"Selected pairwise ANOVA tests:\"]\n",
        "        for a,b in pair_list:\n",
        "            sub = df[df[\"XGroup\"].isin([a,b])]\n",
        "            res = _anova_subset(sub)\n",
        "            if not res[\"ok\"]:\n",
        "                lines.append(f\"{a} vs {b}: {res['err'] or 'failed'}\"); continue\n",
        "            if res[\"n_h\"] >= 2:\n",
        "                lines.append(\n",
        "                    f\"{a} vs {b} (Two-way: {x_label}, {hue_label})  \"\n",
        "                    f\"{x_label}: {_fmt_p(res['p_x'])} | {hue_label}: {_fmt_p(res['p_h'])} | \"\n",
        "                    f\"{x_label}Ã—{hue_label}: {_fmt_p(res['p_int'])}\"\n",
        "                )\n",
        "            else:\n",
        "                lines.append(f\"{a} vs {b} (One-way {x_label}): {_fmt_p(res['p_x'])}\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def fmt(p): return _fmt_p(p)\n",
        "    if g_n == 2 and h_n <= 1:\n",
        "        g1, g2 = sorted(df[\"XGroup\"].unique())\n",
        "        v1 = df[df[\"XGroup\"] == g1][\"value\"].dropna()\n",
        "        v2 = df[df[\"XGroup\"] == g2][\"value\"].dropna()\n",
        "        if len(v1) > 1 and len(v2) > 1:\n",
        "            p = pg.ttest(v1, v2, paired=False)[\"p-val\"].values[0]\n",
        "            return f\"t-test ({x_label}): {fmt(p)}\\n{g1} vs {g2}\"\n",
        "        return \"t-test: not enough data\"\n",
        "\n",
        "    if g_n >= 2 and h_n >= 2:\n",
        "        try:\n",
        "            model = ols('value ~ C(XGroup) + C(HueGroup) + C(XGroup):C(HueGroup)', data=df).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            return (\n",
        "                \"Two-way ANOVA\\n\"\n",
        "                f\"{x_label}: {fmt(float(an.loc['C(XGroup)','PR(>F)']))}\\n\"\n",
        "                f\"{hue_label}: {fmt(float(an.loc['C(HueGroup)','PR(>F)']))}\\n\"\n",
        "                f\"{x_label}Ã—{hue_label}: {fmt(float(an.loc['C(XGroup):C(HueGroup)','PR(>F)']))}\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            return f\"ANOVA failed: {e}\"\n",
        "\n",
        "    if g_n >= 2:\n",
        "        try:\n",
        "            model = ols('value ~ C(XGroup)', data=df).fit()\n",
        "            return f\"One-way ANOVA ({x_label}): {fmt(float(model.f_pvalue))}\"\n",
        "        except Exception as e:\n",
        "            return f\"One-way ANOVA failed: {e}\"\n",
        "    return \"Too few groups for stats\"\n",
        "\n",
        "# -----------------------\n",
        "# 7) Plotting helpers\n",
        "# -----------------------\n",
        "def _p_to_stars(p):\n",
        "    if not np.isfinite(p): return \"\"\n",
        "    if p < 1e-4: return \"****\"\n",
        "    if p < 1e-3: return \"***\"\n",
        "    if p < 1e-2: return \"**\"\n",
        "    if p < 5e-2: return \"*\"\n",
        "    return \"\"\n",
        "\n",
        "def _dot_palette(hues):\n",
        "    hues = list(hues)\n",
        "    if len(hues) == 0: return {}\n",
        "    if len(hues) == 1: return {hues[0]: \"black\"}\n",
        "    if len(hues) == 2: return {hues[0]: \"white\", hues[1]: \"black\"}\n",
        "    defaults = plt.rcParams.get('axes.prop_cycle', None)\n",
        "    colors = defaults.by_key()['color'] if defaults else [\"C0\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\"]\n",
        "    return {h: colors[i % len(colors)] for i, h in enumerate(hues)}\n",
        "\n",
        "def _draw_bracket(ax, x1, x2, y, h, text):\n",
        "    ax.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=\"black\", zorder=5)\n",
        "    ax.text((x1+x2)/2, y+h, text, ha=\"center\", va=\"bottom\", fontsize=16, fontweight=\"bold\")\n",
        "\n",
        "def _plot_metric_clean(df_metric, variable, x_color_map, *, mode=\"ref\", ref_group=None, pair_list=None, return_fig=False):\n",
        "    dfm = df_metric.copy()\n",
        "    order = _order_x_groups(dfm[\"XGroup\"].dropna().unique().tolist())\n",
        "    if not order: return None\n",
        "    if (not ref_group) or (ref_group not in order):\n",
        "        ref_group = _choose_ref_group(order)\n",
        "\n",
        "    x_label_name   = _grouping_label(\"X\")\n",
        "    hue_label_name = _grouping_label(\"Hue\")\n",
        "\n",
        "    hue_levels = [h for h in dfm[\"HueGroup\"].dropna().unique().tolist()]\n",
        "    pal_dots = _dot_palette(hue_levels)\n",
        "\n",
        "    width = max(2, 1 * len(order))\n",
        "    height = 4.0\n",
        "    fig, (ax_plot, ax_text) = plt.subplots(\n",
        "        1, 2, figsize=(width/1.2, height), gridspec_kw={'width_ratios': [3, 1]}\n",
        "    )\n",
        "\n",
        "    # Bars\n",
        "    bar_palette = [x_color_map.get(g, \"tab:blue\") for g in order]\n",
        "    sns.barplot(data=dfm, x=\"XGroup\", y=\"value\", order=order, ci=None, alpha=ALPHA, ax=ax_plot, palette=bar_palette)\n",
        "\n",
        "    # Determine hue levels in a controlled order\n",
        "    raw_hues = dfm[\"HueGroup\"].dropna().unique().tolist()\n",
        "    hue_levels = _order_hue_groups(raw_hues)\n",
        "\n",
        "    # Colors for dots â€” your helper already maps 2 hues as {hues[0]: \"white\", hues[1]: \"black\"}\n",
        "    pal_dots = _dot_palette(hue_levels)\n",
        "\n",
        "    # Points\n",
        "    sns.stripplot(\n",
        "        data=dfm, x=\"XGroup\", y=\"value\",\n",
        "        order=order,\n",
        "        hue=\"HueGroup\",\n",
        "        hue_order=hue_levels,        # <-- enforce hue order\n",
        "        jitter=True, dodge=False, size=7,\n",
        "        edgecolor=\"black\", linewidth=1,\n",
        "        palette=pal_dots,            # <-- colors aligned to hue_order\n",
        "        ax=ax_plot, zorder=3, alpha=ALPHA\n",
        "    )\n",
        "    if ax_plot.legend_ is not None:\n",
        "        ax_plot.legend_.remove()\n",
        "\n",
        "    # Legend (right panel) in the same hue order\n",
        "    if len(hue_levels) >= 2:\n",
        "        handles = [plt.Line2D([0],[0], marker='o', linestyle='None',\n",
        "                              markerfacecolor=pal_dots[h], markeredgecolor='black', label=str(h))\n",
        "                  for h in hue_levels]\n",
        "        ax_text.legend(handles=handles, title=hue_label_name, loc=\"upper left\", bbox_to_anchor=(0, 0.6))\n",
        "\n",
        "    # Annotations\n",
        "    y_min, y_max = ax_plot.get_ylim()\n",
        "    span = (y_max - y_min) if y_max > y_min else 1.0\n",
        "    bump = 0.06 * span\n",
        "    data_max = dfm[\"value\"].max() if dfm[\"value\"].notna().any() else y_max\n",
        "\n",
        "    if mode == \"ref\" and (ref_group in order):\n",
        "        ref_vals = dfm[dfm[\"XGroup\"] == ref_group][\"value\"].dropna().to_numpy()\n",
        "        for g in order:\n",
        "            if g == ref_group: continue\n",
        "            vals = dfm[dfm[\"XGroup\"] == g][\"value\"].dropna().to_numpy()\n",
        "            if len(vals) >= 2 and len(ref_vals) >= 2:\n",
        "                try:\n",
        "                    p = float(pg.ttest(vals, ref_vals, paired=False)[\"p-val\"].values[0])\n",
        "                except Exception:\n",
        "                    p = np.nan\n",
        "                if np.isfinite(p) and p < 0.05:\n",
        "                    xloc = order.index(g)\n",
        "                    gmax = dfm[dfm[\"XGroup\"] == g][\"value\"].max()\n",
        "                    y_star = (gmax if np.isfinite(gmax) else data_max) + bump\n",
        "                    ax_plot.text(xloc, y_star, _p_to_stars(p),\n",
        "                                 ha=\"center\", va=\"bottom\", fontsize=16, fontweight=\"bold\")\n",
        "                    y_max = max(y_max, y_star + bump)\n",
        "        ax_plot.set_ylim(y_min, y_max)\n",
        "\n",
        "    elif mode == \"pairs\" and pair_list:\n",
        "        base = (dfm[\"value\"].max() if dfm[\"value\"].notna().any() else y_max) + bump\n",
        "        step = 0.12 * span\n",
        "        k = 0\n",
        "        for a,b in pair_list:\n",
        "            if (a not in order) or (b not in order):\n",
        "                continue\n",
        "            sub = dfm[dfm[\"XGroup\"].isin([a,b])].dropna(subset=[\"value\"])\n",
        "            if sub[\"XGroup\"].nunique() < 2:\n",
        "                continue\n",
        "            res = _anova_subset(sub)\n",
        "            # draw bracket ONLY if XGroup effect significant\n",
        "            if res[\"ok\"] and np.isfinite(res[\"p_x\"]) and (res[\"p_x\"] < 0.05):\n",
        "                x1 = order.index(a); x2 = order.index(b)\n",
        "                if x1 > x2: x1, x2 = x2, x1\n",
        "                y_here = base + k * step\n",
        "                _draw_bracket(ax_plot, x1, x2, y_here, 0.04 * span, _p_to_stars(res[\"p_x\"]))\n",
        "                y_max = max(y_max, y_here + 0.08 * span)\n",
        "                k += 1\n",
        "        ax_plot.set_ylim(y_min, y_max)\n",
        "\n",
        "    ax_plot.set_title(\"\")\n",
        "    ax_plot.set_xlabel(\"\")\n",
        "    ax_plot.set_ylabel(variable)\n",
        "    plt.setp(ax_plot.get_xticklabels(), rotation=45, ha='right')\n",
        "    sns.despine(ax=ax_plot)\n",
        "\n",
        "    # Right panel: stats + Hue legend\n",
        "    ax_text.axis(\"off\")\n",
        "    ax_text.text(\n",
        "        0, 1,\n",
        "        _stats_text(dfm, x_label_name, hue_label_name, mode=mode, ref_group=ref_group, pair_list=pair_list),\n",
        "        va=\"top\", ha=\"left\", fontsize=12, transform=ax_text.transAxes\n",
        "    )\n",
        "    if len(hue_levels) >= 2:\n",
        "        handles = [plt.Line2D([0],[0], marker='o', linestyle='None',\n",
        "                              markerfacecolor=pal_dots[h], markeredgecolor='black', label=str(h))\n",
        "                   for h in hue_levels]\n",
        "        ax_text.legend(handles=handles, title=hue_label_name, loc=\"upper left\", bbox_to_anchor=(0, 0.6))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig if return_fig else plt.show()\n",
        "\n",
        "# -----------------------\n",
        "# 8) Actions\n",
        "# -----------------------\n",
        "out = widgets.Output()\n",
        "\n",
        "def _selected_x_and_colors():\n",
        "    sel = _selected_x()\n",
        "    color_map = {}\n",
        "    for g in sel:\n",
        "        val = x_colors[g].value.strip()\n",
        "        color_map[g] = val if val else \"tab:blue\"\n",
        "    return sel, color_map\n",
        "\n",
        "def _current_pairs():\n",
        "    return list(pairs_select.value)\n",
        "\n",
        "def _run_plots(_=None):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        sel_x, color_map = _selected_x_and_colors()\n",
        "        if len(sel_x) < 1:\n",
        "            print(\"Select at least one X group.\"); return\n",
        "\n",
        "        mode = mode_radio.value\n",
        "        if mode == \"ref\":\n",
        "            ref = ref_dropdown.value if (ref_dropdown.value in sel_x) else _choose_ref_group(sel_x)\n",
        "            print(f\"Showing X groups: {sel_x}  |  reference for stars: {ref}\")\n",
        "        else:\n",
        "            pair_list = _current_pairs()\n",
        "            if not pair_list:\n",
        "                print(f\"Showing X groups: {sel_x}  |  no pairs selected (select at least one).\"); return\n",
        "            print(f\"Showing X groups: {sel_x}  |  pairs: {pair_list}\")\n",
        "\n",
        "        exclude = {\"PeakAccuracy_Day\",\"PeakAccuracy_Night\",\n",
        "                   \"Win-stay_Day\",\"Win-stay_Night\",\n",
        "                   \"Lose-shift_Day\",\"Lose-shift_Night\"}\n",
        "        metrics = [m for m in long_df[\"variable\"].dropna().unique() if m not in exclude]\n",
        "\n",
        "        for metric in metrics:\n",
        "            subset = long_df[(long_df[\"variable\"] == metric) & (long_df[\"XGroup\"].isin(sel_x))]\n",
        "            if subset[\"value\"].dropna().empty:\n",
        "                print(f\"Skipping {metric} â€” no data for selected X groups.\"); continue\n",
        "            if mode == \"ref\":\n",
        "                _plot_metric_clean(\n",
        "                    subset, metric,\n",
        "                    x_color_map={g: color_map[g] for g in sel_x if g in subset['XGroup'].unique()},\n",
        "                    mode=\"ref\", ref_group=ref\n",
        "                )\n",
        "            else:\n",
        "                _plot_metric_clean(\n",
        "                    subset, metric,\n",
        "                    x_color_map={g: color_map[g] for g in sel_x if g in subset['XGroup'].unique()},\n",
        "                    mode=\"pairs\", pair_list=_current_pairs()\n",
        "                )\n",
        "\n",
        "def _save_plots(_=None):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        sel_x, color_map = _selected_x_and_colors()\n",
        "        if len(sel_x) < 1:\n",
        "            print(\"Select at least one X group.\"); return\n",
        "\n",
        "        mode = mode_radio.value\n",
        "        ref = ref_dropdown.value if (mode == \"ref\") else None\n",
        "        pair_list = _current_pairs() if (mode == \"pairs\") else None\n",
        "        if mode == \"pairs\" and not pair_list:\n",
        "            print(\"Select at least one pair before saving.\"); return\n",
        "\n",
        "        os.makedirs(\"metric_comparisons\", exist_ok=True)\n",
        "        saved = 0\n",
        "\n",
        "        exclude = {\"PeakAccuracy_Day\",\"PeakAccuracy_Night\",\n",
        "                   \"Win-stay_Day\",\"Win-stay_Night\",\n",
        "                   \"Lose-shift_Day\",\"Lose-shift_Night\"}\n",
        "        for metric in [m for m in long_df[\"variable\"].dropna().unique() if m not in exclude]:\n",
        "            subset = long_df[(long_df[\"variable\"] == metric) & (long_df[\"XGroup\"].isin(sel_x))]\n",
        "            if subset[\"value\"].dropna().empty: continue\n",
        "            fig = _plot_metric_clean(\n",
        "                subset, metric,\n",
        "                x_color_map={g: color_map[g] for g in sel_x if g in subset['XGroup'].unique()},\n",
        "                mode=mode, ref_group=ref, pair_list=pair_list, return_fig=True\n",
        "            )\n",
        "            safe = metric.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
        "            fig.savefig(f\"metric_comparisons/{safe}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
        "            plt.close(fig); saved += 1\n",
        "\n",
        "        if saved == 0:\n",
        "            print(\"No figures to save.\"); return\n",
        "        zipname = f\"metric_comparisons_{int(time.time())}.zip\"\n",
        "        shutil.make_archive(zipname.replace(\".zip\",\"\"), 'zip', \"metric_comparisons\")\n",
        "        if colab_files is not None:\n",
        "            colab_files.download(zipname)\n",
        "        print(f\"Saved {zipname}\")\n",
        "\n",
        "plot_btn.on_click(_run_plots)\n",
        "save_btn.on_click(_save_plots)\n",
        "\n",
        "# -----------------------\n",
        "# 9) Assemble compact UI (two columns)\n",
        "# -----------------------\n",
        "# Toggle visibility of ref vs pairs widgets\n",
        "def _toggle_controls(*_):\n",
        "    if mode_radio.value == \"ref\":\n",
        "        ref_dropdown.layout.display = \"\"\n",
        "        pairs_select.layout.display = \"none\"\n",
        "    else:\n",
        "        ref_dropdown.layout.display = \"none\"\n",
        "        pairs_select.layout.display = \"\"\n",
        "_toggle_controls()\n",
        "mode_radio.observe(lambda _: _toggle_controls(), names=\"value\")\n",
        "\n",
        "# Two-column layout container\n",
        "row = widgets.HBox(\n",
        "    [left_col, right_col],\n",
        "    layout=widgets.Layout(\n",
        "        justify_content=\"flex-start\",   # keep columns together\n",
        "        align_items=\"flex-start\",\n",
        "        gap=\"16px\",\n",
        "        width=\"auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='margin-bottom:6px'></h3>\"),\n",
        "    row,\n",
        "    out\n",
        "], layout=widgets.Layout(width=\"auto\"))\n",
        "\n",
        "display(ui)\n",
        "\n",
        "# Auto-run once\n",
        "_run_plots()\n"
      ],
      "metadata": {
        "id": "fhNui51xcBX0",
        "cellView": "form"
      },
      "id": "fhNui51xcBX0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot Day vs Night metrics\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# ---- Config ----\n",
        "BASES = [\"Win-stay\", \"Lose-shift\", \"PeakAccuracy\"]\n",
        "DAY_TAG, NIGHT_TAG = \"_Day\", \"_Night\"\n",
        "BAR_WIDTH = 0.36\n",
        "NIGHT_ALPHA = 0.6\n",
        "DOT_SIZE = 7\n",
        "EDGE_LW_DAY = 2.0\n",
        "EDGE_LW_NIGHT = 1.0\n",
        "\n",
        "# ---- Helpers (reuse from prior cell when available) ----\n",
        "def _safe_order(groups):\n",
        "    groups = [g for g in groups if pd.notna(g)]\n",
        "    if '_order_x_groups' in globals():\n",
        "        try:\n",
        "            return _order_x_groups(groups)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return sorted(groups, key=lambda s: str(s).upper())\n",
        "\n",
        "def _selected_x_groups():\n",
        "    if 'x_checks' in globals() and isinstance(x_checks, dict) and len(x_checks):\n",
        "        sel = [g for g, cb in x_checks.items() if getattr(cb, \"value\", False)]\n",
        "        return _safe_order(sel)\n",
        "    # fallback: all groups present\n",
        "    if 'long_df' not in globals():\n",
        "        raise RuntimeError(\"long_df not found\")\n",
        "    return _safe_order(long_df[\"XGroup\"].dropna().unique().tolist())\n",
        "\n",
        "def _color_map(x_groups):\n",
        "    if 'x_colors' in globals() and isinstance(x_colors, dict) and len(x_colors):\n",
        "        out = {}\n",
        "        for g in x_groups:\n",
        "            w = x_colors.get(g, None)\n",
        "            val = getattr(w, \"value\", None) if w is not None else None\n",
        "            out[g] = (val.strip() if isinstance(val, str) and val.strip() else \"blue\")\n",
        "        return out\n",
        "    defaults = [\"blue\",\"orange\",\"green\",\"red\",\"purple\",\"brown\",\"pink\",\"gray\",\"olive\",\"cyan\"]\n",
        "    return {g: defaults[i % len(defaults)] for i, g in enumerate(x_groups)}\n",
        "\n",
        "def _ensure_daynight_view(df, base):\n",
        "    keep = {f\"{base}{DAY_TAG}\", f\"{base}{NIGHT_TAG}\"}\n",
        "    d = df[df[\"variable\"].isin(keep)].copy()\n",
        "    if d.empty:\n",
        "        return d\n",
        "    d[\"DayNight\"] = np.where(d[\"variable\"].str.endswith(DAY_TAG), \"Day\", \"Night\")\n",
        "    return d\n",
        "\n",
        "def _dot_palette_local(hues):\n",
        "    if '_dot_palette' in globals():\n",
        "        return _dot_palette(hues)\n",
        "    # fallback palette\n",
        "    hues = list(hues)\n",
        "    if len(hues) == 0: return {}\n",
        "    if len(hues) == 1: return {hues[0]: \"black\"}\n",
        "    if len(hues) == 2: return {hues[0]: \"white\", hues[1]: \"black\"}\n",
        "    defaults = plt.rcParams.get('axes.prop_cycle', None)\n",
        "    colors = defaults.by_key()['color'] if defaults else [\"C0\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\"]\n",
        "    return {h: colors[i % len(colors)] for i, h in enumerate(hues)}\n",
        "\n",
        "def _label_for(which=\"X\"):\n",
        "    if '_grouping_label' in globals():\n",
        "        return _grouping_label(which)\n",
        "    return \"XGroup\" if which.lower().startswith(\"x\") else \"HueGroup\"\n",
        "\n",
        "# ---- Stats text (ANOVA with Day/Night included) ----\n",
        "def _anova_daynight_text(df_dn, x_label, hue_label):\n",
        "    d = df_dn.dropna(subset=[\"value\",\"XGroup\",\"DayNight\"]).copy()\n",
        "    if d[\"XGroup\"].nunique() < 2 or d[\"DayNight\"].nunique() < 2:\n",
        "        return \"Too few groups or missing Day/Night to run ANOVA.\"\n",
        "\n",
        "    has_hue = d[\"HueGroup\"].nunique(dropna=True) >= 2\n",
        "    try:\n",
        "        if has_hue:\n",
        "            model = ols('value ~ C(XGroup) + C(DayNight) + C(HueGroup) + '\n",
        "                        'C(XGroup):C(DayNight) + C(XGroup):C(HueGroup) + '\n",
        "                        'C(DayNight):C(HueGroup) + C(XGroup):C(DayNight):C(HueGroup)',\n",
        "                        data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            def pget(term):\n",
        "                return float(an.loc[term, 'PR(>F)']) if term in an.index else np.nan\n",
        "            def fmt(p):\n",
        "                if not np.isfinite(p): return \"n/a\"\n",
        "                return f\"p = {p:.3f}\" if p >= 0.001 else \"p < 0.001\"\n",
        "            return (\n",
        "                \"Three-way ANOVA (XGroup, Day/Night, Hue)\\n\"\n",
        "                f\"{x_label}: {fmt(pget('C(XGroup)'))}\\n\"\n",
        "                f\"Day/Night: {fmt(pget('C(DayNight)'))}\\n\"\n",
        "                f\"{hue_label}: {fmt(pget('C(HueGroup)'))}\\n\"\n",
        "                f\"{x_label}Ã—Day/Night: {fmt(pget('C(XGroup):C(DayNight)'))}\\n\"\n",
        "                f\"{x_label}Ã—{hue_label}: {fmt(pget('C(XGroup):C(HueGroup)'))}\\n\"\n",
        "                f\"Day/NightÃ—{hue_label}: {fmt(pget('C(DayNight):C(HueGroup)'))}\\n\"\n",
        "                f\"{x_label}Ã—Day/NightÃ—{hue_label}: {fmt(pget('C(XGroup):C(DayNight):C(HueGroup)'))}\"\n",
        "            )\n",
        "        else:\n",
        "            model = ols('value ~ C(XGroup) + C(DayNight) + C(XGroup):C(DayNight)', data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            def fmt(p):\n",
        "                if not np.isfinite(p): return \"n/a\"\n",
        "                return f\"p = {p:.3f}\" if p >= 0.001 else \"p < 0.001\"\n",
        "            return (\n",
        "                \"Two-way ANOVA (XGroup, Day/Night)\\n\"\n",
        "                f\"{_label_for('X')}: {fmt(float(an.loc['C(XGroup)','PR(>F)']))}\\n\"\n",
        "                f\"Day/Night: {fmt(float(an.loc['C(DayNight)','PR(>F)']))}\\n\"\n",
        "                f\"{_label_for('X')}Ã—Day/Night: {fmt(float(an.loc['C(XGroup):C(DayNight)','PR(>F)']))}\"\n",
        "            )\n",
        "    except Exception as e:\n",
        "        return f\"ANOVA failed: {e}\"\n",
        "\n",
        "# ---- Main plotting for a single base ----\n",
        "def _plot_day_night_for_base(base, x_groups, color_map):\n",
        "    # subset\n",
        "    df_dn = _ensure_daynight_view(long_df, base)\n",
        "    df_dn = df_dn[df_dn[\"XGroup\"].isin(x_groups)].copy()\n",
        "    if df_dn.empty:\n",
        "        print(f\"Skipping {base}: no Day/Night data for selected groups.\")\n",
        "        return\n",
        "\n",
        "    # stats labels\n",
        "    x_label = _label_for(\"X\")\n",
        "    hue_label = _label_for(\"Hue\")\n",
        "\n",
        "    # palette for dots by HueGroup\n",
        "    hue_levels = [h for h in df_dn[\"HueGroup\"].dropna().unique().tolist()]\n",
        "    pal_dots = _dot_palette_local(hue_levels)\n",
        "\n",
        "    # compute means per group/daynight\n",
        "    means = (df_dn.dropna(subset=[\"value\"])\n",
        "                  .groupby([\"XGroup\",\"DayNight\"], as_index=False)[\"value\"]\n",
        "                  .mean().rename(columns={\"value\":\"mean\"}))\n",
        "    # arrange grid for fast lookup\n",
        "    grid = (means.set_index([\"XGroup\",\"DayNight\"])[\"mean\"]\n",
        "                 .unstack(\"DayNight\")\n",
        "                 .reindex(index=x_groups, columns=[\"Day\",\"Night\"]))\n",
        "\n",
        "    # figure layout: plot + stats panel\n",
        "    width = max(4, 1.2 * len(x_groups))\n",
        "    fig, (ax, ax_txt) = plt.subplots(1, 2, figsize=(width, 4.6), gridspec_kw={'width_ratios': [3, 1]})\n",
        "\n",
        "    # bar positions\n",
        "    x = np.arange(len(x_groups))\n",
        "    off = BAR_WIDTH/2.0 + 0.02\n",
        "    pos_day = x - off\n",
        "    pos_night = x + off\n",
        "\n",
        "    # N I G H T (filled)\n",
        "    for i, g in enumerate(x_groups):\n",
        "        val = grid.loc[g, \"Night\"] if (\"Night\" in grid.columns) else np.nan\n",
        "        if pd.notna(val):\n",
        "            ax.bar(pos_night[i], val, width=BAR_WIDTH, color=color_map[g],\n",
        "                   alpha=NIGHT_ALPHA, edgecolor=\"black\", linewidth=EDGE_LW_NIGHT, zorder=2)\n",
        "\n",
        "    # D A Y (outline only)\n",
        "    for i, g in enumerate(x_groups):\n",
        "        val = grid.loc[g, \"Day\"] if (\"Day\" in grid.columns) else np.nan\n",
        "        if pd.notna(val):\n",
        "            ax.bar(pos_day[i], val, width=BAR_WIDTH, facecolor=(0,0,0,0),\n",
        "                   edgecolor=color_map[g], linewidth=EDGE_LW_DAY, zorder=3)\n",
        "\n",
        "    # overlay individual dots by HueGroup at each bar position\n",
        "    rng = np.random.default_rng(42)\n",
        "    jitter = lambda n: (rng.normal(0, 0.02, size=n))\n",
        "\n",
        "    # Day dots (outline markers)\n",
        "    sdf = df_dn[df_dn[\"DayNight\"] == \"Day\"].dropna(subset=[\"value\"])\n",
        "    for g in x_groups:\n",
        "        sub = sdf[sdf[\"XGroup\"] == g]\n",
        "        if sub.empty: continue\n",
        "        px = pos_day[x_groups.index(g)]\n",
        "        xs = px + jitter(len(sub))\n",
        "        # color by HueGroup; outline only\n",
        "        for h in sub[\"HueGroup\"].unique():\n",
        "            hh = sub[sub[\"HueGroup\"] == h]\n",
        "            if hh.empty: continue\n",
        "            ax.scatter(np.full(len(hh), px) + jitter(len(hh)), hh[\"value\"],\n",
        "                       s=DOT_SIZE**2/2, facecolors=pal_dots.get(h, \"black\"),\n",
        "                       edgecolors=\"black\", linewidths=0.6, alpha=NIGHT_ALPHA, zorder=4)\n",
        "\n",
        "    # Night dots (filled markers)\n",
        "    sdf = df_dn[df_dn[\"DayNight\"] == \"Night\"].dropna(subset=[\"value\"])\n",
        "    for g in x_groups:\n",
        "        sub = sdf[sdf[\"XGroup\"] == g]\n",
        "        if sub.empty: continue\n",
        "        px = pos_night[x_groups.index(g)]\n",
        "        for h in sub[\"HueGroup\"].unique():\n",
        "            hh = sub[sub[\"HueGroup\"] == h]\n",
        "            if hh.empty: continue\n",
        "            ax.scatter(np.full(len(hh), px) + jitter(len(hh)), hh[\"value\"],\n",
        "                       s=DOT_SIZE**2/2, facecolors=pal_dots.get(h, \"black\"),\n",
        "                       edgecolors=\"black\", linewidths=0.6, alpha=NIGHT_ALPHA, zorder=4)\n",
        "\n",
        "    # axes cosmetics\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(x_groups, rotation=45, ha=\"right\")\n",
        "    ax.set_ylabel(base)\n",
        "    ax.set_title(f\"{base}\")\n",
        "    sns.despine(ax=ax)\n",
        "\n",
        "\n",
        "\n",
        "    # stats panel text\n",
        "    ax_txt.axis(\"off\")\n",
        "    ax_txt.text(0, 1, _anova_daynight_text(df_dn, x_label, hue_label),\n",
        "                va=\"top\", ha=\"left\", fontsize=12, transform=ax_txt.transAxes)\n",
        "\n",
        "    if len(hue_levels) >= 2:\n",
        "        handles = [plt.Line2D([0],[0], marker='o', linestyle='None',\n",
        "                              markerfacecolor=pal_dots[h], markeredgecolor='black',\n",
        "                              label=str(h)) for h in hue_levels]\n",
        "        ax_txt.legend(handles=handles, title=hue_label,\n",
        "                      loc=\"upper left\", bbox_to_anchor=(0, 0.3),\n",
        "                      frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---- Run: plot for each base metric ----\n",
        "if 'long_df' not in globals():\n",
        "    raise RuntimeError(\"This cell expects long_df from the previous cell.\")\n",
        "\n",
        "Xsel = _selected_x_groups()\n",
        "if not Xsel:\n",
        "    print(\"No X groups selected or available.\")\n",
        "else:\n",
        "    cmap = _color_map(Xsel)\n",
        "    for base in BASES:\n",
        "        _plot_day_night_for_base(base, Xsel, cmap)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xP24QhI7wdj8"
      },
      "id": "xP24QhI7wdj8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Peak Accuracy\n",
        "\n",
        "import os, numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
        "\n",
        "# --- Config ---\n",
        "TRIALS = 11   # trials before/after switch (i.e., window = [-11, +11])\n",
        "\n",
        "# --- Preconditions ---\n",
        "if 'feds' not in globals() or not isinstance(feds, (list, tuple)) or len(feds) == 0:\n",
        "    raise RuntimeError(\"No FED3 sessions found in `feds`.\")\n",
        "\n",
        "if 'files_to_group_both' not in globals() or files_to_group_both is None or files_to_group_both.empty:\n",
        "    raise RuntimeError(\"`files_to_group_both` is missing/empty. Build Groups first.\")\n",
        "\n",
        "# filename -> XGroup map (match on basename)\n",
        "grp = files_to_group_both.copy()\n",
        "src_col = \"filename\" if \"filename\" in grp.columns else (\"File\" if \"File\" in grp.columns else None)\n",
        "if src_col is None:\n",
        "    raise RuntimeError(\"`files_to_group_both` must include 'filename' or 'File'.\")\n",
        "basename = lambda s: os.path.basename(str(s))\n",
        "grp[\"__base__\"] = grp[src_col].astype(str).map(basename)\n",
        "fname_to_xgroup = dict(grp[[\"__base__\", \"XGroup\"]].dropna().values)\n",
        "\n",
        "rows = []\n",
        "\n",
        "def _is_empty(x):\n",
        "    if x is None: return True\n",
        "    try:\n",
        "        return len(x) == 0\n",
        "    except Exception:\n",
        "        try:\n",
        "            return np.size(x) == 0\n",
        "        except Exception:\n",
        "            return True\n",
        "\n",
        "# --- Build rev_df ---\n",
        "for i, sess in enumerate(feds):\n",
        "    base = basename(getattr(sess, \"name\", f\"session_{i}\"))\n",
        "    xg = fname_to_xgroup.get(base, \"UNASSIGNED\")\n",
        "\n",
        "    # compute peri-switch trials using your f3b helper\n",
        "    try:\n",
        "        peh = f3b.reversal_peh(sess, (-TRIALS, TRIALS), return_avg=False)\n",
        "    except Exception as e:\n",
        "        print(f\"[skip] {base}: reversal_peh failed: {e}\")\n",
        "        continue\n",
        "\n",
        "    if _is_empty(peh):\n",
        "        continue\n",
        "\n",
        "    # peh is list/array of trial-length arrays; stack defensively\n",
        "    for tr in list(peh):\n",
        "        arr = np.asarray(tr).ravel()\n",
        "        for t, v in enumerate(arr):\n",
        "            rows.append({\n",
        "                \"Timepoint\": t - TRIALS + 1,    # center around 0\n",
        "                \"Value\": float(v) if np.isfinite(v) else np.nan,\n",
        "                \"Display_Group\": xg\n",
        "            })\n",
        "\n",
        "rev_df = pd.DataFrame(rows)\n",
        "rev_df = rev_df[np.isfinite(rev_df[\"Value\"])]\n",
        "rev_df = rev_df[rev_df[\"Timepoint\"] != 0] # optional: drop center trial\n",
        "if rev_df.empty:\n",
        "    raise RuntimeError(\"No peri-switch data produced.\")\n",
        "\n",
        "\n",
        "# WT/Control detector (same as other cell)\n",
        "def _is_wt_group_label(g):\n",
        "    u = str(g).strip().upper()\n",
        "    toks = [t for t in re.split(r'[^A-Z0-9]+', u) if t]\n",
        "    return any(t in {\"WT\", \"WILDTYPE\", \"CONTROL\", \"CTRL\"} for t in toks)\n",
        "\n",
        "def _x_levels_local(g):\n",
        "    s = str(g)\n",
        "    return [p.strip() for p in s.split(\"|\")]\n",
        "\n",
        "def _hier_sort_key_local(g):\n",
        "    lv = _x_levels_local(g)\n",
        "    wt_present = any(_is_wt_group_label(tok) for tok in lv) or _is_wt_group_label(g)\n",
        "    wt_rank = 0 if wt_present else 1\n",
        "    norm = []\n",
        "    for tok in lv:\n",
        "        is_blank = 1 if str(tok).strip().upper() in {\"\", \"UNASSIGNED\", \"NONE\", \"NA\", \"N/A\"} else 0\n",
        "        norm.append((is_blank, str(tok).upper()))\n",
        "    return (wt_rank,) + tuple(norm) + (str(g).upper(),)\n",
        "\n",
        "def _order_x_groups_local(groups):\n",
        "    return sorted(groups, key=_hier_sort_key_local)\n",
        "\n",
        "_present = rev_df[\"Display_Group\"].dropna().unique().tolist()\n",
        "\n",
        "# ORDER: if the first cell was run, follow its ordered_x; otherwise reproduce the same rule.\n",
        "if \"ordered_x\" in globals():\n",
        "    group_order = [g for g in ordered_x if g in _present] + [g for g in _present if g not in ordered_x]\n",
        "else:\n",
        "    group_order = _order_x_groups_local(_present)\n",
        "\n",
        "# COLORS: pull from the first cell's x_colors widget map; fall back if unavailable.\n",
        "palette_map = None\n",
        "if \"x_colors\" in globals() and isinstance(x_colors, dict) and len(x_colors) > 0:\n",
        "    def _col(g):\n",
        "        try:\n",
        "            v = x_colors[g].value\n",
        "            v = v.strip() if isinstance(v, str) else \"\"\n",
        "            return v if v else \"tab:blue\"\n",
        "        except Exception:\n",
        "            return \"tab:blue\"\n",
        "    palette_map = {g: _col(g) for g in group_order}\n",
        "plt.figure(figsize=(10, 7))\n",
        "# --- Plot (lines with SEM ribbons) ---\n",
        "group_order = sorted(rev_df[\"Display_Group\"].dropna().unique().tolist())\n",
        "\n",
        "ax = sns.lineplot(\n",
        "    data=rev_df.sort_values([\"Display_Group\", \"Timepoint\"]),\n",
        "    x=\"Timepoint\",\n",
        "    y=\"Value\",\n",
        "    hue=\"Display_Group\",\n",
        "    hue_order=group_order,\n",
        "    palette=palette_map,\n",
        "    estimator=\"mean\",\n",
        "    errorbar=\"se\",\n",
        "    n_boot=0,\n",
        "    lw=2\n",
        ")\n",
        "\n",
        "# Switch marker and label\n",
        "ax.axvline(x=0, color=\"black\", linestyle=\"--\", linewidth=1.25)\n",
        "ax.set_ylim(bottom=0)\n",
        "ymin, ymax = ax.get_ylim()\n",
        "ax.text(0.5, ymin + 0.95*(ymax - ymin), \"Switch\", color=\"red\",\n",
        "        fontsize=12, ha=\"left\", va=\"top\")\n",
        "\n",
        "ax.set_xlabel(\"Trials around switch\")\n",
        "ax.set_ylabel(\"Peak Accuracy\")\n",
        "ax.set_title(\"\")\n",
        "ax.legend(\n",
        "    title=\"\",\n",
        "    frameon=False,\n",
        "    loc=\"center left\",\n",
        "    bbox_to_anchor=(1.12, 0.5),\n",
        "    borderaxespad=0.0\n",
        ")\n",
        "\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Embed TrueType fonts so text stays editable in Illustrator/Inkscape\n",
        "mpl.rcParams['pdf.fonttype'] = 42\n",
        "mpl.rcParams['ps.fonttype']  = 42\n",
        "\n",
        "outdir = \"figures\"\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "fname = f\"peak_accuracy_{datetime.now():%Y-%m-%d}.pdf\"\n",
        "\n",
        "fig = ax.get_figure()  # or plt.gcf() if you prefer\n",
        "fig.savefig(\n",
        "    os.path.join(outdir, fname),\n",
        "    format=\"pdf\",\n",
        "    bbox_inches=\"tight\",\n",
        "    transparent=True,   # set True if you want a transparent background\n",
        ")\n",
        "files.download(os.path.join(outdir, fname))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VJSi78otlrKS",
        "cellView": "form"
      },
      "id": "VJSi78otlrKS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cluster heatmap & PCA\n",
        "\n",
        "import os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as mcolors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ---------- Pick Bandit source ----------\n",
        "if 'Banditmetrics_merged' in globals() and Banditmetrics_merged is not None and not Banditmetrics_merged.empty:\n",
        "    Bandit_src = Banditmetrics_merged.copy()\n",
        "elif 'Banditmetrics_csv' in globals() and Banditmetrics_csv is not None and not Banditmetrics_csv.empty:\n",
        "    Bandit_src = Banditmetrics_csv.copy()\n",
        "elif 'Banditmetrics' in globals() and Banditmetrics is not None and not Banditmetrics.empty:\n",
        "    Bandit_src = Banditmetrics.copy()\n",
        "else:\n",
        "    raise RuntimeError(\"No Bandit table found. Expect Banditmetrics_merged, Banditmetrics_csv, or Banditmetrics.\")\n",
        "\n",
        "# Ensure we have a File column (fallback from filename if needed)\n",
        "if 'File' not in Bandit_src.columns:\n",
        "    if 'filename' in Bandit_src.columns:\n",
        "        Bandit_src = Bandit_src.copy()\n",
        "        Bandit_src['File'] = Bandit_src['filename'].astype(str)\n",
        "    else:\n",
        "        raise RuntimeError(\"Bandit table must include 'File' or 'filename' column.\")\n",
        "\n",
        "# ---------- Groups from the Group UI ----------\n",
        "def _basename(p): return os.path.basename(str(p))\n",
        "\n",
        "if 'files_to_group_both' in globals() and files_to_group_both is not None and not files_to_group_both.empty:\n",
        "    grp_map = files_to_group_both.copy()\n",
        "    src_col = 'filename' if 'filename' in grp_map.columns else 'File'\n",
        "    grp_map[\"File_base\"] = grp_map[src_col].astype(str).apply(_basename)\n",
        "    grp_map = grp_map[[\"File_base\",\"XGroup\",\"HueGroup\"]]\n",
        "else:\n",
        "    raise RuntimeError(\"No groups found. Run the 'Group for plotting' widget (two-column) and click 'Build Groups'.\")\n",
        "\n",
        "# ---------- Merge groups into a working table (ldf) ----------\n",
        "ldf = Bandit_src.copy()\n",
        "ldf[\"File_base\"] = ldf[\"File\"].astype(str).apply(_basename)\n",
        "ldf = ldf.merge(grp_map, on=\"File_base\", how=\"left\")\n",
        "ldf[\"XGroup\"] = ldf[\"XGroup\"].fillna(\"UNASSIGNED\")\n",
        "ldf[\"HueGroup\"] = ldf[\"HueGroup\"].fillna(\"ALL\")\n",
        "\n",
        "# ---------- STRICT METRIC SELECTION (whitelist patterns) ----------\n",
        "# Base metric names you actually have in Banditmetrics\n",
        "_METRIC_BASES = [\n",
        "    \"Win-stay\", \"Lose-shift\", \"PeakAccuracy\",\n",
        "    \"Total_pellets\", \"Total_pokes\",\n",
        "    \"PokesPerPellet\", \"RetrievalTime\", \"PokeTime\",\n",
        "    \"Daily_Pellets\",\n",
        "    # include any others you compute, e.g.:\n",
        "    \"Within_meal_pellet_rate\",\n",
        "]\n",
        "\n",
        "# Build a regex that allows optional _Day/_Night and optional session-type suffixes\n",
        "# e.g., PeakAccuracy, PeakAccuracy_Day, PeakAccuracy_FR1, PeakAccuracy_Day_FR1, etc.\n",
        "def _metric_regex_from_bases(bases):\n",
        "    safes = [re.escape(b) for b in bases]\n",
        "    # ^(?:BASE1|BASE2)...(?:_(Day|Night))?(?:_.+)?$\n",
        "    return re.compile(rf\"^(?:{'|'.join(safes)})(?:_(?:Day|Night))?(?:_.+)?$\")\n",
        "\n",
        "_METRIC_RX = _metric_regex_from_bases(_METRIC_BASES)\n",
        "\n",
        "ID_LIKE = {\n",
        "    \"File\",\"filename\",\"Mouse_ID\",\"Strain\",\"Sex\",\"Genotype\",\"Session_type\",\n",
        "    \"File_base\",\"XGroup\",\"HueGroup\"\n",
        "}\n",
        "\n",
        "# Select metric columns by name pattern only (not dtype)\n",
        "metric_columns = [c for c in ldf.columns\n",
        "                  if isinstance(c, str) and (c not in ID_LIKE) and _METRIC_RX.match(c)]\n",
        "if not metric_columns:\n",
        "    raise RuntimeError(\"No metric columns matched the whitelist. Check names or extend _METRIC_BASES.\")\n",
        "\n",
        "# Coerce metric columns to numeric\n",
        "for c in metric_columns:\n",
        "    ldf[c] = pd.to_numeric(ldf[c], errors=\"coerce\")\n",
        "\n",
        "# ---------- Build long & wide ----------\n",
        "id_keep = [c for c in [\"File\",\"filename\",\"Mouse_ID\",\"Strain\",\"Sex\",\"Genotype\",\"Session_type\",\"XGroup\",\"HueGroup\"] if c in ldf.columns]\n",
        "\n",
        "long_df = ldf.melt(\n",
        "    id_vars=id_keep,\n",
        "    value_vars=metric_columns,\n",
        "    var_name=\"metric\",\n",
        "    value_name=\"value\"\n",
        ").copy()\n",
        "\n",
        "# Wide (one row per file/animal), average duplicates\n",
        "wide_index = [c for c in [\"File\",\"filename\",\"Mouse_ID\",\"Strain\",\"Sex\",\"Genotype\",\"XGroup\",\"HueGroup\"] if c in long_df.columns]\n",
        "wide_metrics = (\n",
        "    long_df.pivot_table(\n",
        "        index=wide_index,\n",
        "        columns=\"metric\",\n",
        "        values=\"value\",\n",
        "        aggfunc=\"mean\",\n",
        "        observed=True\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Metric columns are those not in the index\n",
        "metric_columns = [c for c in wide_metrics.columns if c not in wide_index]\n",
        "\n",
        "# ---------- Group means by (XGroup, HueGroup) for heatmap & PCA feature set ----------\n",
        "if (\"XGroup\" not in wide_metrics.columns) or (\"HueGroup\" not in wide_metrics.columns):\n",
        "    raise RuntimeError(\"XGroup and HueGroup are required to cluster on both.\")\n",
        "\n",
        "_x_order = sorted(wide_metrics[\"XGroup\"].astype(str).unique())\n",
        "_h_order = sorted(wide_metrics[\"HueGroup\"].astype(str).unique())\n",
        "\n",
        "group_means = (\n",
        "    wide_metrics\n",
        "    .assign(XGroup=wide_metrics[\"XGroup\"].astype(str),\n",
        "            HueGroup=wide_metrics[\"HueGroup\"].astype(str))\n",
        "    .groupby([\"XGroup\", \"HueGroup\"], dropna=False)[metric_columns]\n",
        "    .mean()\n",
        "    .reindex(pd.MultiIndex.from_product([_x_order, _h_order],\n",
        "                                        names=[\"XGroup\",\"HueGroup\"]))\n",
        ")\n",
        "\n",
        "# Human-readable row labels \"X | Hue\"\n",
        "group_means.index = [f\"{x} | {h}\" for x, h in group_means.index]\n",
        "\n",
        "# ---------- Heatmap (minâ€“max per column, with numeric annotations) ----------\n",
        "def _fmt_cell(x):\n",
        "    if pd.isna(x): return \"\"\n",
        "    ax = abs(float(x))\n",
        "    return f\"{x:.0f}\" if ax >= 100 else f\"{x:.1f}\" if ax >= 10 else f\"{x:.2f}\"\n",
        "\n",
        "annot_data = group_means.applymap(_fmt_cell)\n",
        "\n",
        "heatmap_scaled = group_means.copy()\n",
        "for col in heatmap_scaled.columns:\n",
        "    col_min, col_max = heatmap_scaled[col].min(), heatmap_scaled[col].max()\n",
        "    if pd.isna(col_min) or pd.isna(col_max):\n",
        "        heatmap_scaled[col] = 0.0\n",
        "    elif col_max == col_min:\n",
        "        heatmap_scaled[col] = 0.5\n",
        "    else:\n",
        "        heatmap_scaled[col] = (heatmap_scaled[col] - col_min) / (col_max - col_min)\n",
        "\n",
        "# keep same row order for annotations\n",
        "annot_data = annot_data.loc[heatmap_scaled.index]\n",
        "\n",
        "# ---------- PCA (same metric set) ----------\n",
        "pca_features = metric_columns[:]  # same set used in heatmap\n",
        "mouse_data = wide_metrics.dropna(subset=pca_features).copy()\n",
        "\n",
        "# Labels: prefer Mouse_ID, else filename (basename), else File\n",
        "if \"Mouse_ID\" in mouse_data.columns and mouse_data[\"Mouse_ID\"].notna().any():\n",
        "    labels = mouse_data[\"Mouse_ID\"].astype(str)\n",
        "elif \"filename\" in mouse_data.columns and mouse_data[\"filename\"].notna().any():\n",
        "    labels = mouse_data[\"filename\"].astype(str).apply(lambda p: os.path.basename(str(p)))\n",
        "else:\n",
        "    labels = mouse_data[\"File\"].astype(str)\n",
        "mouse_data[\"Label\"] = labels\n",
        "\n",
        "for col, default in [(\"XGroup\", \"UNASSIGNED\"), (\"HueGroup\", \"ALL\")]:\n",
        "    if col in mouse_data.columns:\n",
        "        mouse_data[col] = mouse_data[col].astype(str)\n",
        "    else:\n",
        "        mouse_data[col] = default\n",
        "\n",
        "X = StandardScaler().fit_transform(mouse_data[pca_features].values)\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(X)\n",
        "\n",
        "pca_df = pd.DataFrame(pca_result, columns=[\"PC1\", \"PC2\"])\n",
        "pca_df[\"Label\"]    = mouse_data[\"Label\"].values\n",
        "pca_df[\"XGroup\"]   = mouse_data[\"XGroup\"].values\n",
        "pca_df[\"HueGroup\"] = mouse_data[\"HueGroup\"].values\n",
        "\n",
        "# Loadings (top 8 by |PC1|)\n",
        "loadings = pd.DataFrame(pca.components_.T, index=pca_features, columns=[\"PC1\", \"PC2\"])\n",
        "top8_features = loadings.reindex(loadings[\"PC1\"].abs().sort_values(ascending=False).head(8).index)\n",
        "loadings_melted = top8_features[[\"PC1\", \"PC2\"]].reset_index().melt(id_vars=\"index\", var_name=\"PC\", value_name=\"Loading\")\n",
        "\n",
        "# ---------- Plot (heatmap on top; PCA + loadings below) ----------\n",
        "fig = plt.figure(figsize=(18, 12), constrained_layout=True)\n",
        "gs = gridspec.GridSpec(\n",
        "    2, 2, figure=fig,\n",
        "    height_ratios=[0.8, 1.0],\n",
        "    width_ratios=[1.0, 0.8],\n",
        "    hspace=0.1, wspace=0.1\n",
        ")\n",
        "\n",
        "ax_heat    = fig.add_subplot(gs[0, :])   # top spans both\n",
        "ax_scatter = fig.add_subplot(gs[1, 0])   # bottom-left\n",
        "ax_bar     = fig.add_subplot(gs[1, 1])   # bottom-right\n",
        "\n",
        "# 1) Heatmap\n",
        "sns.heatmap(\n",
        "    heatmap_scaled,\n",
        "    ax=ax_heat,\n",
        "    annot=annot_data.values,\n",
        "    fmt=\"\",\n",
        "    cmap=\"Blues\",\n",
        "    linewidths=0.5,\n",
        "    linecolor='gray',\n",
        "    alpha=0.5,\n",
        "    cbar=True\n",
        ")\n",
        "ax_heat.tick_params(axis='x', rotation=70)\n",
        "ax_heat.tick_params(axis='y', rotation=0)\n",
        "ax_heat.set_title(\"\", fontsize=16, color=\"darkblue\")\n",
        "ax_heat.set_xlabel(\"\"); ax_heat.set_ylabel(\"\")\n",
        "\n",
        "# 2) PCA scatter: color by XGroup; marker encodes HueGroup\n",
        "unique_x = sorted(pca_df[\"XGroup\"].unique())\n",
        "\n",
        "x_colors = {}\n",
        "if len(unique_x) >= 1: x_colors[unique_x[0]] = \"dodgerblue\"\n",
        "if len(unique_x) >= 2: x_colors[unique_x[1]] = \"red\"\n",
        "if len(unique_x) > 2:\n",
        "    cmap = cm.get_cmap(\"tab20\", len(unique_x) - 2)\n",
        "    for i, grp in enumerate(unique_x[2:]):\n",
        "        x_colors[grp] = mcolors.to_hex(cmap(i))\n",
        "\n",
        "all_hues = sorted([h for h in pca_df[\"HueGroup\"].unique()])\n",
        "if len(all_hues) == 0:\n",
        "    all_hues = [\"ALL\"]\n",
        "if len(all_hues) == 1:\n",
        "    hue_to_marker = {all_hues[0]: (\"o\", \"filled\")}\n",
        "elif len(all_hues) == 2:\n",
        "    hue_to_marker = {all_hues[0]: (\"o\", \"hollow\"), all_hues[1]: (\"o\", \"filled\")}\n",
        "else:\n",
        "    marker_cycle = [\"o\", \"s\", \"^\", \"D\", \"P\", \"X\", \"*\", \"v\", \"<\", \">\"]\n",
        "    hue_to_marker = {h: (marker_cycle[i % len(marker_cycle)], \"filled\") for i, h in enumerate(all_hues)}\n",
        "\n",
        "for xg in unique_x:\n",
        "    sub_x = pca_df[pca_df[\"XGroup\"] == xg]\n",
        "    for hg in all_hues:\n",
        "        sub = sub_x[sub_x[\"HueGroup\"] == hg]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        marker, fill = hue_to_marker[hg]\n",
        "        if fill == \"hollow\":\n",
        "            ax_scatter.scatter(\n",
        "                sub[\"PC1\"], sub[\"PC2\"],\n",
        "                edgecolors=x_colors.get(xg, \"black\"), facecolors=\"none\",\n",
        "                s=110, linewidth=1.2, marker=marker, alpha=0.85,\n",
        "                label=f\"{xg} | {hg}\"\n",
        "            )\n",
        "        else:\n",
        "            ax_scatter.scatter(\n",
        "                sub[\"PC1\"], sub[\"PC2\"],\n",
        "                color=x_colors.get(xg, \"black\"),\n",
        "                s=110, linewidth=0.8, marker=marker, alpha=0.85,\n",
        "                label=f\"{xg} | {hg}\"\n",
        "            )\n",
        "\n",
        "# point labels\n",
        "for _, row in pca_df.iterrows():\n",
        "    ax_scatter.text(\n",
        "        row[\"PC1\"] + 0.05, row[\"PC2\"] + 0.12, str(row[\"Label\"]),\n",
        "        fontsize=9, color=\"gray\", alpha=0.6, ha=\"center\", va=\"bottom\"\n",
        "    )\n",
        "\n",
        "ax_scatter.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
        "ax_scatter.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
        "\n",
        "handles, labels = ax_scatter.get_legend_handles_labels()\n",
        "pairs = sorted({(lab, h) for lab, h in zip(labels, handles)}, key=lambda x: x[0])\n",
        "if pairs:\n",
        "    sorted_labels, sorted_handles = zip(*pairs)\n",
        "    ax_scatter.legend(sorted_handles, sorted_labels, frameon=True, title=\"XGroup | HueGroup\", fontsize=9)\n",
        "\n",
        "# 3) Loadings barplot\n",
        "sns.barplot(\n",
        "    data=loadings_melted, y=\"index\", x=\"Loading\",\n",
        "    hue=\"PC\", hue_order=[\"PC1\", \"PC2\"],\n",
        "    ax=ax_bar, palette=[\"purple\", \"orange\"], alpha=0.5\n",
        ")\n",
        "ax_bar.set_xlabel(\"Loading Weight\")\n",
        "ax_bar.set_title(\"Top 8 PC Loadings (metrics only)\", fontsize=14)\n",
        "ax_bar.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
        "ax_bar.set_ylabel(\"\")\n",
        "ax_bar.legend(title=\"\", frameon=False, fontsize=9)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tGGBO1iG9FdC",
        "cellView": "form"
      },
      "id": "tGGBO1iG9FdC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (Spyder)",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}