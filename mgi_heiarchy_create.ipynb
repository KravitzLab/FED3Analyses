{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMq95USwr64OmerTRpibcSA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/FED3Analyses/blob/PCA_analysis/mgi_heiarchy_create.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install owlready2\n",
        "!pip install --upgrade jinja2 pyvis\n",
        "!pip install pygraphviz\n",
        "!pip install pronto\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tSftAIGCb0Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install and Import libraries\n",
        "\n",
        "#!pip install pronto\n",
        "#!pip install pyvis\n",
        "#!pip install sentence-transformers\n",
        "\n",
        "import os, re, zipfile\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import csv\n",
        "import pronto\n",
        "import pyvis\n",
        "import ipywidgets as widgets\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "from google.colab import files\n",
        "import networkx as nx\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as mcolors\n",
        "import csv\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import html\n",
        "import faiss\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import combinations\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vDQuWFjhOM6Z",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Upload Ontology File\n",
        "\n",
        "\n",
        "#### Upload ontology file ####\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "#### Load ontology ####\n",
        "mp = pronto.Ontology(filename)\n",
        "\n",
        "\n",
        "# Identify roots (should only have 1 root for MGI)\n",
        "roots = [t for t in mp.terms() if not list(t.superclasses(distance=1))]\n",
        "\n",
        "\n",
        "# Depth function to calculate the minimum distance to the root\n",
        "def get_depth(term):\n",
        "    distances = []\n",
        "    for root in roots:\n",
        "        d = term.distance_from(root)\n",
        "        if d is not None:\n",
        "            distances.append(d)\n",
        "    return min(distances) if distances else 0\n",
        "\n",
        "# Calculate ancestors and descendents\n",
        "def count_ancestors(term):\n",
        "    return len(list(term.superclasses())) - 1  # subtract itself\n",
        "\n",
        "def count_descendants(term):\n",
        "    return len(list(term.subclasses())) - 1  # subtract itself\n",
        "\n",
        "def is_leaf(term):\n",
        "    return len(list(term.subclasses(distance=1))) == 0\n",
        "\n",
        "\n",
        "#### Output CSV ####\n",
        "# this is all nodes for the entire ontology\n",
        "output_file = \"ontology_edges_with_metadata.csv\"\n",
        "\n",
        "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\n",
        "        \"parent_id\", \"parent_label\", \"parent_definition\", \"parent_depth\",\n",
        "        \"child_id\", \"child_label\", \"child_definition\", \"child_depth\",\n",
        "        \"child_is_leaf\", \"num_ancestors_child\", \"num_descendants_child\"\n",
        "    ])\n",
        "\n",
        "    for term in mp.terms():\n",
        "        parents = list(term.superclasses(distance=1))\n",
        "\n",
        "        for parent in parents:\n",
        "            writer.writerow([\n",
        "                parent.id,\n",
        "                parent.name,\n",
        "                parent.definition or \"\",\n",
        "                get_depth(parent),\n",
        "\n",
        "                term.id,\n",
        "                term.name,\n",
        "                term.definition or \"\",\n",
        "                get_depth(term),\n",
        "\n",
        "                \"yes\" if is_leaf(term) else \"no\",\n",
        "                count_ancestors(term),\n",
        "                count_descendants(term)\n",
        "            ])\n",
        "\n",
        "\n",
        "### Download the parent-child ontologies ###\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NTW_nUlUQY9W",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Trim the ontology\n",
        "# define where the root starts and define how many descendents to include\n",
        "\n",
        "# crop network for only behavior and 4 ancestors (abnormal behavior is 2 down)\n",
        "# Load df\n",
        "df = pd.read_csv(\"ontology_edges_with_metadata.csv\")\n",
        "\n",
        "# Define root\n",
        "# mammalian phenotype: MP:0000001\n",
        "#root_id = 'MP:0000001'\n",
        "# abnormal behavior: MP:0004924\n",
        "root_id = 'MP:0004924'\n",
        "trim_root = mp[root_id]\n",
        "\n",
        "# Define the level of ancestries\n",
        "# 36 is the max level of ancestors a child can have (for all inclusion set to 36)\n",
        "print(df['num_ancestors_child'].max())\n",
        "anc_level = 36\n",
        "\n",
        "# Collect descendants (all depths)\n",
        "trimmed_descendants = {trim_root.id}\n",
        "trimmed_descendants.update([t.id for t in trim_root.subclasses()])\n",
        "\n",
        "# Keep only edges where BOTH parent and child are in the behavior subtree\n",
        "df_trimmed = df[\n",
        "    df[\"parent_id\"].isin(trimmed_descendants) &\n",
        "    df[\"child_id\"].isin(trimmed_descendants)\n",
        "]\n",
        "print(df_trimmed.columns)\n",
        "\n",
        "# define levels of ancetries\n",
        "df_trimmed = df_trimmed[df_trimmed[\"num_ancestors_child\"] <= anc_level]\n",
        "\n",
        "# trim self referential edges\n",
        "df_trimmed = df_trimmed[df_trimmed[\"parent_id\"] != df_trimmed[\"child_id\"]]\n",
        "df_trimmed = df_trimmed[df_trimmed[\"parent_label\"] != df_trimmed[\"child_label\"]]\n",
        "df_trimmed = df_trimmed.drop_duplicates(subset=[\"parent_id\", \"child_id\"])\n",
        "\n",
        "\n",
        "#### Download the trimmed behavior parent-child ontologies ####\n",
        "output_file = \"ontology_edges_trimmed.csv\"\n",
        "df_trimmed.to_csv(output_file, index=False)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)"
      ],
      "metadata": {
        "id": "ARxMtpUTcnqj",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Retrieve all the leaf nodes\n",
        "# Get all leaf nodes\n",
        "df_leafs = df_trimmed.copy()\n",
        "\n",
        "# All parent IDs\n",
        "all_parents = set(df_leafs[\"parent_id\"])\n",
        "\n",
        "# All child IDs\n",
        "all_children = set(df_leafs[\"child_id\"])\n",
        "\n",
        "# Leafs = children that are never parents\n",
        "leaf_nodes = all_children - all_parents\n",
        "\n",
        "# Optionally get their labels\n",
        "leaf_labels = df_leafs[df_leafs[\"child_id\"].isin(leaf_nodes)][[\"child_id\", \"child_label\", \"child_definition\"]].drop_duplicates()\n",
        "\n",
        "### Download leaf labels ###\n",
        "output_file = \"leaf_nodes.csv\"\n",
        "leaf_labels.to_csv(output_file, index=False)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)"
      ],
      "metadata": {
        "id": "Uytp5vSZDj8h",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title OPTIONAL - Find the most common words in leafs of MGI branch\n",
        "\n",
        "# Download stopwords if needed\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Combine label + definition text fields\n",
        "text_data = (\n",
        "    leaf_labels[\"child_label\"].fillna('') + \" \" +\n",
        "    leaf_labels[\"child_definition\"].fillna('')\n",
        ")\n",
        "\n",
        "# Convert to one long string\n",
        "all_text = \" \".join(text_data.tolist()).lower()\n",
        "\n",
        "# Remove punctuation and non-alpha\n",
        "all_text = re.sub(r'[^a-z\\s]', ' ', all_text)\n",
        "\n",
        "# Tokenize\n",
        "words = all_text.split()\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "words = [w for w in words if w not in stop_words and len(w) > 2]\n",
        "\n",
        "# Count frequency\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Get top 50 words\n",
        "top_words = word_counts.most_common(50)\n",
        "\n",
        "top_words"
      ],
      "metadata": {
        "cellView": "form",
        "id": "x3oKyPFCIQO_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Map Metrics onto the Ontology\n",
        "\n",
        "#### Upload defined metrics ####\n",
        "# and the info to append to the rag space\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "df_metrics = pd.read_csv(filename, encoding=\"latin1\")\n",
        "\n",
        "# remove rows where metric definitions are null\n",
        "df_metrics = df_metrics.dropna(subset=['metric_definition']).reset_index(drop=True)\n",
        "\n",
        "# Create the model by reference\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "\n",
        "# Find columns with keyword string\n",
        "pattern = \"keyword\"\n",
        "matching_columns = df_metrics.columns[df_metrics.columns.str.contains(pattern)]\n",
        "\n",
        "\n",
        "# create either null column or concatentaion of key words if exists.\n",
        "if len(matching_columns) == 0:\n",
        "    print(\"no matching columns\")\n",
        "    df_metrics[\"keywords_all\"] = \"\"\n",
        "    metric_texts = (\n",
        "        #df_metrics[\"metric_name\"] + \". \" +\n",
        "        df_metrics[\"metric_definition\"]\n",
        "    ).tolist()\n",
        "else:\n",
        "  # Merge all keyword column into 1 keyword column\n",
        "    df_metrics[\"keywords_all\"] = (\n",
        "        df_metrics[matching_columns]\n",
        "        .apply(lambda row: '. '.join(row.dropna().astype(str)), axis=1)\n",
        "    )\n",
        "    # Create text list\n",
        "    metric_texts = (\n",
        "        #df_metrics[\"metric_name\"] + \". \" +\n",
        "        #df_metrics['keywords_all'] + \". \" +\n",
        "        df_metrics[\"metric_definition\"]\n",
        "    ).tolist()\n",
        "\n",
        "\n",
        "\n",
        "onto_texts = (\n",
        "    leaf_labels[\"child_label\"] + \". \" + leaf_labels[\"child_definition\"]\n",
        ").tolist()\n",
        "\n",
        "#print(metric_texts)\n",
        "#print(df_metrics)\n",
        "\n",
        "# create the embeddings\n",
        "metric_emb = model.encode(metric_texts, convert_to_tensor=True)\n",
        "onto_emb = model.encode(onto_texts, convert_to_tensor=True)\n",
        "\n",
        "# compute the similarity matrix\n",
        "sim_matrix = util.cos_sim(metric_emb, onto_emb)\n",
        "\n",
        "\n",
        "# Extract the top matches\n",
        "# Define similarity threshold score\n",
        "threshold = 0.50\n",
        "top_k = 5\n",
        "\n",
        "filtered_matches = []\n",
        "\n",
        "\n",
        "for i, mrow in df_metrics.iterrows():\n",
        "    scores = sim_matrix[i]\n",
        "\n",
        "    # top-k candidate ontology indices\n",
        "    top_idx = scores.topk(top_k).indices.tolist()\n",
        "\n",
        "    for idx in top_idx:\n",
        "        score_val = float(scores[idx])\n",
        "\n",
        "        if score_val >= threshold:\n",
        "            filtered_matches.append({\n",
        "                \"metric_id\": mrow[\"metric_id\"],\n",
        "                \"metric_name\": mrow[\"metric_name\"],\n",
        "                \"metric_keywords\": mrow[\"keywords_all\"],\n",
        "                \"metric_definition\": mrow[\"metric_definition\"],\n",
        "                \"ontology_id\": leaf_labels[\"child_id\"].iloc[idx],\n",
        "                \"ontology_term\": leaf_labels[\"child_label\"].iloc[idx],\n",
        "                \"ontology_definition\": leaf_labels[\"child_definition\"].iloc[idx],\n",
        "                \"similarity\": score_val\n",
        "            })\n",
        "\n",
        "filtered_df = pd.DataFrame(filtered_matches)\n",
        "filtered_df.head()\n",
        "\n",
        "\n",
        "#### Download mapped metrics ####\n",
        "output_file = \"onto_metrics_mapped.csv\"\n",
        "filtered_df.to_csv(output_file, index=False)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n"
      ],
      "metadata": {
        "id": "wEtcCHAYKzqE",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Retrival matching 1 - Map metrics onto Ontology\n",
        "\n",
        "############################ Upload defined metrics ############################\n",
        "# and the info to append to the rag space\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "behavior_name = list(uploaded.keys())[1]\n",
        "\n",
        "df_metrics = pd.read_csv(filename, encoding=\"latin1\")\n",
        "bev_info_df = pd.read_csv(behavior_name, encoding=\"latin1\")\n",
        "\n",
        "# copy the ontology\n",
        "onto_df = df_trimmed[[\n",
        "    \"child_id\",\n",
        "    \"child_label\",\n",
        "    \"child_definition\",\n",
        "    \"parent_id\",\n",
        "    \"parent_label\",\n",
        "    \"parent_definition\"\n",
        "]].copy()\n",
        "\n",
        "# remove rows where metric definitions are null\n",
        "df_metrics = df_metrics.dropna(subset=['metric_definition']).reset_index(drop=True)\n",
        "\n",
        "######################## Adopt model and raise with RAG ########################\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "rag_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Extract the text columns containing passed information\n",
        "behavior_texts = bev_info_df[\"text\"].astype(str).tolist()\n",
        "\n",
        "# pass the ontology informaiton\n",
        "onto_df[\"embed_text\"] = (\n",
        "    onto_df[\"child_label\"].astype(str)\n",
        "    + \". \"\n",
        "    + onto_df[\"child_definition\"].astype(str)\n",
        ")\n",
        "onto_texts = onto_df[\"embed_text\"].tolist()\n",
        "\n",
        "# metadata list in same order as embeddings\n",
        "onto_meta = []\n",
        "for _, row in onto_df.iterrows():\n",
        "    onto_meta.append({\n",
        "        \"child_id\": row.child_id,\n",
        "        \"child_label\": row.child_label,\n",
        "        \"child_definition\": row.child_definition,\n",
        "        \"parent_id\": row.parent_id,\n",
        "        \"parent_label\": row.parent_label,\n",
        "        \"parent_definition\": row.parent_definition,\n",
        "    })\n",
        "\n",
        "print(onto_meta)\n",
        "\n",
        "############################### Build Embeddings ###############################\n",
        "# Extract behavior knowledge (from your custom uploaded CSV)\n",
        "behavior_texts = bev_info_df[\"text\"].astype(str).tolist()\n",
        "\n",
        "# Combine behavior texts (RAG) + ontology texts (child terms)\n",
        "all_texts = behavior_texts + onto_texts\n",
        "\n",
        "# Embed all RAG knowledge + ontology nodes\n",
        "kb_emb = rag_model.encode(all_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "dimension = kb_emb.shape[1]\n",
        "\n",
        "# Build FAISS index\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(kb_emb.astype(\"float32\"))\n",
        "\n",
        "\n",
        "# Find columns with keyword string\n",
        "pattern = \"keyword\"\n",
        "matching_columns = df_metrics.columns[df_metrics.columns.str.contains(pattern)]\n",
        "\n",
        "# Merge keywords if they exist\n",
        "if len(matching_columns) == 0:\n",
        "    df_metrics[\"keywords_all\"] = \"\"\n",
        "else:\n",
        "    df_metrics[\"keywords_all\"] = (\n",
        "        df_metrics[matching_columns]\n",
        "        .apply(lambda row: \". \".join(row.dropna().astype(str)), axis=1)\n",
        "    )\n",
        "\n",
        "# Construct the metric text used for embedding\n",
        "df_metrics[\"metric_text\"] = (\n",
        "    #df_metrics[\"metric_name\"].astype(str)\n",
        "    #+ \". \" +\n",
        "    #df_metrics[\"keywords_all\"].astype(str) + \". \" +\n",
        "    df_metrics[\"metric_definition\"].astype(str)\n",
        ")\n",
        "\n",
        "metric_texts = df_metrics[\"metric_text\"].tolist()\n",
        "\n",
        "# Embed the definitions\n",
        "metric_emb = rag_model.encode(metric_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "\n",
        "#### compute the similarity matrix ####\n",
        "#sim_matrix = util.cos_sim(metric_emb, onto_emb)\n",
        "\n",
        "top_k = 5\n",
        "threshold = 0.50\n",
        "\n",
        "filtered_matches = []\n",
        "\n",
        "# ontology starts after behavior entries\n",
        "start_index_onto = len(behavior_texts)\n",
        "\n",
        "for i, mrow in df_metrics.iterrows():\n",
        "\n",
        "    q_emb = metric_emb[i:i+1]  # FAISS expects shape (1, dim)\n",
        "    D, I = index.search(q_emb.astype(\"float32\"), k=top_k)\n",
        "\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "\n",
        "        # Only keep ontology embedding to compare\n",
        "        if idx < start_index_onto:\n",
        "            continue  # skip behavior knowledge\n",
        "\n",
        "        onto_idx = idx - start_index_onto  # index into onto_df/onto_meta\n",
        "        if onto_idx < 0 or onto_idx >= len(onto_meta):\n",
        "            continue\n",
        "\n",
        "        # convert distance to similarity-like score\n",
        "        similarity = 1 / (1 + score)\n",
        "\n",
        "        if similarity >= threshold:\n",
        "            meta = onto_meta[onto_idx]\n",
        "\n",
        "            filtered_matches.append({\n",
        "                \"metric_id\": mrow[\"metric_id\"],\n",
        "                \"metric_name\": mrow[\"metric_name\"],\n",
        "                \"metric_keywords\": mrow[\"keywords_all\"],\n",
        "                \"metric_definition\": mrow[\"metric_definition\"],\n",
        "                \"ontology_child_id\": meta[\"child_id\"],\n",
        "                \"ontology_child_label\": meta[\"child_label\"],\n",
        "                \"ontology_child_definition\": meta[\"child_definition\"],\n",
        "                \"ontology_parent_id\": meta[\"parent_id\"],\n",
        "                \"ontology_parent_label\": meta[\"parent_label\"],\n",
        "                \"similarity\": similarity\n",
        "            })\n",
        "\n",
        "\n",
        "filtered_df = pd.DataFrame(filtered_matches)\n",
        "filtered_df.head()\n",
        "\n",
        "\n",
        "\n",
        "#### Download mapped metrics ####\n",
        "output_file = \"onto_metrics_mapped_rag.csv\"\n",
        "filtered_df.to_csv(output_file, index=False)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lEP8dWSyzdf9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Retrival matching 2\n",
        "######################### Upload metrics & RAG info ############################\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "behavior_name = list(uploaded.keys())[1]\n",
        "\n",
        "df_metrics = pd.read_csv(filename, encoding=\"latin1\")\n",
        "bev_info_df = pd.read_csv(behavior_name, encoding=\"latin1\")\n",
        "\n",
        "# copy ontology\n",
        "onto_df = df_trimmed[[\n",
        "    \"child_id\", \"child_label\", \"child_definition\",\n",
        "    \"parent_id\", \"parent_label\", \"parent_definition\"\n",
        "]].copy()\n",
        "\n",
        "df_metrics = df_metrics.dropna(subset=['metric_definition']).reset_index(drop=True)\n",
        "\n",
        "############################### Build RAG model ##################################\n",
        "rag_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "### Prepare ontology text ###\n",
        "onto_df[\"embed_text\"] = (\n",
        "    onto_df[\"child_label\"].astype(str)\n",
        "    + \". \" +\n",
        "    onto_df[\"child_definition\"].astype(str)\n",
        ")\n",
        "\n",
        "onto_texts = onto_df[\"embed_text\"].tolist()\n",
        "\n",
        "### Build ontology metadata ###\n",
        "onto_meta = onto_df.to_dict(\"records\")\n",
        "\n",
        "### Encode ontology ###\n",
        "onto_emb = rag_model.encode(\n",
        "    onto_texts, convert_to_numpy=True, normalize_embeddings=True\n",
        ")\n",
        "\n",
        "### Build FAISS index for ontology ONLY ###\n",
        "dimension = onto_emb.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(onto_emb.astype(\"float32\"))\n",
        "\n",
        "\n",
        "########################### Build metric embedding #############################\n",
        "\n",
        "# Merge keywords if present\n",
        "pattern = \"keyword\"\n",
        "matching_columns = df_metrics.columns[df_metrics.columns.str.contains(pattern)]\n",
        "\n",
        "if len(matching_columns) == 0:\n",
        "    df_metrics[\"keywords_all\"] = \"\"\n",
        "else:\n",
        "    df_metrics[\"keywords_all\"] = (\n",
        "        df_metrics[matching_columns]\n",
        "            .apply(lambda row: \". \".join(row.dropna().astype(str)), axis=1)\n",
        "    )\n",
        "\n",
        "# df_metrics[\"metric_text\"] = df_metrics[\"metric_definition\"].astype(str)\n",
        "# Construct the metric text used for embedding\n",
        "df_metrics[\"metric_text\"] = (\n",
        "    # df_metrics[\"metric_name\"].astype(str)\n",
        "    # + \". \" +\n",
        "    # df_metrics[\"keywords_all\"].astype(str) + \". \" +\n",
        "    df_metrics[\"metric_definition\"].astype(str)\n",
        ")\n",
        "\n",
        "metric_texts = df_metrics[\"metric_text\"].tolist()\n",
        "\n",
        "metric_emb = rag_model.encode(\n",
        "    metric_texts, convert_to_numpy=True, normalize_embeddings=True\n",
        ")\n",
        "\n",
        "############################## Matching loop ###################################\n",
        "top_k = 5\n",
        "threshold = 0.50\n",
        "filtered_matches = []\n",
        "\n",
        "for i, mrow in df_metrics.iterrows():\n",
        "    q_emb = metric_emb[i:i+1]  # shape (1, dim)\n",
        "    D, I = index.search(q_emb.astype(\"float32\"), k=top_k)\n",
        "\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "        # convert L2 distance to cosine similarity\n",
        "        similarity = 1 - (score / 2)\n",
        "\n",
        "        if similarity < threshold:\n",
        "            continue\n",
        "\n",
        "        meta = onto_meta[idx]\n",
        "\n",
        "        filtered_matches.append({\n",
        "            \"metric_id\": mrow[\"metric_id\"],\n",
        "            \"metric_name\": mrow[\"metric_name\"],\n",
        "            \"metric_keywords\": mrow[\"keywords_all\"],\n",
        "            \"metric_definition\": mrow[\"metric_definition\"],\n",
        "            \"ontology_child_id\": meta[\"child_id\"],\n",
        "            \"ontology_child_label\": meta[\"child_label\"],\n",
        "            \"ontology_child_definition\": meta[\"child_definition\"],\n",
        "            \"ontology_parent_id\": meta[\"parent_id\"],\n",
        "            \"ontology_parent_label\": meta[\"parent_label\"],\n",
        "            \"similarity\": similarity\n",
        "        })\n",
        "\n",
        "filtered_df = pd.DataFrame(filtered_matches)\n",
        "filtered_df.head()\n",
        "\n",
        "\n",
        "#### Download mapped metrics ####\n",
        "output_file = \"onto_metrics_mapped_rag.csv\"\n",
        "filtered_df.to_csv(output_file, index=False)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n"
      ],
      "metadata": {
        "id": "6aSjkZTPcJEA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Retrival Matching 3\n",
        "# inject embeddings that describe attributes\n",
        "from google.colab import files\n",
        "###################### Upload metrics & injectable info ########################\n",
        "\"\"\"uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "behavior_name = list(uploaded.keys())[1]\n",
        "\n",
        "df_metrics = pd.read_csv(filename, encoding=\"latin1\")\n",
        "bev_info_df = pd.read_csv(behavior_name, encoding=\"latin1\")\"\"\"\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_roles = {\n",
        "    \"metrics\": [\"metric\", \"defined_metrics\"],\n",
        "    \"info\": [\"information\", \"info\"],\n",
        "    \"target\": [\"target\", \"targets\"]\n",
        "}\n",
        "identified = {k: [] for k in file_roles}\n",
        "\n",
        "# classify files\n",
        "for fname in uploaded.keys():\n",
        "    lname = fname.lower()\n",
        "    for role, patterns in file_roles.items():\n",
        "        if any(p in lname for p in patterns):\n",
        "            identified[role].append(fname)\n",
        "\n",
        "# validate\n",
        "for role, files in identified.items():\n",
        "    if len(files) != 1:\n",
        "        raise ValueError(\n",
        "            f\"Expected exactly 1 file for '{role}', found {len(files)}: {files}\"\n",
        "        )\n",
        "\n",
        "# load\n",
        "df_metrics = pd.read_csv(identified[\"metrics\"][0], encoding=\"latin1\")\n",
        "bev_info_df = pd.read_csv(identified[\"info\"][0], encoding=\"latin1\")\n",
        "target_df = pd.read_csv(identified[\"target\"][0], encoding=\"latin1\")\n",
        "\n",
        "\n",
        "############## Read in the LLM weights for the sentence transformer ############\n",
        "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "##################### Define ontology texts for embedding ######################\n",
        "# Parent nodes\n",
        "parents = df_trimmed[[\n",
        "    \"parent_id\",\n",
        "    \"parent_label\",\n",
        "    \"parent_definition\"\n",
        "]].rename(columns={\n",
        "    \"parent_id\": \"node_id\",\n",
        "    \"parent_label\": \"label\",\n",
        "    \"parent_definition\": \"definition\"\n",
        "})\n",
        "\n",
        "# Child nodes\n",
        "children = df_trimmed[[\n",
        "    \"child_id\",\n",
        "    \"child_label\",\n",
        "    \"child_definition\"\n",
        "]].rename(columns={\n",
        "    \"child_id\": \"node_id\",\n",
        "    \"child_label\": \"label\",\n",
        "    \"child_definition\": \"definition\"\n",
        "})\n",
        "\n",
        "# Combine and remove duplicates to get a list of all unique nodes\n",
        "onto_nodes = (\n",
        "    pd.concat([parents, children], ignore_index=True)\n",
        "      .drop_duplicates(subset=[\"node_id\"])\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# create list of parent child node ID's\n",
        "onto_edges = df_trimmed[[\n",
        "    \"parent_id\",\n",
        "    \"child_id\"\n",
        "]].drop_duplicates()\n",
        "\n",
        "# Create column to embed using the label and the definition (for now)\n",
        "onto_nodes[\"embed_text\"] = (\n",
        "    onto_nodes[\"label\"].astype(str) + \". \" +\n",
        "    onto_nodes[\"definition\"].astype(str)\n",
        ")\n",
        "\n",
        "# Retrieve the texts as list and create embeddings\n",
        "onto_texts = onto_nodes[\"embed_text\"].tolist()\n",
        "onto_emb = embedding_model.encode(\n",
        "    onto_texts,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "# build a dictionary for the nodes\n",
        "# this is aligned with embeddings\n",
        "onto_meta = []\n",
        "for _, row in onto_nodes.iterrows():\n",
        "    onto_meta.append({\n",
        "        \"node_id\": row.node_id,\n",
        "        \"label\": row.label,\n",
        "        \"definition\": row.definition\n",
        "    })\n",
        "\n",
        "\n",
        "######## inject behavior information and ontology nodes into embeddings ########\n",
        "# create behavior texts\n",
        "behavior_texts = bev_info_df[\"text\"].tolist()\n",
        "\n",
        "all_texts = behavior_texts + onto_nodes[\"embed_text\"].tolist()\n",
        "\n",
        "# create the knowledge base embeddings\n",
        "kb_emb = embedding_model.encode(\n",
        "    all_texts,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True\n",
        "    )\n",
        "\n",
        "index = faiss.IndexFlatL2(kb_emb.shape[1])\n",
        "index.add(kb_emb.astype(\"float32\"))\n",
        "# find where the ontology embeddings start\n",
        "ontology_start = len(behavior_texts)\n",
        "\n",
        "\n",
        "##################### Build the embeddings for the metrics #####################\n",
        "# Clean up the dataframe\n",
        "df_metrics = df_metrics.dropna(subset=['metric_definition', \"metric_interpretation\"]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Merge keywords if present\n",
        "pattern = \"keyword\"\n",
        "matching_columns = df_metrics.columns[df_metrics.columns.str.contains(pattern)]\n",
        "\n",
        "if len(matching_columns) == 0:\n",
        "    df_metrics[\"keywords_all\"] = \"\"\n",
        "else:\n",
        "    df_metrics[\"keywords_all\"] = (\n",
        "        df_metrics[matching_columns]\n",
        "            .apply(lambda row: \". \".join(row.dropna().astype(str)), axis=1)\n",
        "    )\n",
        "\n",
        "# df_metrics[\"metric_text\"] = df_metrics[\"metric_definition\"].astype(str)\n",
        "# Construct the metric text used for embedding\n",
        "df_metrics[\"metric_text\"] = (\n",
        "    # df_metrics[\"metric_name\"].astype(str)\n",
        "    # + \". \" +\n",
        "    # df_metrics[\"keywords_all\"].astype(str) + \". \" +\n",
        "    df_metrics[\"metric_definition\"].astype(str)\n",
        "    + \": \" + df_metrics[\"metric_interpretation\"].astype(str)\n",
        ")\n",
        "\n",
        "\n",
        "metric_texts = df_metrics[\"metric_text\"].tolist()\n",
        "print(metric_texts)\n",
        "\n",
        "metric_emb = embedding_model.encode(\n",
        "    metric_texts,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "################## Iterate to compare metrics and ontologies ###################\n",
        "# define the top results\n",
        "top_k = 8\n",
        "threshold = 0.50\n",
        "results = []\n",
        "\n",
        "# iterate through the metrics and map to ontology\n",
        "for i, mrow in df_metrics.iterrows():\n",
        "\n",
        "    q_emb = metric_emb[i:i+1]\n",
        "    D, I = index.search(q_emb.astype(\"float32\"), top_k)\n",
        "\n",
        "    for dist, idx in zip(D[0], I[0]):\n",
        "\n",
        "        if idx < ontology_start:\n",
        "            continue\n",
        "\n",
        "        node_idx = idx - ontology_start\n",
        "        similarity = 1 - (dist / 2)\n",
        "\n",
        "        if similarity >= threshold:\n",
        "            node = onto_meta[node_idx]\n",
        "\n",
        "            results.append({\n",
        "                \"metric_id\": mrow.metric_id,\n",
        "                \"metric_name\": mrow.metric_name,\n",
        "                \"metric_definition\": mrow.metric_definition,\n",
        "                \"ontology_node_id\": node[\"node_id\"],\n",
        "                \"ontology_label\": node[\"label\"],\n",
        "                \"ontology_definition\": node[\"definition\"],\n",
        "                #\"node_depth\": node[\"depth\"],\n",
        "                \"similarity\": similarity\n",
        "            })\n",
        "\n",
        "filtered_df = pd.DataFrame(results)\n",
        "filtered_df.head()\n",
        "\n",
        "\n",
        "#### Download mapped metrics ####\n",
        "output_file = \"onto_metrics_mapped_rag.csv\"\n",
        "filtered_df.to_csv(output_file, index=False)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n",
        "\n"
      ],
      "metadata": {
        "id": "jLMnDpgqlLvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title OPTIONAL - compare model setups\n",
        "\n",
        "\n",
        "# Load data\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "comp_df = pd.read_csv(filename, encoding=\"latin1\")\n",
        "\n",
        "\n",
        "# Get all unique model types\n",
        "models = comp_df['model'].unique()\n",
        "print(f\"\\nModels found: {len(models)}\")\n",
        "\n",
        "for i, model in enumerate(models, 1):\n",
        "    df_m = comp_df[comp_df['model'] == model]\n",
        "\n",
        "    count = len(df_m)\n",
        "    unique_metrics = df_m['metric_id'].nunique()\n",
        "\n",
        "    # Filter for similarity > 0.7\n",
        "    df_thr = df_m[df_m['similarity'] > 0.7]\n",
        "    count_thr = df_thr['metric_id'].count()\n",
        "\n",
        "    print(f\"  {i}. {model}: {count} matches, {unique_metrics} unique metrics\")\n",
        "    print(f\"     >0.7 similarity: {count_thr} metrics\")\n",
        "\n",
        "# Create dictionary of dataframes for each model\n",
        "model_dfs = {model: comp_df[comp_df['model'] == model].copy() for model in models}\n",
        "\n",
        "#### overlap analysis ####\n",
        "# Create overlap matrix\n",
        "overlap_matrix = pd.DataFrame(index=models, columns=models, dtype=float)\n",
        "shared_pairs_dict = {}\n",
        "\n",
        "\n",
        "\n",
        "#### plots ####\n",
        "n_models = len(models)\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Plot 1: Similarity distributions for all models\n",
        "\"\"\"ax1 = plt.subplot(2, 2, 1)\n",
        "for model in models:\n",
        "    ax1.hist(model_dfs[model]['similarity'], bins=30, alpha=0.5, label=model)\n",
        "ax1.set_xlabel('Similarity Score')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Similarity Score Distribution')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "ax2 = plt.subplot(2, 2, 2)\n",
        "data_to_plot = [model_dfs[model]['similarity'] for model in models]\n",
        "ax2.boxplot(data_to_plot, labels=models)\n",
        "ax2.set_ylabel('Similarity Score')\n",
        "ax2.set_title('Similarity Score Comparison')\n",
        "ax2.set_xticklabels(models, rotation=45, ha='right')\n",
        "ax2.grid(alpha=0.3)\"\"\"\n",
        "\n",
        "ax3 = plt.subplot(2, 2, 3)\n",
        "avg_sims = [model_dfs[model]['similarity'].mean() for model in models]\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, n_models))\n",
        "ax3.bar(range(n_models), avg_sims, color=colors)\n",
        "ax3.set_xticks(range(n_models))\n",
        "ax3.set_xticklabels(models, rotation=45, ha='right')\n",
        "ax3.set_ylabel('Average Similarity')\n",
        "ax3.set_title('Average Similarity by Model')\n",
        "ax3.grid(alpha=0.3, axis='y')\n",
        "\n",
        "ax4 = plt.subplot(2, 2, 4)\n",
        "match_counts = [len(model_dfs[model]) for model in models]\n",
        "ax4.bar(range(n_models), match_counts, color=colors)\n",
        "ax4.set_xticks(range(n_models))\n",
        "ax4.set_xticklabels(models, rotation=45, ha='right')\n",
        "ax4.set_ylabel('Number of Matches')\n",
        "ax4.set_title('Total Matches by Model')\n",
        "ax4.grid(alpha=0.3, axis='y')\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rnzppHPmi6VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create WORK IN PROGRESS a network graph for the trimmed tree above\n",
        "\n",
        "df_network = df_trimmed.copy()\n",
        "df_mapped = filtered_df.copy()\n",
        "\n",
        "df_mapped = df_mapped.rename(columns={'ontology_node_id': 'child_id',\n",
        "                                      'ontology_label': 'ontology_term' })\n",
        "#df_mapped = df_mapped.rename(columns={'ontology_child_id': 'child_id',\n",
        "#                                      'ontology_child_label': 'ontology_term',\n",
        "#                                      'ontology_child_definition': 'ontology_definition'})\n",
        "df_network = pd.merge(df_network, df_mapped, on='child_id', how='left')\n",
        "\n",
        "\n",
        "#### Build directed graph ####\n",
        "G = nx.DiGraph()\n",
        "\n",
        "for _, row in df_network.iterrows():\n",
        "    parent = row[\"parent_id\"]\n",
        "    child = row[\"child_id\"]\n",
        "\n",
        "    if parent != child:\n",
        "        G.add_edge(parent, child)\n",
        "\n",
        "    # Node attributes\n",
        "    G.nodes[parent]['label'] = row['parent_label']\n",
        "    G.nodes[parent]['title'] = f\"{row['parent_label']} ({parent})\\n{row['parent_definition'] or ''}\"\n",
        "\n",
        "    G.nodes[child]['label'] = row['child_label']\n",
        "    G.nodes[child]['title'] = f\"{row['child_label']} ({child})\\n{row['child_definition'] or ''}\"\n",
        "\n",
        "    # Add metrics\n",
        "    if 'metrics' not in G.nodes[child]:\n",
        "        G.nodes[child]['metrics'] = []\n",
        "    if pd.notna(row.get(\"metric_name\")):\n",
        "        G.nodes[child]['metrics'].append(\n",
        "            f\"{row['metric_name']}: {row['metric_definition'] or ''}\"\n",
        "        )\n",
        "\n",
        "#### Remove self-loops ####\n",
        "G.remove_edges_from(list(nx.selfloop_edges(G)))\n",
        "\n",
        "\n",
        "#### Compute descendants for sizing ####\n",
        "descendant_counts = {n: len(nx.descendants(G, n)) for n in G.nodes()}\n",
        "\n",
        "\n",
        "#### Assign unique colors to branches ####\n",
        "direct_children = list(G.successors(root_id))\n",
        "num_branches = len(direct_children)\n",
        "cmap = cm.get_cmap('tab20', num_branches)\n",
        "branch_colors = {child: mcolors.to_hex(cmap(i)) for i, child in enumerate(direct_children)}\n",
        "\n",
        "#### Propagate branch color to all nodes ####\n",
        "node_colors = {}\n",
        "for branch_root, color in branch_colors.items():\n",
        "    nodes_in_branch = nx.descendants(G, branch_root)\n",
        "    nodes_in_branch.add(branch_root)\n",
        "    for n in nodes_in_branch:\n",
        "        node_colors[n] = color\n",
        "\n",
        "\n",
        "#### Gray out unmapped nodes and propagate upward from mapped nodes ####\n",
        "color_by_map = True\n",
        "\n",
        "if color_by_map:\n",
        "    mapped_nodes = set(df_network.loc[df_network[\"ontology_term\"].notna(), \"child_id\"])\n",
        "\n",
        "    # Gray out all unmapped nodes\n",
        "    for n in G.nodes():\n",
        "        if n not in mapped_nodes:\n",
        "            node_colors[n] = \"#D3D3D3\"  # light gray\n",
        "\n",
        "    # Color upward from mapped nodes\n",
        "    for mapped in mapped_nodes:\n",
        "        mapped_color = node_colors[mapped]\n",
        "        ancestors = nx.ancestors(G, mapped)\n",
        "        for a in ancestors:\n",
        "            if node_colors[a] == \"#D3D3D3\":\n",
        "                node_colors[a] = mapped_color\n",
        "\n",
        "# Highlight root\n",
        "node_colors[root_id] = \"#ff9999\"\n",
        "\n",
        "\n",
        "#### Compute node sizes ####\n",
        "# Should add logic to compute sizes based on number of mapped matrics\n",
        "node_sizes = {n: 10 + descendant_counts.get(n, 0) * 2 for n in G.nodes()}\n",
        "max_descendant_size = max(size for n, size in node_sizes.items() if n != root_id)\n",
        "node_sizes[root_id] = max_descendant_size + 3\n",
        "\n",
        "\n",
        "#### Prepare edge colors ####\n",
        "edge_colors = {}\n",
        "for branch_root, color in branch_colors.items():\n",
        "    if G.has_edge(root_id, branch_root):\n",
        "        edge_colors[(root_id, branch_root)] = color\n",
        "    for u, v in nx.edge_dfs(G, branch_root):\n",
        "        edge_colors[(u, v)] = color\n",
        "\n",
        "# Gray edges connecting to gray nodes\n",
        "for u, v in G.edges():\n",
        "    if node_colors[u] == \"#D3D3D3\" or node_colors[v] == \"#D3D3D3\":\n",
        "        edge_colors[(u, v)] = \"#C0C0C0\"\n",
        "\n",
        "\n",
        "#### Create PyVis network ####\n",
        "net = Network(\n",
        "    notebook=True,\n",
        "    directed=True,\n",
        "    height=\"800px\",\n",
        "    width=\"100%\",\n",
        "    cdn_resources='in_line'\n",
        ")\n",
        "\n",
        "# HTML clean function\n",
        "def clean(x):\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    return html.escape(str(x))\n",
        "\n",
        "# Add nodes with HTML tooltip\n",
        "for node, data in G.nodes(data=True):\n",
        "    # Base ontology section\n",
        "    base_title = data.get('title', '')\n",
        "\n",
        "    # Metrics section\n",
        "    metric_text = \"\"\n",
        "    if \"metrics\" in data and data[\"metrics\"]:\n",
        "        unique_metrics = sorted(set(data[\"metrics\"]))\n",
        "        metric_text = \"\\n\\nMapped Metrics:\\n\" + \"\\n\".join(unique_metrics)\n",
        "\n",
        "    title = base_title + metric_text\n",
        "\n",
        "\n",
        "    net.add_node(\n",
        "            node,\n",
        "            label=data.get(\"label\", node),\n",
        "            title=title,\n",
        "            color=node_colors.get(node, \"#66ccff\"),\n",
        "            size=node_sizes.get(node, 10)\n",
        "        )\n",
        "\n",
        "# Add edges\n",
        "for u, v in G.edges():\n",
        "    color = edge_colors.get((u, v), \"#66ccff\")\n",
        "    net.add_edge(u, v, color=color)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Download ####\n",
        "#filename = \"mgi_network.html\"\n",
        "#net.show(filename)\n",
        "#files.download(\"mgi_network.html\")\n",
        "\n",
        "filename = \"mgi_network.html\"\n",
        "\n",
        "# Step 1: Generate the raw PyVis HTML\n",
        "net.save_graph(filename)\n",
        "\n",
        "# Step 2: Read it back in and inject search bar + lookup JS\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    html_data = f.read()\n",
        "\n",
        "# Fixed lookup search bar + JS function\n",
        "lookup_js = \"\"\"\n",
        "<!-- Search Bar -->\n",
        "<div style=\"position: fixed; top: 10px; left: 10px;\n",
        "     z-index: 9999; background: rgba(255,255,255,0.95);\n",
        "     padding: 10px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.2);\">\n",
        "    <input id=\"nodeSearch\" type=\"text\"\n",
        "           placeholder=\"Search Network...\"\n",
        "           style=\"padding:6px; width:280px; border-radius:6px; border: 1px solid #ccc; font-size: 14px;\">\n",
        "    <div id=\"matchCount\" style=\"margin-top: 5px; font-size: 12px; color: #666;\"></div>\n",
        "</div>\n",
        "\n",
        "<script type=\"text/javascript\">\n",
        "var originalNodeColors = {};\n",
        "var searchNetwork = null;\n",
        "var searchTimeout = null;\n",
        "var isSearchReady = false;\n",
        "\n",
        "// Initialize after page loads\n",
        "window.addEventListener('load', function() {\n",
        "    setTimeout(initSearch, 1000);\n",
        "});\n",
        "\n",
        "function initSearch() {\n",
        "    var net = null;\n",
        "\n",
        "    // Find network object\n",
        "    if (typeof network !== 'undefined') {\n",
        "        net = network;\n",
        "    } else {\n",
        "        for (var key in window) {\n",
        "            if (key.indexOf('network') === 0 && window[key] && window[key].body) {\n",
        "                net = window[key];\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (!net || !net.body || !net.body.data || !net.body.data.nodes) {\n",
        "        setTimeout(initSearch, 500);\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    searchNetwork = net;\n",
        "\n",
        "    // Wait for stabilization before backing up colors\n",
        "    net.once('stabilizationIterationsDone', function() {\n",
        "        try {\n",
        "            var nodesDataset = net.body.data.nodes;\n",
        "            var allIds = nodesDataset.getIds();\n",
        "\n",
        "            allIds.forEach(function(id) {\n",
        "                var node = nodesDataset.get(id);\n",
        "                if (node.color) {\n",
        "                    if (typeof node.color === 'string') {\n",
        "                        originalNodeColors[id] = node.color;\n",
        "                    } else if (node.color.background) {\n",
        "                        originalNodeColors[id] = node.color.background;\n",
        "                    } else {\n",
        "                        originalNodeColors[id] = '#97C2FC';\n",
        "                    }\n",
        "                } else {\n",
        "                    originalNodeColors[id] = '#97C2FC';\n",
        "                }\n",
        "            });\n",
        "\n",
        "            isSearchReady = true;\n",
        "            console.log('Search ready with ' + allIds.length + ' nodes');\n",
        "\n",
        "            // Attach search handler\n",
        "            document.getElementById('nodeSearch').addEventListener('input', function() {\n",
        "                clearTimeout(searchTimeout);\n",
        "                searchTimeout = setTimeout(lookupNode, 300);\n",
        "            });\n",
        "\n",
        "        } catch(e) {\n",
        "            console.error('Error backing up colors:', e);\n",
        "        }\n",
        "    });\n",
        "\n",
        "    // Fallback if stabilization never fires\n",
        "    setTimeout(function() {\n",
        "        if (!isSearchReady) {\n",
        "            try {\n",
        "                var nodesDataset = net.body.data.nodes;\n",
        "                var allIds = nodesDataset.getIds();\n",
        "\n",
        "                allIds.forEach(function(id) {\n",
        "                    var node = nodesDataset.get(id);\n",
        "                    originalNodeColors[id] = (node.color && node.color.background) ? node.color.background :\n",
        "                                            (typeof node.color === 'string' ? node.color : '#97C2FC');\n",
        "                });\n",
        "\n",
        "                isSearchReady = true;\n",
        "                console.log('Search ready (fallback) with ' + allIds.length + ' nodes');\n",
        "\n",
        "                document.getElementById('nodeSearch').addEventListener('input', function() {\n",
        "                    clearTimeout(searchTimeout);\n",
        "                    searchTimeout = setTimeout(lookupNode, 300);\n",
        "                });\n",
        "            } catch(e) {\n",
        "                console.error('Fallback error:', e);\n",
        "            }\n",
        "        }\n",
        "    }, 3000);\n",
        "}\n",
        "\n",
        "function lookupNode() {\n",
        "    if (!isSearchReady || !searchNetwork) {\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    try {\n",
        "        var query = document.getElementById('nodeSearch').value.toLowerCase().trim();\n",
        "        var nodesDataset = searchNetwork.body.data.nodes;\n",
        "        var allIds = nodesDataset.getIds();\n",
        "        var matchedIds = [];\n",
        "\n",
        "        // Clear search - restore all colors\n",
        "        if (query === '') {\n",
        "            var updates = [];\n",
        "            allIds.forEach(function(id) {\n",
        "                updates.push({\n",
        "                    id: id,\n",
        "                    color: originalNodeColors[id]\n",
        "                });\n",
        "            });\n",
        "            nodesDataset.update(updates);\n",
        "            document.getElementById('matchCount').textContent = '';\n",
        "            searchNetwork.unselectAll();\n",
        "            return;\n",
        "        }\n",
        "\n",
        "        // Search all nodes for partial matches\n",
        "        var nodesWithMetrics = [];\n",
        "        var nodesWithoutMetrics = [];\n",
        "\n",
        "        allIds.forEach(function(id) {\n",
        "            try {\n",
        "                var node = nodesDataset.get(id);\n",
        "                var label = (node.label || '').toLowerCase();\n",
        "                var title = (node.title || '').toLowerCase();\n",
        "\n",
        "                // Check if query matches anywhere in label or title\n",
        "                if (label.indexOf(query) !== -1 || title.indexOf(query) !== -1) {\n",
        "                    matchedIds.push(id);\n",
        "\n",
        "                    // Check if this node has mapped metrics\n",
        "                    if (title.indexOf('mapped metrics:') !== -1) {\n",
        "                        nodesWithMetrics.push(id);\n",
        "                    } else {\n",
        "                        nodesWithoutMetrics.push(id);\n",
        "                    }\n",
        "                }\n",
        "            } catch(e) {\n",
        "                // Skip nodes that cause errors\n",
        "            }\n",
        "        });\n",
        "\n",
        "        // Batch update colors: red for nodes with metrics, yellow for nodes without metrics\n",
        "        var updates = [];\n",
        "        allIds.forEach(function(id) {\n",
        "            var color;\n",
        "            if (nodesWithMetrics.indexOf(id) !== -1) {\n",
        "                color = '#FF0000';  // Red for nodes with mapped metrics\n",
        "            } else if (nodesWithoutMetrics.indexOf(id) !== -1) {\n",
        "                color = '#FFFF00';  // Yellow for nodes without mapped metrics\n",
        "            } else {\n",
        "                color = '#DDDDDD';  // Gray for non-matches\n",
        "            }\n",
        "            updates.push({\n",
        "                id: id,\n",
        "                color: color\n",
        "            });\n",
        "        });\n",
        "        nodesDataset.update(updates);\n",
        "\n",
        "        // Update match count\n",
        "        var countText = matchedIds.length === 0 ? 'No matches found' :\n",
        "                        matchedIds.length === 1 ? '1 node found' :\n",
        "                        matchedIds.length + ' nodes found';\n",
        "        document.getElementById('matchCount').textContent = countText;\n",
        "\n",
        "        // Select matched nodes\n",
        "        if (matchedIds.length > 0) {\n",
        "            searchNetwork.selectNodes(matchedIds);\n",
        "        } else {\n",
        "            searchNetwork.unselectAll();\n",
        "        }\n",
        "\n",
        "    } catch(e) {\n",
        "        console.error('Search error:', e);\n",
        "    }\n",
        "}\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Step 3: Insert the search bar before </body>\n",
        "html_data = html_data.replace(\"</body>\", lookup_js + \"\\n</body>\")\n",
        "\n",
        "# Step 4: Write out modified HTML\n",
        "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html_data)\n",
        "\n",
        "# Step 5: Download final file\n",
        "files.download(filename)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xbsQ3tGZNi--",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}