{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLbnk3hSJ2RGWWjfTrCo0E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/FED3Analyses/blob/PCA_analysis/mgi_heiarchy_create.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install owlready2\n",
        "!pip install --upgrade jinja2 pyvis\n",
        "!pip install pygraphviz\n",
        "!pip install pronto\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tSftAIGCb0Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install and Import libraries\n",
        "\n",
        "#!pip install pronto\n",
        "#!pip install pyvis\n",
        "#!pip install sentence-transformers\n",
        "\n",
        "import os, re, zipfile\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import csv\n",
        "import pronto\n",
        "import pyvis\n",
        "import ipywidgets as widgets\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "from google.colab import files\n",
        "import networkx as nx\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as mcolors\n",
        "import csv\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import html\n",
        "import faiss"
      ],
      "metadata": {
        "id": "vDQuWFjhOM6Z",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Upload Ontology File\n",
        "\n",
        "\n",
        "#### Upload ontology file ####\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "#### Load ontology ####\n",
        "mp = pronto.Ontology(filename)\n",
        "\n",
        "\n",
        "# Identify roots (should only have 1 root for MGI)\n",
        "roots = [t for t in mp.terms() if not list(t.superclasses(distance=1))]\n",
        "\n",
        "\n",
        "# Depth function to calculate the minimum distance to the root\n",
        "def get_depth(term):\n",
        "    distances = []\n",
        "    for root in roots:\n",
        "        d = term.distance_from(root)\n",
        "        if d is not None:\n",
        "            distances.append(d)\n",
        "    return min(distances) if distances else 0\n",
        "\n",
        "# Calculate ancestors and descendents\n",
        "def count_ancestors(term):\n",
        "    return len(list(term.superclasses())) - 1  # subtract itself\n",
        "\n",
        "def count_descendants(term):\n",
        "    return len(list(term.subclasses())) - 1  # subtract itself\n",
        "\n",
        "def is_leaf(term):\n",
        "    return len(list(term.subclasses(distance=1))) == 0\n",
        "\n",
        "\n",
        "#### Output CSV ####\n",
        "# this is all nodes for the entire ontology\n",
        "output_file = \"ontology_edges_with_metadata.csv\"\n",
        "\n",
        "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\n",
        "        \"parent_id\", \"parent_label\", \"parent_definition\", \"parent_depth\",\n",
        "        \"child_id\", \"child_label\", \"child_definition\", \"child_depth\",\n",
        "        \"child_is_leaf\", \"num_ancestors_child\", \"num_descendants_child\"\n",
        "    ])\n",
        "\n",
        "    for term in mp.terms():\n",
        "        parents = list(term.superclasses(distance=1))\n",
        "\n",
        "        for parent in parents:\n",
        "            writer.writerow([\n",
        "                parent.id,\n",
        "                parent.name,\n",
        "                parent.definition or \"\",\n",
        "                get_depth(parent),\n",
        "\n",
        "                term.id,\n",
        "                term.name,\n",
        "                term.definition or \"\",\n",
        "                get_depth(term),\n",
        "\n",
        "                \"yes\" if is_leaf(term) else \"no\",\n",
        "                count_ancestors(term),\n",
        "                count_descendants(term)\n",
        "            ])\n",
        "\n",
        "\n",
        "### Download the parent-child ontologies ###\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NTW_nUlUQY9W",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Trim the ontology\n",
        "# define where the root starts and define how many descendents to include\n",
        "\n",
        "# crop network for only behavior and 4 ancestors (abnormal behavior is 2 down)\n",
        "# Load df\n",
        "df = pd.read_csv(\"ontology_edges_with_metadata.csv\")\n",
        "\n",
        "# Define root\n",
        "# mammalian phenotype: MP:0000001\n",
        "#root_id = 'MP:0000001'\n",
        "# abnormal behavior: MP:0004924\n",
        "root_id = 'MP:0004924'\n",
        "trim_root = mp[root_id]\n",
        "\n",
        "# Define the level of ancestries\n",
        "# 36 is the max level of ancestors a child can have (for all inclusion set to 36)\n",
        "print(df['num_ancestors_child'].max())\n",
        "anc_level = 36\n",
        "\n",
        "# Collect descendants (all depths)\n",
        "trimmed_descendants = {trim_root.id}\n",
        "trimmed_descendants.update([t.id for t in trim_root.subclasses()])\n",
        "\n",
        "# Keep only edges where BOTH parent and child are in the behavior subtree\n",
        "df_trimmed = df[\n",
        "    df[\"parent_id\"].isin(trimmed_descendants) &\n",
        "    df[\"child_id\"].isin(trimmed_descendants)\n",
        "]\n",
        "print(df_trimmed.columns)\n",
        "\n",
        "# define levels of ancetries\n",
        "df_trimmed = df_trimmed[df_trimmed[\"num_ancestors_child\"] <= anc_level]\n",
        "\n",
        "# trim self referential edges\n",
        "df_trimmed = df_trimmed[df_trimmed[\"parent_id\"] != df_trimmed[\"child_id\"]]\n",
        "df_trimmed = df_trimmed[df_trimmed[\"parent_label\"] != df_trimmed[\"child_label\"]]\n",
        "df_trimmed = df_trimmed.drop_duplicates(subset=[\"parent_id\", \"child_id\"])\n",
        "\n",
        "\n",
        "#### Download the trimmed behavior parent-child ontologies ####\n",
        "output_file = \"ontology_edges_trimmed.csv\"\n",
        "df_trimmed.to_csv(output_file, index=False)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)"
      ],
      "metadata": {
        "id": "ARxMtpUTcnqj",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Retrieve all the leaf nodes\n",
        "# Get all leaf nodes\n",
        "df_leafs = df_trimmed.copy()\n",
        "\n",
        "# All parent IDs\n",
        "all_parents = set(df_leafs[\"parent_id\"])\n",
        "\n",
        "# All child IDs\n",
        "all_children = set(df_leafs[\"child_id\"])\n",
        "\n",
        "# Leafs = children that are never parents\n",
        "leaf_nodes = all_children - all_parents\n",
        "\n",
        "# Optionally get their labels\n",
        "leaf_labels = df_leafs[df_leafs[\"child_id\"].isin(leaf_nodes)][[\"child_id\", \"child_label\", \"child_definition\"]].drop_duplicates()\n",
        "\n",
        "### Download leaf labels ###\n",
        "output_file = \"leaf_nodes.csv\"\n",
        "leaf_labels.to_csv(output_file, index=False)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)"
      ],
      "metadata": {
        "id": "Uytp5vSZDj8h",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Find the most common words in leafs of MGI branch\n",
        "\n",
        "# Download stopwords if needed\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Combine label + definition text fields\n",
        "text_data = (\n",
        "    leaf_labels[\"child_label\"].fillna('') + \" \" +\n",
        "    leaf_labels[\"child_definition\"].fillna('')\n",
        ")\n",
        "\n",
        "# Convert to one long string\n",
        "all_text = \" \".join(text_data.tolist()).lower()\n",
        "\n",
        "# Remove punctuation and non-alpha\n",
        "all_text = re.sub(r'[^a-z\\s]', ' ', all_text)\n",
        "\n",
        "# Tokenize\n",
        "words = all_text.split()\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "words = [w for w in words if w not in stop_words and len(w) > 2]\n",
        "\n",
        "# Count frequency\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Get top 50 words\n",
        "top_words = word_counts.most_common(50)\n",
        "\n",
        "top_words"
      ],
      "metadata": {
        "cellView": "form",
        "id": "x3oKyPFCIQO_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Map Metrics onto the Ontology\n",
        "\n",
        "#### Upload defined metrics ####\n",
        "# and the info to append to the rag space\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "behavior_name = list(uploaded.keys())[1]\n",
        "df_metrics = pd.read_csv(filename, encoding=\"latin1\")\n",
        "\n",
        "# remove rows where metric definitions are null\n",
        "df_metrics = df_metrics.dropna(subset=['metric_definition']).reset_index(drop=True)\n",
        "\n",
        "# Create the model by reference\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "\n",
        "# Find columns with keyword string\n",
        "pattern = \"keyword\"\n",
        "matching_columns = df_metrics.columns[df_metrics.columns.str.contains(pattern)]\n",
        "\n",
        "\n",
        "# create either null column or concatentaion of key words if exists.\n",
        "if len(matching_columns) == 0:\n",
        "    print(\"no matching columns\")\n",
        "    df_metrics[\"keywords_all\"] = \"\"\n",
        "    metric_texts = (\n",
        "        df_metrics[\"metric_name\"] + \". \" + df_metrics[\"metric_definition\"]\n",
        "    ).tolist()\n",
        "else:\n",
        "  # Merge all keyword column into 1 keyword column\n",
        "    df_metrics[\"keywords_all\"] = (\n",
        "        df_metrics[matching_columns]\n",
        "        .apply(lambda row: '. '.join(row.dropna().astype(str)), axis=1)\n",
        "    )\n",
        "    # Create text list\n",
        "    metric_texts = (\n",
        "        df_metrics[\"metric_name\"] + \". \" + df_metrics['keywords_all'] + \". \" + df_metrics[\"metric_definition\"]\n",
        "    ).tolist()\n",
        "\n",
        "\n",
        "\n",
        "onto_texts = (\n",
        "    leaf_labels[\"child_label\"] + \". \" + leaf_labels[\"child_definition\"]\n",
        ").tolist()\n",
        "\n",
        "print(metric_texts)\n",
        "print(df_metrics)\n",
        "\n",
        "# create the embeddings\n",
        "metric_emb = model.encode(metric_texts, convert_to_tensor=True)\n",
        "onto_emb = model.encode(onto_texts, convert_to_tensor=True)\n",
        "\n",
        "# compute the similarity matrix\n",
        "sim_matrix = util.cos_sim(metric_emb, onto_emb)\n",
        "\n",
        "\n",
        "# Extract the top matches\n",
        "# Define similarity threshold score\n",
        "threshold = 0.4\n",
        "top_k = 5\n",
        "\n",
        "filtered_matches = []\n",
        "\n",
        "\n",
        "\n",
        "for i, mrow in df_metrics.iterrows():\n",
        "    scores = sim_matrix[i]\n",
        "\n",
        "    # top-k candidate ontology indices\n",
        "    top_idx = scores.topk(top_k).indices.tolist()\n",
        "\n",
        "    for idx in top_idx:\n",
        "        score_val = float(scores[idx])\n",
        "\n",
        "        if score_val >= threshold:\n",
        "            filtered_matches.append({\n",
        "                \"metric_id\": mrow[\"metric_id\"],\n",
        "                \"metric_name\": mrow[\"metric_name\"],\n",
        "                \"metric_keywords\": mrow[\"keywords_all\"],\n",
        "                \"metric_definition\": mrow[\"metric_definition\"],\n",
        "                \"ontology_id\": leaf_labels[\"child_id\"].iloc[idx],\n",
        "                \"ontology_term\": leaf_labels[\"child_label\"].iloc[idx],\n",
        "                \"ontology_definition\": leaf_labels[\"child_definition\"].iloc[idx],\n",
        "                \"similarity\": score_val\n",
        "            })\n",
        "\n",
        "filtered_df = pd.DataFrame(filtered_matches)\n",
        "filtered_df.head()\n",
        "\n",
        "\n",
        "\n",
        "### Download mapped metrics ###\n",
        "output_file = \"onto_metrics_mapped.csv\"\n",
        "filtered_df.to_csv(output_file, index=False)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n"
      ],
      "metadata": {
        "id": "wEtcCHAYKzqE",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Map metrics onto Ontology - Using a RAG\n",
        "# @title Map Metrics onto the Ontology\n",
        "\n",
        "############################ Upload defined metrics ############################\n",
        "# and the info to append to the rag space\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "behavior_name = list(uploaded.keys())[1]\n",
        "df_metrics = pd.read_csv(filename, encoding=\"latin1\")\n",
        "bev_info_df = pd.read_csv(behavior_name, encoding=\"latin1\")\n",
        "\n",
        "# copy the ontology\n",
        "onto_df = df_trimmed[[\n",
        "    \"child_id\",\n",
        "    \"child_label\",\n",
        "    \"child_definition\",\n",
        "    \"parent_id\",\n",
        "    \"parent_label\",\n",
        "    \"parent_definition\"\n",
        "]].copy()\n",
        "\n",
        "# remove rows where metric definitions are null\n",
        "df_metrics = df_metrics.dropna(subset=['metric_definition']).reset_index(drop=True)\n",
        "\n",
        "######################## Adopt model and raise with RAG ########################\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# model = SentenceTransformer('all-mpnet-base-v2')\n",
        "rag_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Extract the text columns containing passed information\n",
        "behavior_texts = bev_info_df[\"text\"].astype(str).tolist()\n",
        "\n",
        "# pass the ontology informaiton\n",
        "onto_df[\"embed_text\"] = (\n",
        "    onto_df[\"child_label\"].astype(str)\n",
        "    + \". \"\n",
        "    + onto_df[\"child_definition\"].astype(str)\n",
        ")\n",
        "onto_texts = onto_df[\"embed_text\"].tolist()\n",
        "\n",
        "# metadata list in same order as embeddings\n",
        "onto_meta = []\n",
        "for _, row in onto_df.iterrows():\n",
        "    onto_meta.append({\n",
        "        \"child_id\": row.child_id,\n",
        "        \"child_label\": row.child_label,\n",
        "        \"child_definition\": row.child_definition,\n",
        "        \"parent_id\": row.parent_id,\n",
        "        \"parent_label\": row.parent_label,\n",
        "        \"parent_definition\": row.parent_definition,\n",
        "    })\n",
        "\n",
        "\n",
        "############################### Build Embeddings ###############################\n",
        "# Extract behavior knowledge (from your custom uploaded CSV)\n",
        "behavior_texts = bev_info_df[\"text\"].astype(str).tolist()\n",
        "\n",
        "# Combine behavior texts (RAG) + ontology texts (child terms)\n",
        "all_texts = behavior_texts + onto_texts\n",
        "\n",
        "# Embed all RAG knowledge + ontology nodes\n",
        "kb_emb = rag_model.encode(all_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "dimension = kb_emb.shape[1]\n",
        "\n",
        "# Build FAISS index\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(kb_emb.astype(\"float32\"))\n",
        "\n",
        "\n",
        "# Find columns with keyword string\n",
        "pattern = \"keyword\"\n",
        "matching_columns = df_metrics.columns[df_metrics.columns.str.contains(pattern)]\n",
        "\n",
        "\n",
        "# Find columns that contain the word \"keyword\"\n",
        "pattern = \"keyword\"\n",
        "matching_columns = df_metrics.columns[df_metrics.columns.str.contains(pattern)]\n",
        "\n",
        "# Merge keywords if they exist\n",
        "if len(matching_columns) == 0:\n",
        "    df_metrics[\"keywords_all\"] = \"\"\n",
        "else:\n",
        "    df_metrics[\"keywords_all\"] = (\n",
        "        df_metrics[matching_columns]\n",
        "        .apply(lambda row: \". \".join(row.dropna().astype(str)), axis=1)\n",
        "    )\n",
        "\n",
        "# Construct the metric text used for embedding\n",
        "df_metrics[\"metric_text\"] = (\n",
        "    df_metrics[\"metric_name\"].astype(str)\n",
        "    + \". \"\n",
        "    + df_metrics[\"keywords_all\"].astype(str)\n",
        "    + \". \"\n",
        "    + df_metrics[\"metric_definition\"].astype(str)\n",
        ")\n",
        "\n",
        "metric_texts = df_metrics[\"metric_text\"].tolist()\n",
        "\n",
        "# Embed the definitions\n",
        "metric_emb = rag_model.encode(metric_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "\n",
        "\n",
        "# compute the similarity matrix\n",
        "#sim_matrix = util.cos_sim(metric_emb, onto_emb)\n",
        "\n",
        "top_k = 5\n",
        "threshold = 0.40\n",
        "\n",
        "filtered_matches = []\n",
        "\n",
        "# ontology starts after behavior entries\n",
        "start_index_onto = len(behavior_texts)\n",
        "\n",
        "for i, mrow in df_metrics.iterrows():\n",
        "\n",
        "    q_emb = metric_emb[i:i+1]  # FAISS expects shape (1, dim)\n",
        "    D, I = index.search(q_emb.astype(\"float32\"), k=top_k)\n",
        "\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "\n",
        "        # Only keep ontology embedding to compare\n",
        "        if idx < start_index_onto:\n",
        "            continue  # skip behavior knowledge\n",
        "\n",
        "        onto_idx = idx - start_index_onto  # index into onto_df/onto_meta\n",
        "        if onto_idx < 0 or onto_idx >= len(onto_meta):\n",
        "            continue\n",
        "\n",
        "        # convert distance to similarity-like score\n",
        "        similarity = 1 / (1 + score)\n",
        "\n",
        "        if similarity >= threshold:\n",
        "            meta = onto_meta[onto_idx]\n",
        "\n",
        "            filtered_matches.append({\n",
        "                \"metric_id\": mrow[\"metric_id\"],\n",
        "                \"metric_name\": mrow[\"metric_name\"],\n",
        "                \"metric_keywords\": mrow[\"keywords_all\"],\n",
        "                \"metric_definition\": mrow[\"metric_definition\"],\n",
        "                \"ontology_child_id\": meta[\"child_id\"],\n",
        "                \"ontology_child_label\": meta[\"child_label\"],\n",
        "                \"ontology_child_definition\": meta[\"child_definition\"],\n",
        "                \"ontology_parent_id\": meta[\"parent_id\"],\n",
        "                \"ontology_parent_label\": meta[\"parent_label\"],\n",
        "                \"similarity\": similarity\n",
        "            })\n",
        "\n",
        "\n",
        "filtered_df = pd.DataFrame(filtered_matches)\n",
        "filtered_df.head()\n",
        "\n",
        "\n",
        "\n",
        "### Download mapped metrics ###\n",
        "output_file = \"onto_metrics_mapped.csv\"\n",
        "filtered_df.to_csv(output_file, index=False)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lEP8dWSyzdf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create WORK IN PROGRESS a network graph for the trimmed tree above\n",
        "\n",
        "df_network = df_trimmed.copy()\n",
        "df_mapped = filtered_df.copy()\n",
        "\n",
        "#df_mapped = df_mapped.rename(columns={'ontology_id': 'child_id'})\n",
        "df_mapped = df_mapped.rename(columns={'ontology_child_id': 'child_id',\n",
        "                                      'ontology_child_label': 'ontology_term',\n",
        "                                      'ontology_child_definition': 'ontology_definition'})\n",
        "df_network = pd.merge(df_network, df_mapped, on='child_id', how='left')\n",
        "\n",
        "\n",
        "#### Build directed graph ####\n",
        "G = nx.DiGraph()\n",
        "\n",
        "for _, row in df_network.iterrows():\n",
        "    parent = row[\"parent_id\"]\n",
        "    child = row[\"child_id\"]\n",
        "\n",
        "    if parent != child:\n",
        "        G.add_edge(parent, child)\n",
        "\n",
        "    # Node attributes\n",
        "    G.nodes[parent]['label'] = row['parent_label']\n",
        "    G.nodes[parent]['title'] = f\"{row['parent_label']} ({parent})\\n{row['parent_definition'] or ''}\"\n",
        "\n",
        "    G.nodes[child]['label'] = row['child_label']\n",
        "    G.nodes[child]['title'] = f\"{row['child_label']} ({child})\\n{row['child_definition'] or ''}\"\n",
        "\n",
        "    # Add metrics\n",
        "    if 'metrics' not in G.nodes[child]:\n",
        "        G.nodes[child]['metrics'] = []\n",
        "    if pd.notna(row.get(\"metric_name\")):\n",
        "        G.nodes[child]['metrics'].append(\n",
        "            f\"{row['metric_name']}: {row['metric_definition'] or ''}\"\n",
        "        )\n",
        "\n",
        "#### Remove self-loops ####\n",
        "G.remove_edges_from(list(nx.selfloop_edges(G)))\n",
        "\n",
        "\n",
        "#### Compute descendants for sizing ####\n",
        "descendant_counts = {n: len(nx.descendants(G, n)) for n in G.nodes()}\n",
        "\n",
        "\n",
        "#### Assign unique colors to branches ####\n",
        "direct_children = list(G.successors(root_id))\n",
        "num_branches = len(direct_children)\n",
        "cmap = cm.get_cmap('tab20', num_branches)\n",
        "branch_colors = {child: mcolors.to_hex(cmap(i)) for i, child in enumerate(direct_children)}\n",
        "\n",
        "#### Propagate branch color to all nodes ####\n",
        "node_colors = {}\n",
        "for branch_root, color in branch_colors.items():\n",
        "    nodes_in_branch = nx.descendants(G, branch_root)\n",
        "    nodes_in_branch.add(branch_root)\n",
        "    for n in nodes_in_branch:\n",
        "        node_colors[n] = color\n",
        "\n",
        "\n",
        "#### Gray out unmapped nodes and propagate upward from mapped nodes ####\n",
        "color_by_map = True\n",
        "\n",
        "if color_by_map:\n",
        "    mapped_nodes = set(df_network.loc[df_network[\"ontology_term\"].notna(), \"child_id\"])\n",
        "\n",
        "    # Gray out all unmapped nodes\n",
        "    for n in G.nodes():\n",
        "        if n not in mapped_nodes:\n",
        "            node_colors[n] = \"#D3D3D3\"  # light gray\n",
        "\n",
        "    # Color upward from mapped nodes\n",
        "    for mapped in mapped_nodes:\n",
        "        mapped_color = node_colors[mapped]\n",
        "        ancestors = nx.ancestors(G, mapped)\n",
        "        for a in ancestors:\n",
        "            if node_colors[a] == \"#D3D3D3\":\n",
        "                node_colors[a] = mapped_color\n",
        "\n",
        "# Highlight root\n",
        "node_colors[root_id] = \"#ff9999\"\n",
        "\n",
        "\n",
        "#### Compute node sizes ####\n",
        "# Should add logic to compute sizes based on number of mapped matrics\n",
        "node_sizes = {n: 10 + descendant_counts.get(n, 0) * 2 for n in G.nodes()}\n",
        "max_descendant_size = max(size for n, size in node_sizes.items() if n != root_id)\n",
        "node_sizes[root_id] = max_descendant_size + 3\n",
        "\n",
        "\n",
        "#### Prepare edge colors ####\n",
        "edge_colors = {}\n",
        "for branch_root, color in branch_colors.items():\n",
        "    if G.has_edge(root_id, branch_root):\n",
        "        edge_colors[(root_id, branch_root)] = color\n",
        "    for u, v in nx.edge_dfs(G, branch_root):\n",
        "        edge_colors[(u, v)] = color\n",
        "\n",
        "# Gray edges connecting to gray nodes\n",
        "for u, v in G.edges():\n",
        "    if node_colors[u] == \"#D3D3D3\" or node_colors[v] == \"#D3D3D3\":\n",
        "        edge_colors[(u, v)] = \"#C0C0C0\"\n",
        "\n",
        "\n",
        "#### Create PyVis network ####\n",
        "net = Network(\n",
        "    notebook=True,\n",
        "    directed=True,\n",
        "    height=\"800px\",\n",
        "    width=\"100%\",\n",
        "    cdn_resources='in_line'\n",
        ")\n",
        "\n",
        "# HTML clean function\n",
        "def clean(x):\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    return html.escape(str(x))\n",
        "\n",
        "# Add nodes with HTML tooltip\n",
        "for node, data in G.nodes(data=True):\n",
        "    # Base ontology section\n",
        "    base_title = data.get('title', '')\n",
        "\n",
        "    # Metrics section\n",
        "    metric_text = \"\"\n",
        "    if \"metrics\" in data and data[\"metrics\"]:\n",
        "        unique_metrics = sorted(set(data[\"metrics\"]))\n",
        "        metric_text = \"\\n\\nMapped Metrics:\\n\" + \"\\n\".join(unique_metrics)\n",
        "\n",
        "    title = base_title + metric_text\n",
        "\n",
        "    net.add_node(\n",
        "            node,\n",
        "            label=data.get(\"label\", node),\n",
        "            title=title,\n",
        "            color=node_colors.get(node, \"#66ccff\"),\n",
        "            size=node_sizes.get(node, 10)\n",
        "        )\n",
        "\n",
        "# Add edges\n",
        "for u, v in G.edges():\n",
        "    color = edge_colors.get((u, v), \"#66ccff\")\n",
        "    net.add_edge(u, v, color=color)\n",
        "\n",
        "\n",
        "\n",
        "#### Download\n",
        "filename = \"mgi_network.html\"\n",
        "net.show(filename)\n",
        "files.download(\"mgi_network.html\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "xbsQ3tGZNi--"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}