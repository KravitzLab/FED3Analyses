{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/FED3Analyses/blob/PsygeneL0-to-L1-QC/PsygeneFEDL0_to_L1_QC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwD-vv_BEfjR"
      },
      "source": [
        "# This is a notebook for running FED3 quality control.\n",
        "Created by Chantelle Murrell  \n",
        "Files are renamed based on Mouse_ID in a key. Multiple files per FED and multiple FEDs per mouse ID are concatenated.   \n",
        "This was created to get files from L0 to L1 for Psygene.  \n",
        "<br>\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqe_1a1j1bQaqhOxq0VvukPqfolLRUqOdl-g&s\" width=\"300\" />\n",
        "\n",
        "Updated: 02.12.25\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Es4aBKrGByvD"
      },
      "outputs": [],
      "source": [
        "# @title Install and import libraries\n",
        "!pip install -q pingouin\n",
        "!pip -q install \"git+https://github.com/earnestt1234/fed3.git\"\n",
        "!pip install -q ipywidgets\n",
        "\n",
        "#%% import libraries and set plot parameters\n",
        "import fed3\n",
        "import fed3.plot as fplot\n",
        "import pingouin as pg\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "import io\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "from google.colab import files\n",
        "from datetime import datetime, timedelta\n",
        "import zipfile\n",
        "import tempfile\n",
        "import warnings\n",
        "import ipywidgets as widgets\n",
        "import re\n",
        "from IPython.display import display, clear_output\n",
        "warnings.filterwarnings('ignore')  # this is a bit dangerous but we'll supress all warnings\n",
        "print(\"Packages installed.\")\n",
        "\n",
        "plt.rcParams.update({'font.size': 12, 'figure.autolayout': True})\n",
        "plt.rcParams['figure.figsize'] = [6, 4]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "huZX9OmDD6mX"
      },
      "outputs": [],
      "source": [
        "# @title Import L0 files (Files straight off the FED sorted by session type)\n",
        "\n",
        "\n",
        "feds = []\n",
        "loaded_files = []\n",
        "session_types = []\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "def _coerce_fed3_time_like(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Make FED3 dataframe safe for fed3.as_aligned by:\n",
        "      - ensuring a DatetimeIndex\n",
        "      - converting elapsed/time-like columns to timedeltas\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "\n",
        "    # 1) Ensure a DatetimeIndex\n",
        "    if not isinstance(d.index, pd.DatetimeIndex):\n",
        "        # Try common columns\n",
        "        if \"MM:DD:YYYY hh:mm:ss\" in d.columns:\n",
        "            dti = pd.to_datetime(d[\"MM:DD:YYYY hh:mm:ss\"], format=\"%m:%d:%Y %H:%M:%S\", errors=\"coerce\")\n",
        "        elif \"DateTime\" in d.columns:\n",
        "            dti = pd.to_datetime(d[\"DateTime\"], errors=\"coerce\")\n",
        "        elif {\"Date\",\"Time\"}.issubset(d.columns):\n",
        "            dti = pd.to_datetime(d[\"Date\"].astype(str) + \" \" + d[\"Time\"].astype(str), errors=\"coerce\")\n",
        "        elif \"Timestamp\" in d.columns:\n",
        "            dti = pd.to_datetime(d[\"Timestamp\"], errors=\"coerce\")\n",
        "        else:\n",
        "            # Last resort: try to parse the index itself\n",
        "            dti = pd.to_datetime(d.index, errors=\"coerce\")\n",
        "\n",
        "        if dti.notna().any():\n",
        "            d.index = dti\n",
        "        else:\n",
        "            # If nothing parsed, leave index as-is; as_aligned may still work using elapsed\n",
        "            pass\n",
        "\n",
        "    # 2) Coerce elapsed/timedelta-like columns (names vary across firmwares/exports)\n",
        "    elapsed_candidates = [\n",
        "        \"Elapsed\", \"Elapsed_Time\", \"Elapsed Time\", \"ElapsedTime\",\n",
        "        \"Time_Since_Start\", \"Time Since Start\"\n",
        "    ]\n",
        "    for col in elapsed_candidates:\n",
        "        if col in d.columns:\n",
        "            # If string-like, convert to timedelta (supports \"HH:MM:SS\" or seconds)\n",
        "            if not np.issubdtype(d[col].dtype, np.timedelta64):\n",
        "                # First try \"HH:MM:SS\" style\n",
        "                td = pd.to_timedelta(d[col], errors=\"coerce\")\n",
        "                # If everything is NaT, try treating as seconds\n",
        "                if td.isna().all():\n",
        "                    with pd.option_context('mode.use_inf_as_na', True):\n",
        "                        secs = pd.to_numeric(d[col], errors=\"coerce\")\n",
        "                    td = pd.to_timedelta(secs, unit=\"s\", errors=\"coerce\")\n",
        "                d[col] = td\n",
        "\n",
        "    # 3) Also coerce any column literally named \"Time\" to timedelta if it looks like HH:MM:SS\n",
        "    if \"Time\" in d.columns and not np.issubdtype(d[\"Time\"].dtype, np.timedelta64):\n",
        "        td = pd.to_timedelta(d[\"Time\"], errors=\"coerce\")\n",
        "        # don't overwrite if that destroys legitimate clock times; only set if it parsed well\n",
        "        if td.notna().sum() >= max(5, int(0.5 * len(d))):\n",
        "            d[\"Time\"] = td\n",
        "\n",
        "    return d\n",
        "\n",
        "def extract_fed_and_date(filename):\n",
        "    match = re.match(r\"FED(\\d{3})_(\\d{6})\", filename)\n",
        "    if not match:\n",
        "        return None, None, None\n",
        "    fed_id = match.group(1)\n",
        "    raw_date = match.group(2)\n",
        "    mm, dd, yy = raw_date[:2], raw_date[2:4], raw_date[4:]\n",
        "    yyyy = \"20\" + yy if int(yy) < 50 else \"19\" + yy\n",
        "    file_date = datetime.strptime(f\"{yyyy}-{mm}-{dd}\", \"%Y-%m-%d\")\n",
        "    return fed_id, file_date.strftime(\"%Y%m%d\"), file_date\n",
        "\n",
        "for name, data in uploaded.items():\n",
        "    if name.lower().endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(io.BytesIO(data)) as zip_file:\n",
        "            for zip_info in zip_file.infolist():\n",
        "                if not zip_info.filename.lower().endswith(\".csv\"):\n",
        "                    continue\n",
        "\n",
        "                file_data = zip_file.read(zip_info)\n",
        "                if len(file_data) <= 1024:\n",
        "                    continue\n",
        "\n",
        "                with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".csv\", delete=False) as tmp_file:\n",
        "                    tmp_file.write(file_data)\n",
        "                    tmp_path = tmp_file.name\n",
        "\n",
        "                try:\n",
        "                    # Quick pass to grab session type\n",
        "                    raw_df = pd.read_csv(tmp_path)\n",
        "                    session_type = raw_df[\"Session_type\"].iloc[0] if \"Session_type\" in raw_df.columns else \"Unknown\"\n",
        "\n",
        "                    # Load via fed3\n",
        "                    df = fed3.load(tmp_path)\n",
        "                    df.name = zip_info.filename\n",
        "                    df.attrs = {\"Session_type\": session_type}\n",
        "\n",
        "                    # Coerce time-like fields\n",
        "                    df = _coerce_fed3_time_like(df)\n",
        "\n",
        "                    feds.append(df)\n",
        "                    loaded_files.append(zip_info.filename)\n",
        "                    session_types.append(session_type)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {zip_info.filename}: {e}\")\n",
        "                finally:\n",
        "                    os.remove(tmp_path)\n",
        "\n",
        "    elif name.lower().endswith(\".csv\"):\n",
        "        if len(data) <= 1024:\n",
        "            continue\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".csv\", delete=False) as tmp_file:\n",
        "            tmp_file.write(data)\n",
        "            tmp_path = tmp_file.name\n",
        "\n",
        "        try:\n",
        "            raw_df = pd.read_csv(tmp_path)\n",
        "            session_type = raw_df[\"Session_type\"].iloc[0] if \"Session_type\" in raw_df.columns else \"Unknown\"\n",
        "\n",
        "            df = fed3.load(tmp_path)\n",
        "            df.name = name\n",
        "            df.attrs = {\"Session_type\": session_type}\n",
        "\n",
        "            df = _coerce_fed3_time_like(df)\n",
        "\n",
        "            feds.append(df)\n",
        "            loaded_files.append(name)\n",
        "            session_types.append(session_type)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {name}: {e}\")\n",
        "        finally:\n",
        "            os.remove(tmp_path)\n",
        "\n",
        "# ---- Plot ----\n",
        "if feds:\n",
        "    try:\n",
        "        # Align on datetime now that time fields are coerced\n",
        "        fed3.as_aligned(feds, alignment=\"datetime\", inplace=True)\n",
        "    except Exception as e:\n",
        "        print(f\"as_aligned(datetime) failed: {e}\")\n",
        "        # Fallback: try default alignment\n",
        "        try:\n",
        "            fed3.as_aligned(feds, inplace=True)\n",
        "        except Exception as e2:\n",
        "            print(f\"as_aligned(default) also failed: {e2}\")\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    fplot.line(feds, y='pellets')\n",
        "    plt.legend().remove()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No valid files loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "d3p14ynRA_ye"
      },
      "outputs": [],
      "source": [
        "# @title Upload Key\n",
        "uploader = widgets.FileUpload(\n",
        "    accept='.xlsx',\n",
        "    multiple=False,\n",
        "    description='Upload Key'\n",
        ")\n",
        "display(uploader)\n",
        "\n",
        "uploaded_key_path = None  # Global path to use later\n",
        "\n",
        "def handle_upload(change):\n",
        "    global uploaded_key_path\n",
        "    if uploader.value:\n",
        "        uploaded = uploader.value\n",
        "        key_name = next(iter(uploaded))\n",
        "        content = uploaded[key_name]['content']\n",
        "        with open(key_name, 'wb') as f:\n",
        "            f.write(content)\n",
        "        uploaded_key_path = key_name\n",
        "        print(f\"✅ Uploaded and saved key to: {key_name}\")\n",
        "\n",
        "uploader.observe(handle_upload, names='value')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0Ig_PM55J42"
      },
      "outputs": [],
      "source": [
        "# @title Identify multiple files per animal\n",
        "# --- helpers ---\n",
        "# Uses variables you already have: loaded_files, uploaded_key_path\n",
        "# @title Identify multiple files per animal (uses FED_StartDate ±14d to disambiguate cohorts)\n",
        "\n",
        "import re, pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "# --- helpers ---\n",
        "# Uses: loaded_files, uploaded_key_path\n",
        "\n",
        "def parse_fed3_cell(val):\n",
        "    if pd.isna(val): return []\n",
        "    s = str(val)\n",
        "    s = re.sub(r\"\\b(?:and)\\b\", \",\", s, flags=re.I)\n",
        "    s = re.sub(r\"[+,/&]\", \",\", s)\n",
        "    s = re.sub(r\"[;|]\", \",\", s)\n",
        "    parts = [p.strip() for p in s.split(\",\") if p.strip()]\n",
        "    return sorted({int(m) for p in parts for m in re.findall(r\"\\d+\", p)})\n",
        "\n",
        "fed_in_name_pat = re.compile(r\"(?i)fed0*([0-9]{1,3})\")\n",
        "date_token_pat  = re.compile(r\"_(\\d{6})(?:_|\\.|$)\")  # _DDMMYY_ or _MMDDYY_\n",
        "\n",
        "def extract_fed_from_loaded_filename(fname: str):\n",
        "    m = fed_in_name_pat.search(str(fname).strip())\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def parse_6digit_ddmmyy(token: str):\n",
        "    # 040925 -> 2025-09-04 (DMY)\n",
        "    return pd.to_datetime(token, format=\"%d%m%y\", errors=\"coerce\", dayfirst=True)\n",
        "\n",
        "def parse_6digit_mmddyy(token: str):\n",
        "    # 040925 -> 2025-04-09 (MDY)\n",
        "    return pd.to_datetime(token, format=\"%m%d%y\", errors=\"coerce\", dayfirst=False)\n",
        "\n",
        "def extract_date_candidates_from_filename(fname: str):\n",
        "    \"\"\"\n",
        "    Return both DMY and MDY candidates from a 6-digit token in the filename.\n",
        "    \"\"\"\n",
        "    s = str(fname).strip()\n",
        "    m = date_token_pat.search(s) or re.search(r\"(\\d{6})\", s)\n",
        "    if not m:\n",
        "        return []\n",
        "    tok = m.group(1)\n",
        "    dmy = parse_6digit_ddmmyy(tok)\n",
        "    mdy = parse_6digit_mmddyy(tok)\n",
        "    cands = []\n",
        "    if not pd.isna(dmy): cands.append((\"DMY\", dmy))\n",
        "    if not pd.isna(mdy): cands.append((\"MDY\", mdy))\n",
        "    # de-dup if both parse to the same date (rare)\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for fmt, dt in cands:\n",
        "        k = pd.Timestamp(dt).date()\n",
        "        if k not in seen:\n",
        "            out.append((fmt, pd.Timestamp(dt)))\n",
        "            seen.add(k)\n",
        "    return out\n",
        "\n",
        "# --- Load key ---\n",
        "key = pd.read_excel(uploaded_key_path, sheet_name=0)\n",
        "if \"FED_StartDate\" not in key.columns:\n",
        "    raise ValueError(\"Key must have a 'FED_StartDate' column.\")\n",
        "\n",
        "animal_col = \"Mouse_ID\" if \"Mouse_ID\" in key.columns else next(\n",
        "    c for c in key.columns if re.search(r\"mouse\", str(c), re.I)\n",
        ")\n",
        "fed_col = \"FED3\" if \"FED3\" in key.columns else next(\n",
        "    c for c in key.columns if re.fullmatch(r\"\\s*fed\\s*_?3\\s*\", str(c), flags=re.I)\n",
        ")\n",
        "\n",
        "def normalize_animal(x):\n",
        "    if pd.isna(x): return \"Unknown\"\n",
        "    sx = str(x).strip()\n",
        "    if sx == \"\": return \"Unknown\"\n",
        "    if re.fullmatch(r\"\\d+(\\.0+)?\", sx):\n",
        "        return str(int(float(sx)))\n",
        "    return sx\n",
        "\n",
        "key = key.copy()\n",
        "key[animal_col] = key[animal_col].map(normalize_animal)\n",
        "\n",
        "# Parse FED_StartDate both ways (for safety)\n",
        "raw_start = key[\"FED_StartDate\"]\n",
        "key[\"FED_StartDate_mdy\"] = pd.to_datetime(raw_start, errors=\"coerce\", dayfirst=False)\n",
        "key[\"FED_StartDate_dmy\"] = pd.to_datetime(raw_start, errors=\"coerce\", dayfirst=True)\n",
        "\n",
        "# Explode key to (orig_idx, Animal_ID, FED3_int, FED_StartDate_* variants)\n",
        "key = key.reset_index().rename(columns={\"index\": \"orig_idx\"})\n",
        "key[\"_FED3_list\"] = key[fed_col].apply(parse_fed3_cell)\n",
        "key_exploded = (\n",
        "    key.explode(\"_FED3_list\", ignore_index=True)\n",
        "       .dropna(subset=[\"_FED3_list\"])\n",
        "       .rename(columns={\"_FED3_list\": \"FED3_int\"})\n",
        ")\n",
        "key_exploded[\"FED3_int\"] = key_exploded[\"FED3_int\"].astype(int)\n",
        "\n",
        "# --- matching helper ---\n",
        "window = timedelta(days=17)\n",
        "\n",
        "def pick_best_key_row_for_any_filedate(fed_id: int, filedate_candidates):\n",
        "    \"\"\"\n",
        "    Consider BOTH filename date parses (DMY and MDY) and BOTH key parses (DMY/MDY).\n",
        "    Choose the combination with the smallest delta within ±window.\n",
        "    Returns: dict with keys {orig_idx, Mouse_ID, FED_StartDate_used, file_fmt_used}\n",
        "    or None if no candidate within window. Also returns 'nearest' (outside window) for debug.\n",
        "    \"\"\"\n",
        "    sub = key_exploded[key_exploded[\"FED3_int\"] == int(fed_id)]\n",
        "    if sub.empty or not filedate_candidates:\n",
        "        return None, None\n",
        "\n",
        "    cols_base = [\"orig_idx\", animal_col, \"FED3_int\"]\n",
        "    dmy = sub[cols_base + [\"FED_StartDate_dmy\"]].rename(columns={\"FED_StartDate_dmy\":\"FED_StartDate_used\"}).copy()\n",
        "    dmy[\"key_fmt\"] = \"DMY\"\n",
        "    mdy = sub[cols_base + [\"FED_StartDate_mdy\"]].rename(columns={\"FED_StartDate_mdy\":\"FED_StartDate_used\"}).copy()\n",
        "    mdy[\"key_fmt\"] = \"MDY\"\n",
        "    key_cand = pd.concat([dmy, mdy], ignore_index=True).dropna(subset=[\"FED_StartDate_used\"])\n",
        "    if key_cand.empty:\n",
        "        return None, None\n",
        "\n",
        "    # Build cross-product with file candidates\n",
        "    rows = []\n",
        "    for ffmt, fdt in filedate_candidates:\n",
        "        tmp = key_cand.copy()\n",
        "        tmp[\"file_fmt\"] = ffmt\n",
        "        tmp[\"file_date\"] = fdt\n",
        "        tmp[\"abs_delta\"] = (tmp[\"file_date\"] - tmp[\"FED_StartDate_used\"]).abs()\n",
        "        rows.append(tmp)\n",
        "    all_cand = pd.concat(rows, ignore_index=True)\n",
        "\n",
        "    # For debug: the nearest even if outside window\n",
        "    nearest = all_cand.sort_values([\"abs_delta\", \"FED_StartDate_used\"], ascending=[True, False]).iloc[0]\n",
        "\n",
        "    within = all_cand[all_cand[\"abs_delta\"] <= window]\n",
        "    if within.empty:\n",
        "        return None, nearest\n",
        "\n",
        "    best = within.sort_values([\"abs_delta\", \"FED_StartDate_used\"], ascending=[True, False]).iloc[0]\n",
        "    return {\n",
        "        \"orig_idx\": int(best[\"orig_idx\"]),\n",
        "        \"Mouse_ID\": best[animal_col],\n",
        "        \"FED_StartDate_used\": pd.to_datetime(best[\"FED_StartDate_used\"]).normalize(),\n",
        "        \"file_fmt_used\": best[\"file_fmt\"]\n",
        "    }, nearest\n",
        "\n",
        "# --- Extract FED and both file-date candidates from filenames ---\n",
        "files_df = pd.DataFrame({\"file\": [str(f).strip() for f in loaded_files]})\n",
        "files_df[\"FED3\"] = files_df[\"file\"].map(extract_fed_from_loaded_filename)\n",
        "files_df = files_df.dropna(subset=[\"FED3\"]).astype({\"FED3\": int}).drop_duplicates(subset=[\"file\"])\n",
        "files_df[\"file_date_candidates\"] = files_df[\"file\"].map(extract_date_candidates_from_filename)\n",
        "\n",
        "# --- Map each file ---\n",
        "matches = []\n",
        "unmatched = []\n",
        "for _, r in files_df.iterrows():\n",
        "    fed_id = int(r[\"FED3\"])\n",
        "    cands  = r[\"file_date_candidates\"]\n",
        "    best, nearest = pick_best_key_row_for_any_filedate(fed_id, cands)\n",
        "    if best is None:\n",
        "        # For debugging, report the closer of the two filename parses to the nearest key date\n",
        "        if nearest is None:\n",
        "            unmatched.append({\n",
        "                \"file\": r[\"file\"], \"FED3\": fed_id, \"file_fmt\": None,\n",
        "                \"file_date\": None, \"nearest_key_date\": None, \"delta_days\": None\n",
        "            })\n",
        "        else:\n",
        "            unmatched.append({\n",
        "                \"file\": r[\"file\"], \"FED3\": fed_id,\n",
        "                \"file_fmt\": str(nearest[\"file_fmt\"]),\n",
        "                \"file_date\": pd.to_datetime(nearest[\"file_date\"]),\n",
        "                \"nearest_key_date\": pd.to_datetime(nearest[\"FED_StartDate_used\"]),\n",
        "                \"delta_days\": int(nearest[\"abs_delta\"].days)\n",
        "            })\n",
        "        continue\n",
        "\n",
        "    matches.append({\n",
        "        \"file\": r[\"file\"],\n",
        "        \"FED3\": fed_id,\n",
        "        \"file_fmt_used\": best[\"file_fmt_used\"],\n",
        "        \"file_date\": next(dt for fmt, dt in cands if fmt == best[\"file_fmt_used\"]),\n",
        "        \"Mouse_ID\": best[\"Mouse_ID\"],\n",
        "        \"FED_StartDate\": best[\"FED_StartDate_used\"]\n",
        "    })\n",
        "\n",
        "matched_df = pd.DataFrame(matches)\n",
        "unmatched_df = pd.DataFrame(unmatched)\n",
        "\n",
        "# --- Summaries ---\n",
        "print(\"\\nFiles grouped by Mouse_ID and matched FED_StartDate (±14 days):\")\n",
        "if matched_df.empty:\n",
        "    print(\"  None\")\n",
        "else:\n",
        "    grouped = (\n",
        "        matched_df.groupby([\"Mouse_ID\", \"FED_StartDate\"], as_index=False)\n",
        "                  .agg(n_files=(\"file\", \"nunique\"),\n",
        "                       files=(\"file\", lambda xs: sorted(set(xs))),\n",
        "                       feds=(\"FED3\", lambda xs: sorted(set(int(x) for x in xs))),\n",
        "                       file_formats=(\"file_fmt_used\", lambda xs: sorted(set(xs))))\n",
        "                  .sort_values([\"Mouse_ID\", \"FED_StartDate\"])\n",
        "                  .reset_index(drop=True)\n",
        "    )\n",
        "    for _, r in grouped.iterrows():\n",
        "        adate = pd.to_datetime(r[\"FED_StartDate\"]).strftime(\"%Y-%m-%d\")\n",
        "        print(f\"  {r['Mouse_ID']} @ {adate} -> {r['n_files']} file(s); FED(s): {r['feds']}; file fmt(s): {r['file_formats']}\")\n",
        "        for f in r[\"files\"]:\n",
        "            print(f\"     - {f}\")\n",
        "\n",
        "# Animals that appear in >1 FED_StartDate group\n",
        "print(\"\\nAnimals appearing in multiple FED_StartDate groups (possible cross-cohort reuse):\")\n",
        "if matched_df.empty:\n",
        "    print(\"  None\")\n",
        "else:\n",
        "    counts_by_animal = matched_df.groupby(\"Mouse_ID\")[\"FED_StartDate\"].nunique().reset_index(name=\"n_start_dates\")\n",
        "    multi_groups = counts_by_animal[counts_by_animal[\"n_start_dates\"] > 1][\"Mouse_ID\"].tolist()\n",
        "    if not multi_groups:\n",
        "        print(\"  None\")\n",
        "    else:\n",
        "        for aid in sorted(multi_groups, key=str):\n",
        "            sub = matched_df[matched_df[\"Mouse_ID\"] == aid]\n",
        "            for sd, sub2 in sub.groupby(\"FED_StartDate\"):\n",
        "                adate = pd.to_datetime(sd).strftime(\"%Y-%m-%d\")\n",
        "                files_list = sorted(set(sub2[\"file\"].tolist()))\n",
        "                feds_list  = sorted(set(int(x) for x in sub2[\"FED3\"].tolist()))\n",
        "                print(f\"  {aid} @ {adate} -> {len(files_list)} file(s); FED(s): {feds_list}\")\n",
        "                for f in files_list:\n",
        "                    print(f\"     - {f}\")\n",
        "\n",
        "# Unmatched files (show nearest key date to help diagnose)\n",
        "if not unmatched_df.empty:\n",
        "    print(\"\\nUnmatched files (no key FED_StartDate within ±14 days for that FED). Nearest key date shown for debugging:\")\n",
        "    for _, r in unmatched_df.iterrows():\n",
        "        fdate = \"NaT\" if pd.isna(r[\"file_date\"]) else pd.to_datetime(r[\"file_date\"]).strftime(\"%Y-%m-%d\")\n",
        "        kdate = \"NaT\" if pd.isna(r.get(\"nearest_key_date\")) else pd.to_datetime(r[\"nearest_key_date\"]).strftime(\"%Y-%m-%d\")\n",
        "        delta = \"NA\" if pd.isna(r.get(\"delta_days\")) else int(r[\"delta_days\"])\n",
        "        fmt   = r.get(\"file_fmt\") or \"?\"\n",
        "        print(f\"  FED{str(int(r['FED3'])).zfill(3)} file({fmt}) @ {fdate}; nearest key date: {kdate} (Δ={delta}d): {r['file']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Aor_B1jgYSBq"
      },
      "outputs": [],
      "source": [
        "# @title Concatenate files to one per Mouse_ID\n",
        "# ---------- filename/date helpers ----------\n",
        "if 'matched_df' not in globals() or matched_df.empty:\n",
        "    raise RuntimeError(\"matched_df is empty or undefined. Run the matching cell first.\")\n",
        "\n",
        "import os, io, re, zipfile, pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# We'll use just what we need for concatenation\n",
        "mapped = (\n",
        "    matched_df[['file', 'Mouse_ID', 'FED_StartDate']]\n",
        "    .dropna(subset=['file', 'Mouse_ID', 'FED_StartDate'])\n",
        "    .copy()\n",
        ")\n",
        "animal_col = 'Mouse_ID'  # keep consistent\n",
        "\n",
        "# ---------- filename/date helpers ----------\n",
        "fed_date_pat = re.compile(r\"(?i)^.*?\\bFED0*(\\d{1,3})_(\\d{6})\")\n",
        "\n",
        "def parse_fed_and_date_from_name(fname: str):\n",
        "    \"\"\"\n",
        "    Return (fed_int, date_str_MMDDYY, datetime_obj) from names like 'FED072_080425_00.CSV'.\n",
        "    \"\"\"\n",
        "    m = fed_date_pat.search(str(fname).strip())\n",
        "    if not m:\n",
        "        return None, None, None\n",
        "    fed = int(m.group(1))\n",
        "    raw = m.group(2)  # MMDDYY\n",
        "    mm, dd, yy = raw[:2], raw[2:4], raw[4:]\n",
        "    yyyy = (\"20\" + yy) if int(yy) < 50 else (\"19\" + yy)\n",
        "    dt = datetime.strptime(f\"{yyyy}-{mm}-{dd}\", \"%Y-%m-%d\")\n",
        "    return fed, raw, dt  # return raw (MMDDYY) for naming\n",
        "\n",
        "# ---------- discover zips from multiple sources ----------\n",
        "zip_bytes_names = []\n",
        "if 'uploaded' in globals() and isinstance(uploaded, dict):\n",
        "    zip_bytes_names = [k for k in uploaded.keys() if k.lower().endswith(\".zip\")]\n",
        "\n",
        "zip_file_paths = []\n",
        "if 'loaded_files' in globals():\n",
        "    try:\n",
        "        # loaded_files might be a list of paths\n",
        "        zip_file_paths = [p for p in loaded_files if str(p).lower().endswith(\".zip\") and os.path.exists(str(p))]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ---------- robust CSV reader ----------\n",
        "def read_csv_from_sources(filename: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Try, in order:\n",
        "      1) Direct filesystem path\n",
        "      2) uploaded[...] direct bytes\n",
        "      3) CSV inside uploaded ZIP bytes\n",
        "      4) CSV inside on-disk ZIPs listed in loaded_files\n",
        "    Match by exact member name first; fall back to basename match.\n",
        "    \"\"\"\n",
        "    fn = str(filename)\n",
        "    bname = os.path.basename(fn)\n",
        "\n",
        "    # 1) Direct filesystem\n",
        "    try:\n",
        "        if os.path.exists(fn):\n",
        "            return pd.read_csv(fn)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) uploaded[...] direct\n",
        "    if 'uploaded' in globals() and isinstance(uploaded, dict):\n",
        "        if fn in uploaded:\n",
        "            return pd.read_csv(io.StringIO(uploaded[fn].decode(\"utf-8\")))\n",
        "        if bname in uploaded:\n",
        "            return pd.read_csv(io.StringIO(uploaded[bname].decode(\"utf-8\")))\n",
        "\n",
        "        # 3) inside uploaded ZIP bytes\n",
        "        for zname in zip_bytes_names:\n",
        "            with zipfile.ZipFile(io.BytesIO(uploaded[zname])) as zf:\n",
        "                # exact\n",
        "                try:\n",
        "                    data = zf.read(fn)\n",
        "                    return pd.read_csv(io.StringIO(data.decode(\"utf-8\")))\n",
        "                except KeyError:\n",
        "                    pass\n",
        "                # basename fallback\n",
        "                try:\n",
        "                    # look for a member whose basename matches\n",
        "                    for member in zf.namelist():\n",
        "                        if os.path.basename(member) == bname:\n",
        "                            data = zf.read(member)\n",
        "                            return pd.read_csv(io.StringIO(data.decode(\"utf-8\")))\n",
        "                except KeyError:\n",
        "                    pass\n",
        "\n",
        "    # 4) inside on-disk ZIPs\n",
        "    for zpath in zip_file_paths:\n",
        "        try:\n",
        "            with zipfile.ZipFile(zpath, 'r') as zf:\n",
        "                # exact\n",
        "                try:\n",
        "                    data = zf.read(fn)\n",
        "                    return pd.read_csv(io.StringIO(data.decode(\"utf-8\")))\n",
        "                except KeyError:\n",
        "                    pass\n",
        "                # basename fallback\n",
        "                for member in zf.namelist():\n",
        "                    if os.path.basename(member) == bname:\n",
        "                        data = zf.read(member)\n",
        "                        return pd.read_csv(io.StringIO(data.decode(\"utf-8\")))\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    raise FileNotFoundError(f\"Could not locate '{filename}' in filesystem, uploaded files, or known ZIPs.\")\n",
        "\n",
        "# ---------- concatenation logic ----------\n",
        "def concat_one_group(file_list):\n",
        "    \"\"\"\n",
        "    Given a list of filenames for one Mouse_ID x FED_StartDate group, sort by the first timestamp in each file,\n",
        "    offset cumulative counts, check session consistency, and return a combined DataFrame and the ordered filenames.\n",
        "    \"\"\"\n",
        "    # Collect first timestamps for sorting\n",
        "    info = []\n",
        "    for fn in file_list:\n",
        "        df = read_csv_from_sources(fn)\n",
        "        if 'MM:DD:YYYY hh:mm:ss' not in df.columns:\n",
        "            raise ValueError(f\"{fn} missing 'MM:DD:YYYY hh:mm:ss' column.\")\n",
        "        df['MM:DD:YYYY hh:mm:ss'] = pd.to_datetime(df['MM:DD:YYYY hh:mm:ss'])\n",
        "        if df['MM:DD:YYYY hh:mm:ss'].isna().all():\n",
        "            raise ValueError(f\"{fn} has no valid timestamps in 'MM:DD:YYYY hh:mm:ss'.\")\n",
        "        first_ts = df['MM:DD:YYYY hh:mm:ss'].iloc[0]\n",
        "        info.append((first_ts, fn))\n",
        "\n",
        "    info.sort(key=lambda x: x[0])  # earliest first\n",
        "    ordered = [fn for _, fn in info]\n",
        "\n",
        "    # Initialize with the first file\n",
        "    df_combined = read_csv_from_sources(ordered[0])\n",
        "    df_combined['MM:DD:YYYY hh:mm:ss'] = pd.to_datetime(df_combined['MM:DD:YYYY hh:mm:ss'])\n",
        "\n",
        "    # Track counts\n",
        "    required_cols = ['Left_Poke_Count', 'Right_Poke_Count', 'Pellet_Count']\n",
        "    for c in required_cols:\n",
        "        if c not in df_combined.columns:\n",
        "            raise ValueError(f\"{ordered[0]} missing required column '{c}'.\")\n",
        "\n",
        "    left_max   = df_combined['Left_Poke_Count'].max()\n",
        "    right_max  = df_combined['Right_Poke_Count'].max()\n",
        "    pellet_max = df_combined['Pellet_Count'].max()\n",
        "\n",
        "    base_session = df_combined['Session_type'].iloc[0] if 'Session_type' in df_combined.columns else None\n",
        "\n",
        "    for fn in ordered[1:]:\n",
        "        df_new = read_csv_from_sources(fn)\n",
        "        df_new['MM:DD:YYYY hh:mm:ss'] = pd.to_datetime(df_new['MM:DD:YYYY hh:mm:ss'])\n",
        "\n",
        "        # Session type consistency (if present)\n",
        "        if base_session is not None and 'Session_type' in df_new.columns:\n",
        "            if df_new['Session_type'].iloc[0] != base_session:\n",
        "                raise ValueError(\n",
        "                    f\"Session type mismatch: '{ordered[0]}' is '{base_session}' \"\n",
        "                    f\"but '{fn}' is '{df_new['Session_type'].iloc[0]}'\"\n",
        "                )\n",
        "\n",
        "        # Warn if big timestamp gap\n",
        "        gap = df_new['MM:DD:YYYY hh:mm:ss'].iloc[0] - df_combined['MM:DD:YYYY hh:mm:ss'].iloc[-1]\n",
        "        if gap > pd.Timedelta(hours=24):\n",
        "            print(f\"Warning: >24h gap between files: {ordered[0]} -> {fn}\")\n",
        "\n",
        "        # Offset cumulative counts\n",
        "        for c, offset in [('Left_Poke_Count', left_max),\n",
        "                          ('Right_Poke_Count', right_max),\n",
        "                          ('Pellet_Count', pellet_max)]:\n",
        "            if c not in df_new.columns:\n",
        "                raise ValueError(f\"{fn} missing required column '{c}'.\")\n",
        "            df_new[c] = df_new[c] + offset\n",
        "\n",
        "        # Append\n",
        "        df_combined = pd.concat([df_combined, df_new], ignore_index=True)\n",
        "\n",
        "        # Update trackers\n",
        "        left_max   = df_combined['Left_Poke_Count'].max()\n",
        "        right_max  = df_combined['Right_Poke_Count'].max()\n",
        "        pellet_max = df_combined['Pellet_Count'].max()\n",
        "\n",
        "    df_combined.attrs[\"Concatenated\"] = True\n",
        "    return df_combined, ordered\n",
        "\n",
        "# ---------- drive the concatenation per (Mouse_ID, FED_StartDate) ----------\n",
        "outputs = []\n",
        "group_cols = [animal_col, 'FED_StartDate']\n",
        "\n",
        "for (mouse_id, fed_start_date), sub in mapped.groupby(group_cols):\n",
        "    files_for_group = sorted(set(sub['file'].tolist()))\n",
        "    if len(files_for_group) <= 1:\n",
        "        continue  # skip single-file groups\n",
        "\n",
        "    combined, ordered_files = concat_one_group(files_for_group)\n",
        "\n",
        "    # Determine output name\n",
        "    fed_first, date_str, _ = parse_fed_and_date_from_name(ordered_files[0])\n",
        "    if fed_first is not None and date_str is not None:\n",
        "        out_name = f\"FED{fed_first:03d}_{date_str}_00.CSV\"\n",
        "    else:\n",
        "        sd_str = pd.to_datetime(fed_start_date).strftime(\"%Y%m%d\")\n",
        "        out_name = f\"{mouse_id}_{sd_str}.csv\"\n",
        "\n",
        "    combined.to_csv(out_name, index=False)\n",
        "    outputs.append((str(mouse_id), pd.to_datetime(fed_start_date), out_name, ordered_files))\n",
        "\n",
        "# Summary\n",
        "print(\"Wrote concatenated files (groups with >1 file only):\")\n",
        "for mouse_id, fed_start_date, out_name, ordered_files in outputs:\n",
        "    sd = pd.to_datetime(fed_start_date).strftime(\"%Y-%m-%d\")\n",
        "    print(f\"  {mouse_id} @ {sd} -> {out_name}  (from {len(ordered_files)} file(s))\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKLpgq_SD3mU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title QC Check (concat-only for concatenated animals; originals otherwise)\n",
        "\n",
        "\n",
        "import os, re, io\n",
        "import pandas as pd\n",
        "from datetime import timedelta, datetime\n",
        "\n",
        "# ---- Gather concatenation info if available ----\n",
        "outputs = globals().get(\"outputs\", [])  # [(animal_id, start_date, out_name, ordered_files)] or old 3-tuple shape\n",
        "\n",
        "# Ensure globals used later exist\n",
        "if \"loaded_files\" not in globals() or loaded_files is None:\n",
        "    loaded_files = []\n",
        "if \"feds\" not in globals() or feds is None:\n",
        "    feds = []\n",
        "\n",
        "# ---- Make concatenated outputs visible to QC (append to loaded_files/feds if missing) ----\n",
        "# Tries fed3.load if available; otherwise loads with pandas and sets index + minimal attrs.\n",
        "try:\n",
        "    import fed3  # your usual loader if available\n",
        "except Exception:\n",
        "    fed3 = None\n",
        "\n",
        "def _load_minimal_dataframe(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    if \"MM:DD:YYYY hh:mm:ss\" in df.columns:\n",
        "        df[\"MM:DD:YYYY hh:mm:ss\"] = pd.to_datetime(df[\"MM:DD:YYYY hh:mm:ss\"])\n",
        "        df = df.set_index(\"MM:DD:YYYY hh:mm:ss\")\n",
        "    # carry Session_type into attrs if present\n",
        "    if \"Session_type\" in df.columns and not df.empty:\n",
        "        try:\n",
        "            df.attrs[\"Session_type\"] = str(df[\"Session_type\"].iloc[0])\n",
        "        except Exception:\n",
        "            pass\n",
        "    return df\n",
        "\n",
        "for rec in outputs:\n",
        "    if len(rec) == 4:\n",
        "        _aid, _start_date, out_name, _src = rec\n",
        "    elif len(rec) == 3:\n",
        "        _aid, out_name, _src = rec\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected outputs tuple shape: {len(rec)}: {rec}\")\n",
        "\n",
        "    # Add to loaded_files if not already present (by exact string)\n",
        "    if out_name not in loaded_files:\n",
        "        loaded_files.append(out_name)\n",
        "        try:\n",
        "            df_loaded = fed3.load(out_name) if fed3 is not None else _load_minimal_dataframe(out_name)\n",
        "        except Exception:\n",
        "            df_loaded = _load_minimal_dataframe(out_name)\n",
        "        # Tag as concatenated (CSV doesn’t keep attrs):\n",
        "        try:\n",
        "            df_loaded.attrs[\"Concatenated\"] = True\n",
        "        except Exception:\n",
        "            pass\n",
        "        feds.append(df_loaded)\n",
        "\n",
        "# ---- Map animal -> list of concatenated output paths (usually 1 each)\n",
        "concat_by_animal = {}\n",
        "for rec in outputs:\n",
        "    if len(rec) == 4:\n",
        "        animal_id, _start_date, out_name, _src = rec\n",
        "    elif len(rec) == 3:\n",
        "        animal_id, out_name, _src = rec\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected outputs tuple shape: {len(rec)}: {rec}\")\n",
        "    concat_by_animal.setdefault(str(animal_id), []).append(out_name)\n",
        "\n",
        "# ---- Determine mapping source for originals\n",
        "if \"mapped\" not in globals():\n",
        "    if \"matched_df\" in globals() and isinstance(matched_df, pd.DataFrame) and not matched_df.empty:\n",
        "        animal_col = \"Animal_ID\"\n",
        "        mapped = matched_df[[\"file\", animal_col]].copy()\n",
        "    else:\n",
        "        raise RuntimeError(\"Neither 'mapped' nor a usable 'matched_df' is available for QC.\")\n",
        "elif \"animal_col\" not in globals():\n",
        "    # Best guess if not set\n",
        "    animal_col = \"Animal_ID\" if \"Animal_ID\" in mapped.columns else mapped.columns[0]\n",
        "\n",
        "# ---- Build manifest: (animal_id, identifier, is_concat) ----\n",
        "#  - Animals present in concat_by_animal: include ONLY those concatenated CSV(s)\n",
        "#  - All other animals: include ALL originals\n",
        "manifest_for_qc = []\n",
        "\n",
        "# 1) concatenated targets\n",
        "for animal_id, out_names in concat_by_animal.items():\n",
        "    for out_name in out_names:\n",
        "        manifest_for_qc.append((animal_id, out_name, True))\n",
        "\n",
        "# 2) originals for animals that were NOT concatenated\n",
        "for animal_id, sub in mapped.groupby(animal_col):\n",
        "    if str(animal_id) in concat_by_animal:\n",
        "        continue  # exclude originals for concatenated animals\n",
        "    files_for_animal = sorted(set(sub[\"file\"].tolist()))\n",
        "    for fn in files_for_animal:\n",
        "        manifest_for_qc.append((str(animal_id), fn, False))\n",
        "\n",
        "if not manifest_for_qc:\n",
        "    print(\"No concatenations and no original files found for QC.\")\n",
        "    # Still define empty outputs for downstream robustness\n",
        "    passed_indices = []\n",
        "    flagged_indices = []\n",
        "    flagged_reasons = {}\n",
        "else:\n",
        "    # ---- Deduplicate manifest entries while preserving concat/orig identity ----\n",
        "    seen = set()\n",
        "    unique_manifest = []\n",
        "    for animal_id, ident, is_concat in manifest_for_qc:\n",
        "        key = (\"concat\", os.path.abspath(ident)) if is_concat else (\"orig\", ident)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        unique_manifest.append((animal_id, ident, is_concat))\n",
        "\n",
        "    # =========================\n",
        "    # Map manifest -> GLOBAL indices\n",
        "    # =========================\n",
        "    # We will QC the subset of GLOBAL feds/loaded_files whose basenames match the manifest.\n",
        "    # This keeps passed/flagged indices compatible with downstream rename logic.\n",
        "\n",
        "    # Basenames we expect to QC this run\n",
        "    names_to_include = set(os.path.basename(ident) for _aid, ident, _c in unique_manifest)\n",
        "\n",
        "    # Build selected_idx = indices into GLOBAL arrays (feds, loaded_files)\n",
        "    selected_idx = []\n",
        "    seen_names = set()\n",
        "    for i, fn in enumerate(loaded_files):\n",
        "        base = os.path.basename(fn)\n",
        "        if base in names_to_include and base not in seen_names:\n",
        "            selected_idx.append(i)\n",
        "            seen_names.add(base)\n",
        "\n",
        "    if not selected_idx:\n",
        "        print(\"QC: No matching files from manifest were found in loaded_files; nothing to check.\")\n",
        "        # Define empty outputs for downstream robustness\n",
        "        passed_indices = []\n",
        "        flagged_indices = []\n",
        "        flagged_reasons = {}\n",
        "        print(\"QC complete (this run only): 0 passed, 0 flagged.\")\n",
        "    else:\n",
        "        # ---- Build run views from the selected GLOBAL indices (do not change order)\n",
        "        feds_run  = [feds[i] for i in selected_idx]\n",
        "        files_run = [loaded_files[i] for i in selected_idx]\n",
        "\n",
        "        # Map run-local index -> GLOBAL index (used to key reasons globally)\n",
        "        run_to_global = {i_run: selected_idx[i_run] for i_run in range(len(selected_idx))}\n",
        "\n",
        "        # ===== QC checks (reasons keyed by GLOBAL index) =====\n",
        "        duration_threshold = timedelta(hours=12)\n",
        "        required_days = {\"Bandit100\": 2.5, \"Bandit80\": 1.5, \"FR1\": 0.8, \"PR1\": 1.5}\n",
        "\n",
        "        passed_indices = []            # GLOBAL indices\n",
        "        flagged_indices = []           # GLOBAL indices\n",
        "        flagged_reasons = {}           # {GLOBAL index: reason string}\n",
        "\n",
        "        def extract_file_date(filename):\n",
        "            # Pattern: FED###_MMDDYY...\n",
        "            match = re.match(r\"FED\\d{3}_(\\d{6})\", filename)\n",
        "            if not match:\n",
        "                return None\n",
        "            mm, dd, yy = match.group(1)[:2], match.group(1)[2:4], match.group(1)[4:]\n",
        "            yyyy = \"20\" + yy if int(yy) < 50 else \"19\" + yy\n",
        "            return datetime.strptime(f\"{yyyy}-{mm}-{dd}\", \"%Y-%m-%d\")\n",
        "\n",
        "        file_dates = [extract_file_date(os.path.basename(f)) for f in files_run]\n",
        "\n",
        "        # Group run-local indices by file date (cohort)\n",
        "        cohort_map = {}\n",
        "        for i_run, date in enumerate(file_dates):\n",
        "            cohort_map.setdefault(date, []).append(i_run)\n",
        "\n",
        "        for cohort_date, cohort_i_runs in cohort_map.items():\n",
        "            # Compute cohort medians in RUN space\n",
        "            start_times = []\n",
        "            end_times   = []\n",
        "            for i_run in cohort_i_runs:\n",
        "                df_tmp = feds_run[i_run]\n",
        "                # Ensure datetime index\n",
        "                df_idx = pd.to_datetime(df_tmp.index)\n",
        "                start_times.append(df_idx[0])\n",
        "                end_times.append(df_idx[-1])\n",
        "\n",
        "            median_start = pd.Series(start_times).median()\n",
        "            median_end   = pd.Series(end_times).median()\n",
        "\n",
        "            for i_run in cohort_i_runs:\n",
        "                df = feds_run[i_run].copy()\n",
        "                df.index = pd.to_datetime(df.index)\n",
        "\n",
        "                i_global = run_to_global[i_run]  # GLOBAL index for all bookkeeping\n",
        "                device = str(df['Device_Number'].iloc[0]).zfill(3) if 'Device_Number' in df.columns and not df.empty else \"UNK\"\n",
        "\n",
        "                # Prefer DataFrame attrs; fall back to column if needed\n",
        "                session_type = str(df.attrs.get(\"Session_type\",\n",
        "                                    df['Session_type'].iloc[0] if 'Session_type' in df.columns and not df.empty else \"Unknown\")).strip().replace(\" \", \"_\")\n",
        "                min_days = float(required_days.get(session_type, 1))  # default 1 day\n",
        "\n",
        "                reason_list = []\n",
        "\n",
        "                # Start/End deviation from cohort medians\n",
        "                if abs(df.index[0] - median_start) > duration_threshold:\n",
        "                    reason_list.append(\n",
        "                        f\"Device {device}: Start time {df.index[0]} is >8 hrs from cohort median {median_start}\"\n",
        "                    )\n",
        "                if abs(df.index[-1] - median_end) > duration_threshold:\n",
        "                    reason_list.append(\n",
        "                        f\"Device {device}: End time {df.index[-1]} is >8 hrs from cohort median {median_end}\"\n",
        "                    )\n",
        "\n",
        "                # --- Duration checks (ENFORCE required_days) ---\n",
        "                duration_hours = (df.index.max() - df.index.min()).total_seconds() / 3600.0\n",
        "                min_hours = 24.0 * min_days\n",
        "                if not pd.isna(duration_hours) and duration_hours < min_hours:\n",
        "                    reason_list.append(\n",
        "                        f\"Device {device}: Duration {duration_hours:.1f} h < required {min_hours:.1f} h for {session_type}.\"\n",
        "                    )\n",
        "\n",
        "                # === Estimated pellets/day (no noon bins) ===\n",
        "                if 'Pellet_Count' not in df.columns or df['Pellet_Count'].dropna().empty:\n",
        "                    reason_list.append(f\"Device {device}: Missing/empty Pellet_Count.\")\n",
        "                    flagged_indices.append(i_global)\n",
        "                    flagged_reasons[i_global] = \" | \".join(reason_list)\n",
        "                    continue\n",
        "\n",
        "                # Ensure numeric and aligned to time index\n",
        "                pc = pd.to_numeric(df['Pellet_Count'], errors='coerce').dropna()\n",
        "                if pc.empty or len(df.index) < 2:\n",
        "                    reason_list.append(f\"Device {device}: Not enough data to estimate pellets/day.\")\n",
        "                    flagged_indices.append(i_global)\n",
        "                    flagged_reasons[i_global] = \" | \".join(reason_list)\n",
        "                    continue\n",
        "\n",
        "                # Duration guard already computed (duration_hours)\n",
        "                if duration_hours <= 0:\n",
        "                    reason_list.append(f\"Device {device}: Non-positive duration.\")\n",
        "                    flagged_indices.append(i_global)\n",
        "                    flagged_reasons[i_global] = \" | \".join(reason_list)\n",
        "                    continue\n",
        "\n",
        "                # Pellet events = sum of positive increments (robust to counter resets)\n",
        "                diffs = pc.diff().fillna(0)\n",
        "                pellet_events = float(diffs.clip(lower=0).sum())\n",
        "\n",
        "                # Fallback if strictly monotonic but diffs sum is zero (degenerate edge case)\n",
        "                if pellet_events == 0 and pc.iloc[-1] >= pc.iloc[0]:\n",
        "                    pellet_events = float(pc.iloc[-1] - pc.iloc[0])\n",
        "\n",
        "                # Estimated pellets per day\n",
        "                est_pellets_per_day = (pellet_events / duration_hours) * 24.0\n",
        "                df.attrs['Estimated_Pellets_Per_Day'] = est_pellets_per_day\n",
        "\n",
        "                # Threshold check\n",
        "                PELLET_THRESHOLD = 75  # change if needed\n",
        "                if pd.isna(est_pellets_per_day):\n",
        "                    reason_list.append(f\"Device {device}: Could not compute estimated pellets/day.\")\n",
        "                elif est_pellets_per_day < PELLET_THRESHOLD:\n",
        "                    reason_list.append(\n",
        "                        f\"Device {device}: Estimated {est_pellets_per_day:.1f} pellets/day < {PELLET_THRESHOLD}.\"\n",
        "                    )\n",
        "\n",
        "                # Flatline check (>8h no change in Pellet_Count)\n",
        "                df_change = df[df['Pellet_Count'] > 0].copy()\n",
        "                df_change['Change'] = df_change['Pellet_Count'].ne(df_change['Pellet_Count'].shift()).cumsum()\n",
        "                for _, group in df_change.groupby('Change'):\n",
        "                    if (group.index[-1] - group.index[0]) > duration_threshold:\n",
        "                        value = group['Pellet_Count'].iloc[0]\n",
        "                        reason_list.append(\n",
        "                            f\"Device {device}: 'Pellet_Count' stayed at {value} for >12 hrs starting {group.index[0]}\"\n",
        "                        )\n",
        "                        break\n",
        "\n",
        "                if reason_list:\n",
        "                    flagged_indices.append(i_global)\n",
        "                    flagged_reasons[i_global] = \" | \".join(reason_list\n",
        "                    )\n",
        "                else:\n",
        "                    passed_indices.append(i_global)\n",
        "\n",
        "        print(f\"QC complete (this run only): {len(passed_indices)} passed, {len(flagged_indices)} flagged.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z3UFrrMwD3TN"
      },
      "outputs": [],
      "source": [
        "# @title Manual Review Slider (UI only)\n",
        "from collections import defaultdict\n",
        "# ===== Review prefilter (dedupe concatenations by FED/date) =====\n",
        "six_pat = re.compile(r\"^FED(\\d{3})_(\\d{6})\", re.IGNORECASE)\n",
        "\n",
        "def key_from_filename(fname: str):\n",
        "    s = str(fname).strip()\n",
        "    m = six_pat.match(s)\n",
        "    if not m:\n",
        "        return None\n",
        "    fed = int(m.group(1))     # 072 -> 72\n",
        "    date_str = m.group(2)     # MMDDYY\n",
        "    return (fed, date_str)\n",
        "\n",
        "def infer_fed_from_df(df, fname):\n",
        "    # prefer numeric column, else fallback to filename FED### anywhere\n",
        "    if isinstance(df, pd.DataFrame) and not df.empty and 'Device_Number' in df.columns:\n",
        "        try:\n",
        "            return int(df['Device_Number'].iloc[0])\n",
        "        except Exception:\n",
        "            pass\n",
        "    m = re.search(r\"FED(\\d{3})\", os.path.basename(str(fname)), re.IGNORECASE)\n",
        "    if m:\n",
        "        try:\n",
        "            return int(m.group(1))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def infer_mmddyy_from_df(df):\n",
        "    # try common datetime columns; convert to date; format as MMDDYY\n",
        "    for col in (\"Date_Time\", \"Datetime\", \"Time\", \"Timestamp\"):\n",
        "        if col in df.columns:\n",
        "            try:\n",
        "                t = pd.to_datetime(df[col], errors=\"coerce\")\n",
        "                if t.notna().any():\n",
        "                    d = pd.to_datetime(t.dropna().iloc[0]).date()\n",
        "                    return pd.to_datetime(d).strftime(\"%m%d%y\")\n",
        "            except Exception:\n",
        "                pass\n",
        "    # try attrs\n",
        "    for a in (\"Session_Date\", \"Date\", \"Start_Time\"):\n",
        "        val = getattr(df, \"attrs\", {}).get(a)\n",
        "        if val is not None:\n",
        "            try:\n",
        "                d = pd.to_datetime(val, errors=\"coerce\").date()\n",
        "                if pd.notna(d):\n",
        "                    return pd.to_datetime(d).strftime(\"%m%d%y\")\n",
        "            except Exception:\n",
        "                pass\n",
        "    return None\n",
        "\n",
        "def robust_key_for_index(i):\n",
        "    # 1) filename\n",
        "    k = key_from_filename(loaded_files[i])\n",
        "    if k is not None:\n",
        "        return k\n",
        "    # 2) fallbacks from DF\n",
        "    df = feds[i]\n",
        "    fed = infer_fed_from_df(df, loaded_files[i])\n",
        "    date_str = infer_mmddyy_from_df(df)\n",
        "    if fed is not None and date_str is not None:\n",
        "        return (fed, date_str)\n",
        "    return None  # ungroupable\n",
        "\n",
        "def is_concat(idx):\n",
        "    # treat a variety of truthy representations as concatenated\n",
        "    v = feds[idx].attrs.get(\"Concatenated\", False)\n",
        "    if isinstance(v, str):\n",
        "        return v.strip().lower() in {\"1\", \"true\", \"yes\", \"y\", \"concat\", \"concatenated\"}\n",
        "    return bool(v)\n",
        "\n",
        "flag_groups = defaultdict(list)\n",
        "nonstandard_flagged = []\n",
        "\n",
        "for i in flagged_indices:  # i are GLOBAL\n",
        "    k = robust_key_for_index(i)\n",
        "    if k is None:\n",
        "        nonstandard_flagged.append(i)\n",
        "    else:\n",
        "        flag_groups[k].append(i)\n",
        "\n",
        "keep_indices = set()\n",
        "suppressed_indices = set()\n",
        "\n",
        "# Rule: if any concatenated exists in a group, keep only the concatenated file(s).\n",
        "# Otherwise keep all originals in that group.\n",
        "for k, idxs in flag_groups.items():\n",
        "    concats = [i for i in idxs if is_concat(i)]\n",
        "    if concats:\n",
        "        keep_indices.update(concats)\n",
        "        suppressed_indices.update([i for i in idxs if i not in concats])\n",
        "    else:\n",
        "        keep_indices.update(idxs)\n",
        "\n",
        "# Always include any nonstandard-named flagged files in the UI (can’t group them)\n",
        "keep_indices.update(nonstandard_flagged)\n",
        "\n",
        "# Final list for review UI. Prefer to show concatenated first (cosmetic).\n",
        "def sort_key(i):\n",
        "    return (0 if is_concat(i) else 1, loaded_files[i])\n",
        "flagged_indices_review = sorted([i for i in flagged_indices if i in keep_indices], key=sort_key)\n",
        "\n",
        "print(f\"\\n[Review Prefilter] Flagged total            : {len(flagged_indices)}\")\n",
        "print(f\"[Review Prefilter] Nonstandard-name flagged : {len(nonstandard_flagged)}\")\n",
        "print(f\"[Review Prefilter] Suppressed originals     : {len(suppressed_indices)}\")\n",
        "if suppressed_indices:\n",
        "    print(\"  Suppressed (concatenated counterpart exists for same FED/MMDDYY):\")\n",
        "    for i in sorted(suppressed_indices, key=lambda x: loaded_files[x]):\n",
        "        print(f\"    - {loaded_files[i]}  -> kept concat present\")\n",
        "print(f\"[Review Prefilter] Flagged shown in UI      : {len(flagged_indices_review)}\")\n",
        "\n",
        "\n",
        "# ===== Manual review UI (reasons fetched by GLOBAL index) =====\n",
        "approved_indices = []\n",
        "override_indices = []\n",
        "manually_rejected_indices = []\n",
        "inclusion_map = {}        # {GLOBAL index: True/False}\n",
        "decision_log = {}\n",
        "\n",
        "if not flagged_indices_review:\n",
        "    print(\"All files passed QC — no review needed.\")\n",
        "else:\n",
        "    plot_output = widgets.Output()\n",
        "    button_output = widgets.Output()\n",
        "    decision_log_output = widgets.Output()\n",
        "\n",
        "    current_idx = {\"value\": 0}\n",
        "    review_complete = {\"done\": False}\n",
        "\n",
        "    def log_decision(idx_global, decision):\n",
        "        fname = loaded_files[idx_global]\n",
        "        fed_id = str(feds[idx_global][\"Device_Number\"].iloc[0]).zfill(3)\n",
        "        status = \"Included\" if decision else \"Excluded\"\n",
        "        decision_log[idx_global] = f\"{status}: {fname} (FED{fed_id})\"\n",
        "        with decision_log_output:\n",
        "            clear_output(wait=True)\n",
        "            for line in decision_log.values():\n",
        "                print(line)\n",
        "\n",
        "    def show_current_file():\n",
        "        idx_global = flagged_indices_review[current_idx[\"value\"]]\n",
        "        df = feds[idx_global]\n",
        "        fname = loaded_files[idx_global]\n",
        "\n",
        "        with plot_output:\n",
        "            clear_output(wait=True)\n",
        "            fed3.as_aligned([df], alignment=\"datetime\", inplace=True)\n",
        "            plt.figure(figsize=(8, 4))\n",
        "            # Your plotting helper; keep as you had it\n",
        "            fplot.line([df], y='pellets')\n",
        "            plt.title(fname)\n",
        "            plt.tight_layout()\n",
        "            legend = plt.legend()\n",
        "            if legend:\n",
        "                legend.remove()\n",
        "            plt.show()\n",
        "\n",
        "        with button_output:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"File {current_idx['value'] + 1} of {len(flagged_indices_review)}: {fname}\")\n",
        "            # FIX A in action: reasons keyed by GLOBAL index\n",
        "            print(\"Reason(s):\", flagged_reasons.get(idx_global, \"(no reason found)\"))\n",
        "            status = inclusion_map.get(idx_global, None)\n",
        "            print(\"Current decision:\", \"Included\" if status else \"Excluded\" if status is False else \"Undecided\")\n",
        "\n",
        "            include_btn = widgets.Button(description=\"Include\", button_style='success')\n",
        "            exclude_btn = widgets.Button(description=\"Exclude\", button_style='danger')\n",
        "\n",
        "            def handle_decision(decision):\n",
        "                if review_complete[\"done\"]:\n",
        "                    return\n",
        "\n",
        "                # cleanup and record\n",
        "                for lst in (approved_indices, override_indices, manually_rejected_indices):\n",
        "                    if idx_global in lst:\n",
        "                        lst.remove(idx_global)\n",
        "\n",
        "                inclusion_map[idx_global] = decision\n",
        "                if decision:\n",
        "                    approved_indices.append(idx_global)\n",
        "                    override_indices.append(idx_global)\n",
        "                else:\n",
        "                    manually_rejected_indices.append(idx_global)\n",
        "\n",
        "                log_decision(idx_global, decision)\n",
        "\n",
        "                if current_idx[\"value\"] < len(flagged_indices_review) - 1:\n",
        "                    current_idx[\"value\"] += 1\n",
        "                    show_current_file()\n",
        "                else:\n",
        "                    review_complete[\"done\"] = True\n",
        "                    with plot_output:\n",
        "                        clear_output(wait=True)\n",
        "                        print(\"Review complete. All flagged files have been reviewed.\")\n",
        "                    with button_output:\n",
        "                        clear_output(wait=True)\n",
        "\n",
        "            include_btn.on_click(lambda _: handle_decision(True))\n",
        "            exclude_btn.on_click(lambda _: handle_decision(False))\n",
        "            display(widgets.HBox([include_btn, exclude_btn]))\n",
        "\n",
        "    display(plot_output, button_output, widgets.Label(\"Review Log:\"), decision_log_output)\n",
        "    show_current_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "D_tiiEYjWTLQ"
      },
      "outputs": [],
      "source": [
        "# @title Rename Files, and Update Key with Exclusions and Flag Reasons\n",
        "import os, re, zipfile\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "from google.colab import files as gcolab_files  # for files.download\n",
        "from openpyxl.styles import PatternFill\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "# ---- Bridge QC → Rename: treat QC passes as approved, apply overrides, drop manual rejects ----\n",
        "def _to_int_set(x):\n",
        "    try:\n",
        "        return {int(i) for i in x}\n",
        "    except NameError:\n",
        "        return set()\n",
        "    except Exception:\n",
        "        return {int(i) for i in list(x)}\n",
        "\n",
        "n = len(feds)\n",
        "qc_pass_set      = _to_int_set(passed_indices)\n",
        "manual_approve   = _to_int_set(approved_indices)\n",
        "overrides_set    = _to_int_set(override_indices)\n",
        "manual_rejects   = _to_int_set(manually_rejected_indices)\n",
        "\n",
        "combined = (qc_pass_set | manual_approve | overrides_set) - manual_rejects\n",
        "approved_indices = sorted(i for i in combined if 0 <= i < n)\n",
        "\n",
        "def process_key_and_rename(key_file, approved_indices, feds, loaded_files,\n",
        "                           manually_rejected_indices, override_indices, flagged_reasons):\n",
        "\n",
        "    # --- Load key (do NOT coerce FED3) ---\n",
        "    key_df = pd.read_excel(key_file)\n",
        "    if \"FED_StartDate\" not in key_df.columns:\n",
        "        raise ValueError(\"Key must have a 'FED_StartDate' column.\")\n",
        "    key_df[\"FED_StartDate\"] = pd.to_datetime(key_df[\"FED_StartDate\"], errors=\"coerce\")\n",
        "\n",
        "    # --- Helpers ---\n",
        "    def parse_fed3_cell(val):\n",
        "        if pd.isna(val):\n",
        "            return []\n",
        "        s = str(val)\n",
        "        s = re.sub(r\"\\b(?:and)\\b\", \",\", s, flags=re.I)\n",
        "        s = re.sub(r\"[+,/&]\", \",\", s)\n",
        "        s = re.sub(r\"[;|]\", \",\", s)\n",
        "        parts = [p.strip() for p in s.split(\",\") if p.strip()]\n",
        "        return sorted({int(m) for p in parts for m in re.findall(r\"\\d+\", p)})\n",
        "\n",
        "    def z3(x):  # zero-pad int to 3 digits\n",
        "        return str(int(x)).zfill(3)\n",
        "\n",
        "    def safe_gene_id(row):\n",
        "        # Gene_ID is numeric; return zero-padded 3-digit string\n",
        "        return f\"{int(row['Gene_ID']):03d}\" if 'Gene_ID' in row and pd.notna(row['Gene_ID']) else \"000\"\n",
        "\n",
        "    def normalize_session(st):\n",
        "        st = str(st if st is not None else \"Unknown\").strip()\n",
        "        return st.replace(\" \", \"_\") if st else \"Unknown\"\n",
        "\n",
        "    # --- Explode key: one FED per row, keep pointer to original row ---\n",
        "    key_df = key_df.reset_index().rename(columns={\"index\": \"orig_idx\"})\n",
        "    key_df[\"_FED3_list\"] = key_df[\"FED3\"].apply(parse_fed3_cell)\n",
        "    key_exploded = (\n",
        "        key_df.explode(\"_FED3_list\", ignore_index=True)\n",
        "              .dropna(subset=[\"_FED3_list\"])\n",
        "              .rename(columns={\"_FED3_list\": \"FED3_int\"})\n",
        "    )\n",
        "    key_exploded[\"FED3_int\"] = key_exploded[\"FED3_int\"].astype(int)\n",
        "\n",
        "    # --- ±14d window matching helper (handles multi-cohort FEDs) ---\n",
        "    window = timedelta(days=14)\n",
        "\n",
        "    def pick_best_key_row(fed_id: int, file_date) -> pd.Series | None:\n",
        "        \"\"\"\n",
        "        Find the nearest FED_StartDate for this FED within ±window.\n",
        "        - Prefer smallest absolute time delta.\n",
        "        - Break ties by preferring the later FED_StartDate (assume newer cohort).\n",
        "        Returns a single row (Series) from key_exploded, or None if no match.\n",
        "        \"\"\"\n",
        "        if pd.isna(file_date):\n",
        "            return None\n",
        "        file_date_ts = pd.to_datetime(file_date, errors=\"coerce\")\n",
        "        if pd.isna(file_date_ts):\n",
        "            return None\n",
        "\n",
        "        sub = key_exploded[key_exploded[\"FED3_int\"] == int(fed_id)].copy()\n",
        "        if sub.empty:\n",
        "            return None\n",
        "        sub[\"abs_delta\"] = (file_date_ts - sub[\"FED_StartDate\"]).abs()\n",
        "        sub = sub[sub[\"abs_delta\"] <= window]\n",
        "        if sub.empty:\n",
        "            return None\n",
        "        return sub.sort_values([\"abs_delta\", \"FED_StartDate\"], ascending=[True, False]).iloc[0]\n",
        "\n",
        "    # --- Outputs / logs ---\n",
        "    output_dir = \"renamed_output\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    renamed_files = []\n",
        "    files_not_in_key = []\n",
        "    no_file_log = []\n",
        "    flagged_log = []\n",
        "\n",
        "    key_modified = False\n",
        "    last_gene_name = last_gene_id = last_session_type = None\n",
        "\n",
        "    # --- Styling targets for the Excel output ---\n",
        "    # Each entry: (orig_idx, col_name, tag) where tag in {'reject','include','no_file'}\n",
        "    cell_styles = []\n",
        "\n",
        "    # --- Rename and collect matched data (approved only) ---\n",
        "    match_lookup = {}\n",
        "    for idx in approved_indices:\n",
        "        df = feds[idx]\n",
        "        original_name = loaded_files[idx]\n",
        "\n",
        "        # Get FED id and file_date from the source filename\n",
        "        fed_id, _date_str_unused, file_date = extract_fed_and_date(original_name)\n",
        "        if fed_id is None or file_date is None:\n",
        "            continue\n",
        "\n",
        "        # Find the matching key row within ±14-day window for this FED\n",
        "        cand = pick_best_key_row(fed_id, file_date)\n",
        "        if cand is None:\n",
        "            files_not_in_key.append((original_name, z3(fed_id)))\n",
        "            continue\n",
        "\n",
        "        orig_idx = cand[\"orig_idx\"]\n",
        "        row_orig = key_df.loc[key_df[\"orig_idx\"] == orig_idx].iloc[0]\n",
        "\n",
        "        # --- Build components for the new filename ---\n",
        "        match_lookup[idx] = row_orig\n",
        "        mouse_id   = row_orig.get(\"Mouse_ID\", \"Unknown\")\n",
        "        gene_name  = str(row_orig.get(\"Gene\", \"NA\"))\n",
        "        gene_id    = safe_gene_id(row_orig)\n",
        "\n",
        "        raw_mouse = (\"\" if pd.isna(mouse_id) else str(mouse_id)).strip()\n",
        "        if raw_mouse == \"\":\n",
        "            animal = \"Unknown\"\n",
        "        elif re.fullmatch(r\"\\d+(\\.0+)?\", raw_mouse):\n",
        "            animal = str(int(float(raw_mouse)))  # normalize 12.0 -> 12\n",
        "        else:\n",
        "            animal = raw_mouse\n",
        "\n",
        "        session_type = normalize_session(df.attrs.get(\"Session_type\", \"Unknown\"))\n",
        "\n",
        "        # Use a clean YYYYMMDD derived from file_date\n",
        "        date_str = pd.to_datetime(file_date).strftime(\"%Y%m%d\")\n",
        "        new_name = f\"{animal}_{session_type}_{date_str}.csv\"\n",
        "\n",
        "        # Overwrite-if-exists (NO suffixes)\n",
        "        candidate = os.path.join(output_dir, new_name)\n",
        "        df.reset_index().to_csv(candidate, index=False)\n",
        "        renamed_files.append(candidate)\n",
        "\n",
        "        last_gene_name, last_gene_id, last_session_type = gene_name, gene_id, session_type\n",
        "\n",
        "    # --- Zip renamed files (optional, keep if you still want the zip) ---\n",
        "    zip_name = None\n",
        "    if renamed_files:\n",
        "        zip_name = f\"{last_gene_name}_{last_gene_id}_{last_session_type}_L1.zip\"\n",
        "        with zipfile.ZipFile(zip_name, \"w\") as zipf:\n",
        "            for file_path in renamed_files:\n",
        "                zipf.write(file_path, arcname=os.path.basename(file_path))\n",
        "        gcolab_files.download(zip_name)\n",
        "\n",
        "    # --- Sessions seen in this batch ---\n",
        "    session_for_idx = {}\n",
        "    session_types_present = set()\n",
        "    for i, df_i in enumerate(feds):\n",
        "        st = normalize_session(getattr(df_i, \"attrs\", {}).get(\"Session_type\", \"Unknown\"))\n",
        "        session_for_idx[i] = st\n",
        "        session_types_present.add(st)\n",
        "    if not session_types_present:\n",
        "        session_types_present = {\"Unknown\"}\n",
        "\n",
        "    # --- PRESENT animals per session (any uploaded file: approved, rejected, or override) ---\n",
        "    present_by_session = {st: set() for st in session_types_present}\n",
        "    for i, df_i in enumerate(feds):\n",
        "        original_name_i = os.path.basename(str(loaded_files[i]))\n",
        "        fed_id_i, _, file_date_i = extract_fed_and_date(original_name_i)\n",
        "        if fed_id_i is None or file_date_i is None:\n",
        "            continue\n",
        "        cand = pick_best_key_row(fed_id_i, file_date_i)\n",
        "        if cand is None:\n",
        "            continue\n",
        "        st = normalize_session(session_for_idx.get(i, \"Unknown\"))\n",
        "        try:\n",
        "            present_by_session.setdefault(st, set()).add(int(cand[\"orig_idx\"]))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # --- EXPECTED animals per session = all animals in the key ---\n",
        "    all_animals = set(int(x) for x in key_df[\"orig_idx\"].tolist())\n",
        "    expected_by_session = {st: set(all_animals) for st in session_types_present}\n",
        "\n",
        "    # --- NO FILE = expected - present (per session). Write \"No file\" and mark red.\n",
        "    for st in sorted(session_types_present):\n",
        "        target_col = f\"{st}_EX\"\n",
        "        no_file_rows = expected_by_session[st] - present_by_session.get(st, set())\n",
        "        for orig_idx in sorted(no_file_rows):\n",
        "            key_df.loc[key_df[\"orig_idx\"] == orig_idx, target_col] = \"No file\"  # Capital N\n",
        "            r0 = key_df.loc[key_df[\"orig_idx\"] == orig_idx].iloc[0]\n",
        "            no_file_log.append((r0.get(\"FED3\"), r0.get(\"Mouse_ID\")))\n",
        "            cell_styles.append((orig_idx, target_col, \"no_file\"))\n",
        "            key_modified = True\n",
        "\n",
        "    # --- Flagged reasons for rejected or overridden files (write back to original key row) ---\n",
        "    for idx in set(list(override_indices) + list(manually_rejected_indices)):\n",
        "        df = feds[idx]\n",
        "        if 'Device_Number' not in df.columns or df.empty:\n",
        "            continue\n",
        "        try:\n",
        "            fed_int = int(df['Device_Number'].iloc[0])\n",
        "        except Exception:\n",
        "            continue\n",
        "        fed_str = z3(fed_int)\n",
        "        session_type = normalize_session(df.attrs.get(\"Session_type\", \"Unknown\"))\n",
        "        reason = flagged_reasons.get(idx, \"Flagged without reason\")\n",
        "        session_col = f\"{session_type}_EX\"\n",
        "\n",
        "        original_name = loaded_files[idx]\n",
        "        fed_id2, _, file_date2 = extract_fed_and_date(original_name)\n",
        "        if fed_id2 is None or file_date2 is None:\n",
        "            continue\n",
        "\n",
        "        cand = pick_best_key_row(fed_id2, file_date2)\n",
        "        if cand is not None:\n",
        "            orig_idx = cand[\"orig_idx\"]\n",
        "            key_df.loc[key_df[\"orig_idx\"] == orig_idx, session_col] = str(reason)\n",
        "\n",
        "            # Style tag: rejected = red, manually included (override) = orange\n",
        "            if idx in manually_rejected_indices:\n",
        "                cell_styles.append((orig_idx, session_col, \"reject\"))\n",
        "            elif idx in override_indices:\n",
        "                cell_styles.append((orig_idx, session_col, \"include\"))\n",
        "\n",
        "            included = idx in approved_indices\n",
        "            flagged_log.append((fed_str,\n",
        "                                key_df.loc[key_df[\"orig_idx\"] == orig_idx, \"Mouse_ID\"].iloc[0],\n",
        "                                reason,\n",
        "                                included))\n",
        "            key_modified = True\n",
        "\n",
        "    # --- Minimal prints so you can verify ---\n",
        "    print(f\"[No-file marks] {len(no_file_log)} rows:\", no_file_log[:10])\n",
        "    print(\"[Sessions present]:\", sorted(session_types_present))\n",
        "\n",
        "    # --- Save a SINGLE updated key (styled overlay onto the original sheet) ---\n",
        "    styled_path = f\"{last_gene_name}_{last_gene_id}_key_updated.xlsx\" if last_gene_name and last_gene_id else \"updated_key.xlsx\"\n",
        "\n",
        "    if key_modified or renamed_files:\n",
        "        wb = load_workbook(key_file)\n",
        "        # pick the first sheet with FED_StartDate in header row\n",
        "        ws = None\n",
        "        for name in wb.sheetnames:\n",
        "            cand_ws = wb[name]\n",
        "            headers = [c.value for c in cand_ws[1]]\n",
        "            if headers and \"FED_StartDate\" in headers:\n",
        "                ws = cand_ws\n",
        "                break\n",
        "        if ws is None:\n",
        "            ws = wb.active\n",
        "\n",
        "        # header map\n",
        "        header_to_col = {}\n",
        "        for col_idx in range(1, ws.max_column + 1):\n",
        "            header_to_col[str(ws.cell(row=1, column=col_idx).value)] = col_idx\n",
        "\n",
        "        def ensure_column(col_name: str) -> int:\n",
        "            if col_name in header_to_col and header_to_col[col_name]:\n",
        "                return header_to_col[col_name]\n",
        "            new_idx = ws.max_column + 1\n",
        "            ws.cell(row=1, column=new_idx).value = col_name\n",
        "            header_to_col[col_name] = new_idx\n",
        "            return new_idx\n",
        "\n",
        "        # fills\n",
        "        red_fill    = PatternFill(fill_type=\"solid\", start_color=\"FFFFC7CE\", end_color=\"FFFFC7CE\")\n",
        "        orange_fill = PatternFill(fill_type=\"solid\", start_color=\"FFFFE5B2\", end_color=\"FFFFE5B2\")\n",
        "\n",
        "        # 1) FORCE-WRITE all *_EX values from key_df to the sheet\n",
        "        ex_cols = [c for c in key_df.columns if isinstance(c, str) and c.endswith(\"_EX\")]\n",
        "        for col_name in ex_cols:\n",
        "            col_idx = ensure_column(col_name)\n",
        "            for i in range(len(key_df)):\n",
        "                excel_row = int(key_df.loc[i, \"orig_idx\"]) + 2  # header is row 1\n",
        "                val = key_df.loc[i, col_name] if col_name in key_df.columns else None\n",
        "                if pd.isna(val):\n",
        "                    continue\n",
        "                ws.cell(row=excel_row, column=col_idx).value = str(val)\n",
        "\n",
        "        # 2) Apply fills: unconditional RED for \"No file\" + tags\n",
        "        #    First, paint \"No file\" occurrences across all *_EX columns:\n",
        "        for col_name in ex_cols:\n",
        "            col_idx = ensure_column(col_name)\n",
        "            for i in range(len(key_df)):\n",
        "                excel_row = int(key_df.loc[i, \"orig_idx\"]) + 2\n",
        "                cell = ws.cell(row=excel_row, column=col_idx)\n",
        "                v = cell.value\n",
        "                if isinstance(v, str) and v.strip().lower() == \"no file\":\n",
        "                    cell.fill = red_fill  # always paint red for 'No file'\n",
        "\n",
        "        #    Then, apply explicit styles from cell_styles for reject/include\n",
        "        for orig_idx, col_name, tag in cell_styles:\n",
        "            col_idx = ensure_column(col_name)\n",
        "            excel_row = int(orig_idx) + 2\n",
        "            cell = ws.cell(row=excel_row, column=col_idx)\n",
        "            if tag == \"reject\":\n",
        "                cell.fill = red_fill\n",
        "            elif tag == \"include\":\n",
        "                cell.fill = orange_fill\n",
        "\n",
        "        wb.save(styled_path)\n",
        "        gcolab_files.download(styled_path)\n",
        "\n",
        "    # --- Final logs ---\n",
        "    if flagged_log:\n",
        "        print(\"\\n[Flagged entries]\")\n",
        "        for fed_str, mouse_id, reason, included in flagged_log:\n",
        "            print(f\" - FED{fed_str} (Mouse_ID: {mouse_id}) -> {'Included' if included else 'Rejected'}: {reason}\")\n",
        "\n",
        "    if renamed_files and zip_name:\n",
        "        print(f\"\\n[Summary] Renamed {len(renamed_files)} file(s); zipped to {zip_name}.\")\n",
        "    else:\n",
        "        print(f\"\\n[Summary] Renamed {len(renamed_files)} file(s); no zip created.\")\n",
        "\n",
        "    return {\n",
        "        \"no_file_log\": no_file_log,\n",
        "        \"flagged_log\": flagged_log,\n",
        "        \"renamed_files\": renamed_files,\n",
        "        \"zip_name\": zip_name,\n",
        "        \"key_modified\": key_modified,\n",
        "        \"sessions\": sorted(session_types_present),\n",
        "    }\n",
        "res = process_key_and_rename(\n",
        "    key_file=uploaded_key_path,\n",
        "    approved_indices=approved_indices,\n",
        "    feds=feds,\n",
        "    loaded_files=loaded_files,\n",
        "    manually_rejected_indices=manually_rejected_indices,\n",
        "    override_indices=override_indices,\n",
        "    flagged_reasons=flagged_reasons\n",
        ")\n",
        "\n",
        "print(f\"[RESULT] no_file_log: {len(res['no_file_log'])}, flagged_log: {len(res['flagged_log'])}, \"\n",
        "      f\"renamed: {len(res['renamed_files'])}, sessions: {res['sessions']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}