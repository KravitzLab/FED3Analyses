{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/FED3Analyses/blob/main/FED3_PR1_V1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FED3 PR1 analysis\n",
        "<br>\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqe_1a1j1bQaqhOxq0VvukPqfolLRUqOdl-g&s\" width=\"200\" />\n",
        "\n",
        "Updated: 01.12.25  \n",
        "Version 1.1.0\n",
        "Authored by Chantelle Murrell and Sebastian Alves\n",
        "\n"
      ],
      "metadata": {
        "id": "hEm7saj7IRD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install libraries and import them {\"run\":\"auto\"}\n",
        "\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Packages to ensure are installed (add others here if you like)\n",
        "packages = {\n",
        "    \"fed3\": \"git+https://github.com/earnestt1234/fed3.git\",\n",
        "    \"fed3bandit\": \"fed3bandit\",\n",
        "    \"pingouin\": \"pingouin\",\n",
        "    \"ipydatagrid\": \"ipydatagrid\",\n",
        "    \"openpyxl\": \"openpyxl\",\n",
        "}\n",
        "\n",
        "for name, source in packages.items():\n",
        "    if importlib.util.find_spec(name) is None:\n",
        "        print(f\"Installing {name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", source])\n",
        "\n",
        "# ----------------------------\n",
        "# Imports\n",
        "# ----------------------------\n",
        "# Standard library\n",
        "import copy\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tempfile\n",
        "import threading\n",
        "import time\n",
        "import warnings\n",
        "import zipfile\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "from os.path import basename, splitext\n",
        "\n",
        "# Third-party\n",
        "from ipydatagrid import DataGrid, TextRenderer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "import fed3\n",
        "import fed3.plot as fplot\n",
        "import fed3bandit as f3b\n",
        "from scipy.stats import f_oneway\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.gridspec as gridspec\n",
        "from google.colab import files\n",
        "from tqdm.auto import tqdm\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration\n",
        "# ----------------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.rcParams.update({'font.size': 12, 'figure.autolayout': True})\n",
        "plt.rcParams['figure.figsize'] = [6, 4]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "print(\"Packages installed and imports ready.\")\n"
      ],
      "metadata": {
        "id": "k-YGEmW-KCNK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload files\n",
        "\n",
        "# Reset caches to avoid duplicates if you re-run this cell\n",
        "feds, loaded_files, session_types = [], [], []\n",
        "\n",
        "def extract_session_type(csv_path, fallback=\"Unknown\"):\n",
        "    \"\"\"Read 'Session_Type ' or variants; return first non-empty value.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, sep=None, engine='python', dtype=str)\n",
        "        df.columns = [c.strip() for c in df.columns]\n",
        "        lower = {c.casefold(): c for c in df.columns}\n",
        "        for cand in [\"session_type\", \"session type\", \"sessiontype\", \"session\"]:\n",
        "            if cand in lower:\n",
        "                col = lower[cand]\n",
        "                vals = df[col].dropna().astype(str).str.strip()\n",
        "                vals = vals[vals.ne(\"\")]\n",
        "                if not vals.empty:\n",
        "                    return vals.iloc[0]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return fallback\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for name, data in uploaded.items():\n",
        "    if name.lower().endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(io.BytesIO(data)) as zf:\n",
        "            for zi in zf.infolist():\n",
        "                if not zi.filename.lower().endswith(\".csv\"):\n",
        "                    continue\n",
        "                file_data = zf.read(zi)\n",
        "                if len(file_data) <= 1024:\n",
        "                    continue\n",
        "                with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".csv\", delete=False) as tmp:\n",
        "                    tmp.write(file_data); tmp_path = tmp.name\n",
        "                try:\n",
        "                    session_type = extract_session_type(tmp_path)\n",
        "                    df = fed3.load(tmp_path)\n",
        "                    df.name = os.path.basename(zi.filename)\n",
        "                    df.attrs = {\"Session_type\": session_type}\n",
        "                    feds.append(df)\n",
        "                    loaded_files.append(os.path.basename(zi.filename))\n",
        "                    session_types.append(session_type)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {zi.filename}: {e}\")\n",
        "                finally:\n",
        "                    os.remove(tmp_path)\n",
        "    elif name.lower().endswith(\".csv\"):\n",
        "        if len(data) <= 1024:\n",
        "            continue\n",
        "        with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".csv\", delete=False) as tmp:\n",
        "            tmp.write(data); tmp_path = tmp.name\n",
        "        try:\n",
        "            session_type = extract_session_type(tmp_path)\n",
        "            df = fed3.load(tmp_path)\n",
        "            df.name = os.path.basename(name)\n",
        "            df.attrs = {\"Session_type\": session_type}\n",
        "            feds.append(df)\n",
        "            loaded_files.append(os.path.basename(name))\n",
        "            session_types.append(session_type)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {name}: {e}\")\n",
        "        finally:\n",
        "            os.remove(tmp_path)\n",
        "\n",
        "print(f\"Loaded {len(loaded_files)} files. Session types captured for all.\")\n",
        "# Optional quick plot\n",
        "if feds:\n",
        "    try:\n",
        "        fed3.as_aligned(feds, alignment=\"datetime\", inplace=True)\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        fplot.line(feds, y='pellets'); plt.legend().remove(); plt.tight_layout(); plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Plotting skipped: {e}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TONj_5IvKdNs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Build Key\n",
        "\n",
        "import os, glob, io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ipydatagrid import DataGrid, TextRenderer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from google.colab import files as colab_files\n",
        "from google.colab import output as colab_output\n",
        "\n",
        "# Require that the file-upload cell has already populated these:\n",
        "assert 'loaded_files' in globals() and 'session_types' in globals(), \\\n",
        "    \"Run the 'Upload FED3 files' cell first.\"\n",
        "\n",
        "colab_output.enable_custom_widget_manager()\n",
        "\n",
        "# ---------------------------\n",
        "# Base: bare-bones Key_Df from loaded data\n",
        "# ---------------------------\n",
        "def _make_base_key_df():\n",
        "    return pd.DataFrame({\"filename\": loaded_files, \"Session_type\": session_types})\n",
        "\n",
        "def _file_base(s):\n",
        "    return os.path.splitext(os.path.basename(str(s)))[0].strip()\n",
        "\n",
        "def _norm_base_lower(s):\n",
        "    return _file_base(s).lower()\n",
        "\n",
        "# ---------------------------\n",
        "# Key scanner: detect Mouse_ID or filename columns\n",
        "# ---------------------------\n",
        "def _scan_key_columns(df):\n",
        "    \"\"\"\n",
        "    Returns dict:\n",
        "      {\n",
        "        'has_mouse': bool,\n",
        "        'has_filename': bool,\n",
        "        'filename_col': 'filename'|'File'|None,\n",
        "        'msg': str\n",
        "      }\n",
        "    Accepts keys that have either Mouse_ID or a filename column (filename/File).\n",
        "    \"\"\"\n",
        "    info = {'has_mouse': False, 'has_filename': False, 'filename_col': None, 'msg': ''}\n",
        "    try:\n",
        "        cols = [str(c).strip() for c in df.columns]\n",
        "        has_mouse = 'Mouse_ID' in cols\n",
        "        fname_col = 'filename' if 'filename' in cols else ('File' if 'File' in cols else None)\n",
        "        info.update({\n",
        "            'has_mouse': has_mouse,\n",
        "            'has_filename': fname_col is not None,\n",
        "            'filename_col': fname_col\n",
        "        })\n",
        "        if has_mouse:\n",
        "            info['msg'] = \"'Mouse_ID' found.\"\n",
        "        elif fname_col:\n",
        "            info['msg'] = f\"'{fname_col}' found; will match on filename.\"\n",
        "        else:\n",
        "            info['msg'] = \"Neither 'Mouse_ID' nor 'filename'/'File' found in provided key.\"\n",
        "    except Exception as e:\n",
        "        info['msg'] = f\"Error while checking key: {e}\"\n",
        "    return info\n",
        "\n",
        "# ---------------------------\n",
        "# Read uploaded key (CSV/XLSX), accept Mouse_ID or filename\n",
        "# ---------------------------\n",
        "def _read_key_from_upload(name, content_bytes):\n",
        "    \"\"\"Return (df_or_None, message). Reads CSV/XLSX bytes from Colab upload.\"\"\"\n",
        "    ext = name.lower().rsplit('.', 1)[-1] if '.' in name else ''\n",
        "    try:\n",
        "        bio = io.BytesIO(content_bytes)\n",
        "        if ext == 'xlsx':\n",
        "            xls = pd.ExcelFile(bio, engine='openpyxl')\n",
        "            frames = [pd.read_excel(xls, sheet_name=s) for s in xls.sheet_names]\n",
        "            key_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        elif ext == 'csv':\n",
        "            key_df = pd.read_csv(bio, sep=None, engine='python')\n",
        "        else:\n",
        "            return None, f\"Unsupported key type .{ext}\"\n",
        "\n",
        "        key_df = key_df.copy()\n",
        "        key_df.columns = [str(c).strip() for c in key_df.columns]\n",
        "        scan = _scan_key_columns(key_df)\n",
        "        if not (scan['has_mouse'] or scan['has_filename']):\n",
        "            return None, scan['msg']\n",
        "\n",
        "        # Normalize types/columns we might use later\n",
        "        if scan['has_mouse']:\n",
        "            globals()['KEY_MATCH_MODE'] = 'mouse_id'\n",
        "            key_df['Mouse_ID'] = key_df['Mouse_ID'].astype(str).str.strip()\n",
        "\n",
        "        if scan['has_filename']:\n",
        "            globals()['KEY_MATCH_MODE'] = 'filename'\n",
        "            fcol = scan['filename_col']\n",
        "            key_df[fcol] = key_df[fcol].astype(str).str.strip()\n",
        "            key_df['_key_file_base_lower'] = key_df[fcol].map(_norm_base_lower)\n",
        "        else:\n",
        "            globals()['KEY_MATCH_MODE'] = None\n",
        "\n",
        "        # Persist a deterministic copy on disk for reproducibility\n",
        "        fixed_path = f\"_uploaded_key.{ext}\"\n",
        "        with open(fixed_path, \"wb\") as f:\n",
        "            f.write(content_bytes)\n",
        "        globals()['uploaded_key_path'] = fixed_path\n",
        "\n",
        "        return key_df, f\"Key loaded from upload ({name}) and saved to {fixed_path}. {scan['msg']}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error reading uploaded key: {e}\"\n",
        "\n",
        "# ---------------------------\n",
        "# Matching filename <-> Mouse_ID\n",
        "# ---------------------------\n",
        "def _match_mouse_id_to_filenames(filenames, key_df):\n",
        "    \"\"\"Return DataFrame: filename, Mouse_ID, match_status based on Mouse_ID substring in filename.\"\"\"\n",
        "    base_names_lower = [_norm_base_lower(f) for f in filenames]\n",
        "    mouse_ids = (\n",
        "        key_df['Mouse_ID']\n",
        "        .dropna().astype(str).map(str.strip)\n",
        "        .replace({'': np.nan}).dropna().unique().tolist()\n",
        "    )\n",
        "    rows = []\n",
        "    for fname, base in zip(filenames, base_names_lower):\n",
        "        hits = [mid for mid in mouse_ids if str(mid).lower() in base]\n",
        "        if len(hits) == 1:\n",
        "            rows.append({\"filename\": fname, \"Mouse_ID\": hits[0], \"match_status\": \"Matched (Mouse_ID in filename)\"})\n",
        "        elif len(hits) > 1:\n",
        "            longest = max(len(str(h)) for h in hits)\n",
        "            best = [h for h in hits if len(str(h)) == longest]\n",
        "            if len(best) == 1:\n",
        "                rows.append({\"filename\": fname, \"Mouse_ID\": best[0], \"match_status\": \"Matched (longest Mouse_ID token)\"})\n",
        "            else:\n",
        "                rows.append({\"filename\": fname, \"Mouse_ID\": None, \"match_status\": f\"Ambiguous Mouse_ID: {hits}\"})\n",
        "        else:\n",
        "            rows.append({\"filename\": fname, \"Mouse_ID\": None, \"match_status\": \"Mouse_ID not found in filename\"})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------------------------\n",
        "# Build/Rematch function\n",
        "# ---------------------------\n",
        "status_box = widgets.Output()\n",
        "Key_Df = _make_base_key_df()  # start bare-bones\n",
        "\n",
        "def _collapse_key_suffixes(df, keep_filename_from_base=True):\n",
        "    \"\"\"\n",
        "    Remove duplicate columns created by merge suffixes.\n",
        "    - If both 'col' and 'col_key' exist, keep the key version (rename col_key -> col, drop base col).\n",
        "    - Special-case: if keep_filename_from_base and base == 'filename',\n",
        "      drop 'filename_key' and keep the base 'filename'.\n",
        "    \"\"\"\n",
        "    cols = list(df.columns)\n",
        "    rename_map = {}\n",
        "    drop_cols = []\n",
        "\n",
        "    for col in cols:\n",
        "        if col.endswith('_key'):\n",
        "            base = col[:-4]\n",
        "            if keep_filename_from_base and base == 'filename':\n",
        "                # keep the 'filename' from the bare-bones (actual loaded_files)\n",
        "                drop_cols.append(col)\n",
        "            elif base in df.columns:\n",
        "                # key version wins: drop base col, rename *_key -> base\n",
        "                drop_cols.append(base)\n",
        "                rename_map[col] = base\n",
        "            else:\n",
        "                # no base column; just strip \"_key\"\n",
        "                rename_map[col] = base\n",
        "\n",
        "    df = df.drop(columns=drop_cols, errors='ignore').rename(columns=rename_map)\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_or_rematch_key_df(key_df=None, msg_hint=\"\"):\n",
        "    \"\"\"\n",
        "    If key_df provided and valid:\n",
        "      - If a filename column exists (filename/File), always use filename-based merge.\n",
        "        - If Mouse_ID is also present, it is preserved from the key file.\n",
        "      - Else, if only Mouse_ID exists, use substring-based Mouse_ID matching.\n",
        "    Else: keep bare-bones.\n",
        "\n",
        "    When a key is used, any duplicate column names between the bare-bones\n",
        "    and the key are resolved so that the key's values win (except 'filename').\n",
        "    \"\"\"\n",
        "    global Key_Df\n",
        "    files_df = _make_base_key_df().copy()\n",
        "    files_df['_file_base_lower'] = files_df['filename'].map(_norm_base_lower)\n",
        "\n",
        "    if key_df is None:\n",
        "        # Pure bare-bones Key_Df\n",
        "        Key_Df = files_df.drop(columns=['_file_base_lower']).copy()\n",
        "        Key_Df[\"match_status\"] = \"No key\"\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Key status: No key provided; showing bare-bones Key_Df.\")\n",
        "        return\n",
        "\n",
        "    # Identify key capabilities\n",
        "    scan = _scan_key_columns(key_df)\n",
        "    kd = key_df.copy()\n",
        "\n",
        "    # Helper: make a unique version of the key for whichever join we use\n",
        "    def _dedup(df, subset_cols):\n",
        "        dup_counts = df[subset_cols].astype(str).agg('|'.join, axis=1).value_counts()\n",
        "        n_dups = int((dup_counts > 1).sum())\n",
        "        if n_dups:\n",
        "            with status_box:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Note: {n_dups} duplicate key(s) on {subset_cols}; taking the first occurrence.\")\n",
        "        return df.drop_duplicates(subset=subset_cols, keep=\"first\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # 1) Prefer filename-based route whenever filename column exists\n",
        "    #    (whether or not Mouse_ID is present)\n",
        "    # ---------------------------\n",
        "    if scan['has_filename']:\n",
        "        fcol = scan['filename_col']\n",
        "\n",
        "        # Normalize filename column in key\n",
        "        kd[fcol] = kd[fcol].astype(str).str.strip()\n",
        "        kd['_key_file_base_lower'] = kd[fcol].map(_norm_base_lower)\n",
        "\n",
        "        # One row per normalized filename in the key\n",
        "        key_unique = _dedup(kd, ['_key_file_base_lower'])\n",
        "\n",
        "        # Merge by normalized basename (case-insensitive, extension-stripped)\n",
        "        Key_Df = (\n",
        "            files_df\n",
        "            .merge(\n",
        "                key_unique,\n",
        "                left_on=\"_file_base_lower\",\n",
        "                right_on=\"_key_file_base_lower\",\n",
        "                how=\"left\",\n",
        "                suffixes=(\"\", \"_key\")\n",
        "            )\n",
        "            .drop(columns=['_file_base_lower', '_key_file_base_lower'])\n",
        "        )\n",
        "\n",
        "        # Clean up duplicate columns so key overwrites bare-bones where appropriate\n",
        "        Key_Df = _collapse_key_suffixes(Key_Df, keep_filename_from_base=True)\n",
        "\n",
        "        # match_status for filename-based matching\n",
        "        Key_Df[\"match_status\"] = np.where(\n",
        "            Key_Df[fcol].notna(),\n",
        "            \"Matched (filename)\",\n",
        "            \"Filename not found in key\"\n",
        "        )\n",
        "\n",
        "    # ---------------------------\n",
        "    # 2) If no filename column but Mouse_ID exists, use substring route\n",
        "    # ---------------------------\n",
        "    elif scan['has_mouse']:\n",
        "        # Mouse_ID route (fallback when no filename column exists)\n",
        "        kd['Mouse_ID'] = kd['Mouse_ID'].astype(str).str.strip()\n",
        "        key_unique = _dedup(kd, ['Mouse_ID'])\n",
        "\n",
        "        matched = _match_mouse_id_to_filenames(\n",
        "            files_df['filename'].tolist(),\n",
        "            key_unique\n",
        "        )[[\"filename\", \"Mouse_ID\", \"match_status\"]]\n",
        "\n",
        "        Key_Df = (\n",
        "            files_df\n",
        "            .merge(matched, on=\"filename\", how=\"left\")\n",
        "            .merge(key_unique, on=\"Mouse_ID\", how=\"left\", suffixes=(\"\", \"_key\"))\n",
        "            .drop(columns=['_file_base_lower'])\n",
        "        )\n",
        "\n",
        "        # Clean up duplicate columns (e.g., Mouse_ID vs Mouse_ID_key)\n",
        "        Key_Df = _collapse_key_suffixes(Key_Df, keep_filename_from_base=True)\n",
        "\n",
        "    # ---------------------------\n",
        "    # 3) Neither filename nor Mouse_ID in key\n",
        "    # ---------------------------\n",
        "    else:\n",
        "        # Neither route available (shouldn't happen due to earlier check)\n",
        "        Key_Df = files_df.drop(columns=['_file_base_lower']).copy()\n",
        "        Key_Df[\"match_status\"] = \"Key missing Mouse_ID and filename columns\"\n",
        "\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        if msg_hint:\n",
        "            print(msg_hint)\n",
        "        print(f\"Merged key columns into Key_Df ({len(Key_Df)} rows, {len(Key_Df.columns)} cols).\")\n",
        "\n",
        "# ---------------------------\n",
        "# Grid UI\n",
        "# ---------------------------\n",
        "def make_grid(df: pd.DataFrame):\n",
        "    g = DataGrid(\n",
        "        df,\n",
        "        editable=True,\n",
        "        selection_mode='cell',\n",
        "        layout={'height': '420px'},\n",
        "        base_row_size=28,\n",
        "        base_column_size=120,\n",
        "    )\n",
        "    g.default_renderer = TextRenderer(text_wrap=True)\n",
        "    return g\n",
        "\n",
        "def rebuild_grid(msg=\"\"):\n",
        "    global grid, ui\n",
        "    df = Key_Df.copy().reset_index(drop=True)\n",
        "    new_grid = make_grid(df)\n",
        "    ui.children = (upload_row, new_grid, controls, status_box)\n",
        "    grid = new_grid\n",
        "    with status_box:\n",
        "        if msg:\n",
        "            print(msg)\n",
        "        print(f\"Grid now shows Key_Df ({len(df)} rows, {len(df.columns)} cols)\")\n",
        "\n",
        "# ---------------------------\n",
        "# Colab-native upload button ONLY (no path UI)\n",
        "# ---------------------------\n",
        "upload_btn = widgets.Button(description=\"Upload\", button_style=\"primary\", layout=widgets.Layout(width=\"120px\"))\n",
        "reset_btn = widgets.Button(description=\"Reset Key\", button_style=\"warning\", layout=widgets.Layout(width=\"120px\"))\n",
        "download_button = widgets.Button(description='Download', button_style='success', layout=widgets.Layout(width=\"120px\"))\n",
        "\n",
        "def on_colab_upload(_):\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Opening Colab upload dialog...\")\n",
        "    uploaded = colab_files.upload()  # opens the native Colab picker\n",
        "    if not uploaded:\n",
        "        with status_box:\n",
        "            print(\"No file selected.\")\n",
        "        return\n",
        "    name, content = next(iter(uploaded.items()))\n",
        "    key_df, msg = _read_key_from_upload(name, content)\n",
        "    if key_df is None:\n",
        "        build_or_rematch_key_df(None)\n",
        "        rebuild_grid(f\"Key status: {msg}\")\n",
        "    else:\n",
        "        build_or_rematch_key_df(key_df, msg_hint=f\"Key status: {msg}\")\n",
        "        rebuild_grid()\n",
        "\n",
        "def _load_saved_key_from_disk():\n",
        "    \"\"\"Returns (key_df_or_None, message) from uploaded_key_path if present/valid.\"\"\"\n",
        "    key_path = globals().get('uploaded_key_path', None)\n",
        "    if not (key_path and os.path.exists(key_path)):\n",
        "        return None, \"No saved key on disk to reload.\"\n",
        "    try:\n",
        "        ext = key_path.lower().rsplit('.', 1)[-1] if '.' in key_path else ''\n",
        "        if ext == 'xlsx':\n",
        "            xls = pd.ExcelFile(key_path, engine='openpyxl')\n",
        "            frames = [pd.read_excel(xls, sheet_name=s) for s in xls.sheet_names]\n",
        "            key_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        elif ext == 'csv':\n",
        "            key_df = pd.read_csv(key_path, sep=None, engine='python')\n",
        "        else:\n",
        "            return None, f\"Unsupported key type .{ext}\"\n",
        "\n",
        "        key_df = key_df.copy()\n",
        "        key_df.columns = [str(c).strip() for c in key_df.columns]\n",
        "        scan = _scan_key_columns(key_df)\n",
        "        if not (scan['has_mouse'] or scan['has_filename']):\n",
        "            return None, scan['msg']\n",
        "\n",
        "        if scan['has_mouse']:\n",
        "            key_df['Mouse_ID'] = key_df['Mouse_ID'].astype(str).str.strip()\n",
        "        if scan['has_filename']:\n",
        "            fcol = scan['filename_col']\n",
        "            key_df[fcol] = key_df[fcol].astype(str).str.strip()\n",
        "            key_df['_key_file_base_lower'] = key_df[fcol].map(_norm_base_lower)\n",
        "\n",
        "        return key_df, f\"Key reloaded from {os.path.basename(key_path)}. {scan['msg']}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error reading saved uploaded key: {e}\"\n",
        "\n",
        "def on_reset(_):\n",
        "    \"\"\"\n",
        "    HARD RESET: forget any saved key and show bare-bones Key_Df.\n",
        "    Deletes _uploaded_key.(xlsx|csv) if present and clears uploaded_key_path.\n",
        "    \"\"\"\n",
        "    # 1) Forget path in memory\n",
        "    globals().pop('uploaded_key_path', None)\n",
        "\n",
        "    # 2) Remove any persisted key files from disk\n",
        "    import os\n",
        "    for ext in (\"xlsx\", \"csv\"):\n",
        "        try:\n",
        "            os.remove(f\"_uploaded_key.{ext}\")\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    # 3) Show bare-bones\n",
        "    build_or_rematch_key_df(None)\n",
        "    rebuild_grid(\"Reset: cleared saved key; showing bare-bones Key_Df.\")\n",
        "\n",
        "def download_df(_):\n",
        "    global Key_Df\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Saving latest Key_Df as XLSX ...\")\n",
        "    try:\n",
        "        path = \"/content/Key_Df.xlsx\"\n",
        "        Key_Df.to_excel(path, index=False, engine='openpyxl')\n",
        "        colab_files.download(path)\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Saved and downloading Key_Df.xlsx ...\")\n",
        "    except Exception as e:\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Error while saving/downloading: {e}\")\n",
        "\n",
        "upload_btn.on_click(on_colab_upload)\n",
        "reset_btn.on_click(on_reset)\n",
        "download_button.on_click(download_df)\n",
        "\n",
        "upload_row = widgets.HBox([\n",
        "    widgets.HTML(\"<b>Optional key:</b>\"),\n",
        "    upload_btn,\n",
        "    reset_btn,\n",
        "    download_button\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# Edit / rematch / download controls\n",
        "# ---------------------------\n",
        "new_col_name    = widgets.Text(placeholder='Enter new column name', description='New Col:')\n",
        "add_col_button  = widgets.Button(description='Add Column', button_style='info')\n",
        "apply_button    = widgets.Button(description='Apply Changes', button_style='primary', layout=widgets.Layout(width=\"120px\"))\n",
        "\n",
        "def add_column(_):\n",
        "    global Key_Df\n",
        "    col = new_col_name.value.strip()\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        if not col:\n",
        "            print(\"Please enter a column name.\"); return\n",
        "        if col in Key_Df.columns:\n",
        "            print(f\"Column '{col}' already exists.\"); return\n",
        "        Key_Df[col] = \"\"\n",
        "        print(f\"Added column '{col}' to Key_Df.\")\n",
        "    rebuild_grid()\n",
        "\n",
        "def apply_edits(_):\n",
        "    global Key_Df\n",
        "    try:\n",
        "        Key_Df = grid.data.copy().reset_index(drop=True)\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Applied grid edits to Key_Df ({len(Key_Df)} rows, {len(Key_Df.columns)} cols).\")\n",
        "    except Exception as e:\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Error applying edits: {e}\")\n",
        "\n",
        "add_col_button.on_click(add_column)\n",
        "apply_button.on_click(apply_edits)\n",
        "\n",
        "controls = widgets.HBox([new_col_name, add_col_button, apply_button])\n",
        "\n",
        "# ---------------------------\n",
        "# Initialize UI\n",
        "# ---------------------------\n",
        "Key_Df = _make_base_key_df()\n",
        "Key_Df[\"match_status\"] = \"No key\"\n",
        "\n",
        "grid = make_grid(Key_Df.copy().reset_index(drop=True))\n",
        "ui = widgets.VBox([upload_row, grid, controls, status_box])\n",
        "display(ui)"
      ],
      "metadata": {
        "id": "_6ugA24nNdd2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Individual plots\n",
        "\n",
        "assert 'Key_Df' in globals() and isinstance(Key_Df, pd.DataFrame), \"Build/rematch Key_Df first.\"\n",
        "\n",
        "# metadata_df = copy of Key_Df\n",
        "metadata_df = Key_Df.copy().reset_index(drop=True)\n",
        "if 'filename' in metadata_df.columns:\n",
        "    metadata_df['filename'] = metadata_df['filename'].astype(str).map(os.path.basename)\n",
        "\n",
        "# -------- Plotting Function --------\n",
        "def plot_file(file_index):\n",
        "    \"\"\"\n",
        "    Returns (fig, filename) for UI to display/save.\n",
        "    Prints a message and returns (None, None) on errors.\n",
        "    \"\"\"\n",
        "    # Safety check\n",
        "    if 'feds' not in globals() or file_index >= len(feds):\n",
        "        print(f\"Index {file_index} is out of range (max {len(feds)-1 if 'feds' in globals() else 'N/A'}).\")\n",
        "        return None, None\n",
        "\n",
        "    df = feds[file_index]\n",
        "    # Keep only pellet rows, but don't mutate df\n",
        "    pellet_df = df[df['Event'] == 'Pellet'].copy()\n",
        "    if pellet_df.empty:\n",
        "        print(f\"No pellet events for file index {file_index}.\")\n",
        "        return None, None\n",
        "\n",
        "    # Determine filename for title/return\n",
        "    if 'loaded_files' in globals() and len(loaded_files) > file_index:\n",
        "        raw_file = loaded_files[file_index]\n",
        "    elif 'files' in globals() and len(files) > file_index:\n",
        "        raw_file = files[file_index]\n",
        "    else:\n",
        "        raw_file = f\"file_{file_index}\"\n",
        "    filename = os.path.basename(str(raw_file))\n",
        "\n",
        "    # Match Mouse_ID from metadata, robust to path vs basename\n",
        "    title_str = filename\n",
        "    if 'filename' in metadata_df.columns:\n",
        "        md_fn = metadata_df['filename'].astype(str).map(os.path.basename)\n",
        "        hit = metadata_df.loc[md_fn == filename]\n",
        "        if not hit.empty:\n",
        "            match_row = hit.iloc[0]\n",
        "            if 'Mouse_ID' in match_row and pd.notna(match_row['Mouse_ID']):\n",
        "                title_str = f\"{match_row['Mouse_ID']}\"\n",
        "\n",
        "    # Determine x-axis\n",
        "    x_series = None\n",
        "    x_is_datetime = isinstance(pellet_df.index, pd.DatetimeIndex)\n",
        "    if x_is_datetime:\n",
        "        x_series = pellet_df.index\n",
        "    else:\n",
        "        for col in ['Timestamp', 'Time', 'DateTime', 'Datetime', 'datetime']:\n",
        "            if col in pellet_df.columns:\n",
        "                ts = pd.to_datetime(pellet_df[col], errors='coerce')\n",
        "                if ts.notna().any():\n",
        "                    pellet_df['_x'] = ts\n",
        "                    x_series = pellet_df['_x']\n",
        "                    x_is_datetime = True\n",
        "                    break\n",
        "        if x_series is None:\n",
        "            x_series = pellet_df.index  # fall back to index\n",
        "\n",
        "    # Build figure (DO NOT show here)\n",
        "    fig = plt.figure(figsize=(7, 5))\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    hue_vals = pellet_df['Block_Pellet_Count'].clip(upper=40)\n",
        "    sns.scatterplot(\n",
        "        data=pellet_df,\n",
        "        x=x_series,\n",
        "        y='Block_Pellet_Count',\n",
        "        hue=hue_vals,\n",
        "        palette='spring',\n",
        "        alpha=0.6,\n",
        "        legend=False,\n",
        "        ax=ax\n",
        "    )\n",
        "\n",
        "    # Night shading if datetime x\n",
        "    if x_is_datetime:\n",
        "        import datetime as dt\n",
        "        night_start = dt.time(18, 0)  # 18:00\n",
        "        night_end   = dt.time(6, 0)   # 06:00\n",
        "\n",
        "        start_date = pd.to_datetime(pd.Series(x_series)).min().normalize()\n",
        "        end_date   = pd.to_datetime(pd.Series(x_series)).max().normalize()\n",
        "\n",
        "        # shade from each day's 18:00 to next day's 06:00\n",
        "        for day in pd.date_range(start_date, end_date - pd.Timedelta(days=1)):\n",
        "            start = pd.Timestamp.combine(day, night_start)\n",
        "            end   = pd.Timestamp.combine(day + pd.Timedelta(days=1), night_end)\n",
        "            ax.axvspan(start, end, color='gray', alpha=0.2)\n",
        "\n",
        "        import matplotlib.dates as mdates\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
        "        ax.set_xlabel('')\n",
        "    else:\n",
        "        ax.set_xlabel('Index')\n",
        "    ax.set_ylabel('Pellet Count in Block')\n",
        "    ax.set_title(title_str)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    # Return for UI to display/save later\n",
        "    return fig, filename\n",
        "\n",
        "# ---------- UI: single instance ----------\n",
        "demand_ui = {}\n",
        "\n",
        "def _sanitize_filename(s: str) -> str:\n",
        "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", s)\n",
        "\n",
        "# Slider bounds from number of feds (not files)\n",
        "n_items = len(feds) if 'feds' in globals() else 1\n",
        "slider = widgets.IntSlider(\n",
        "    min=0, max=max(0, n_items-1), step=1, value=0,\n",
        "    description=\"File\", continuous_update=True\n",
        ")\n",
        "save_btn = widgets.Button(description=\"Save PDF\")\n",
        "status = widgets.HTML()\n",
        "out = widgets.Output()\n",
        "\n",
        "def _update_plot(change=None):\n",
        "    with out:\n",
        "        out.clear_output(wait=True)\n",
        "        fig, file = plot_file(slider.value)\n",
        "        if fig is None:\n",
        "            status.value = \"<span style='color:#b00'>No figure to display.</span>\"\n",
        "            return\n",
        "        display(fig)      # show exactly one figure\n",
        "        plt.close(fig)    # close backend handle to prevent accumulation\n",
        "        demand_ui[\"_last_fig\"] = fig\n",
        "        demand_ui[\"_last_file\"] = file\n",
        "        status.value = \"\" # clear old status\n",
        "\n",
        "def _save_pdf(_):\n",
        "    fig = demand_ui.get(\"_last_fig\")\n",
        "    file = demand_ui.get(\"_last_file\", \"figure\")\n",
        "    if fig is None:\n",
        "        status.value = \"<span style='color:#b00'>No figure to save.</span>\"\n",
        "        return\n",
        "    base = _sanitize_filename(os.path.splitext(os.path.basename(str(file)))[0])\n",
        "    fname = f\"demand_{base}_{datetime.now():%Y%m%d_%H%M%S}.pdf\"\n",
        "    fig.savefig(fname, format=\"pdf\", bbox_inches=\"tight\")\n",
        "    if 'gfiles' in globals() and gfiles is not None:\n",
        "        status.value = f\"Preparing download: <code>{fname}</code>…\"\n",
        "        try:\n",
        "            gfiles.download(fname)\n",
        "        except Exception as e:\n",
        "            status.value = f\"Saved locally at <code>{os.path.abspath(fname)}</code> (download helper failed: {e}).\"\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{os.path.abspath(fname)}</code>.\"\n",
        "\n",
        "slider.observe(_update_plot, names=\"value\")\n",
        "save_btn.on_click(_save_pdf)\n",
        "\n",
        "# Keep references so we can close them next run\n",
        "box = widgets.HBox([slider, save_btn])\n",
        "demand_ui.update({\"slider\": slider, \"save_btn\": save_btn, \"status\": status, \"out\": out, \"box\": box})\n",
        "\n",
        "# Display the controls and initial plot\n",
        "display(box, status, out)\n",
        "_update_plot()\n"
      ],
      "metadata": {
        "id": "PCXLxmMa-QXD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Analyse PR metrics (clean Bandit-style metadata_df + merge/export)\n",
        "\n",
        "import os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import curve_fit\n",
        "from IPython.display import display, clear_output, HTML, FileLink\n",
        "import ipywidgets as widgets\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm as _tqdm\n",
        "except Exception:\n",
        "    def _tqdm(x, **kwargs): return x\n",
        "\n",
        "# --------------------\n",
        "# Preconditions\n",
        "# --------------------\n",
        "assert 'feds' in globals() and isinstance(feds, (list, tuple)) and len(feds) > 0, \"No FED3 sessions loaded (feds).\"\n",
        "assert 'Key_Df' in globals() and isinstance(Key_Df, pd.DataFrame), \"Build/rematch Key_Df first.\"\n",
        "\n",
        "# --------------------\n",
        "# Helpers (match Bandit)\n",
        "# --------------------\n",
        "def _clean_colname(c):\n",
        "    c = str(c).strip()\n",
        "    c = c.replace(\"$\", \"\")\n",
        "    c = re.sub(r\"\\s+\", \"_\", c)\n",
        "    c = re.sub(r\"_+\", \"_\", c)\n",
        "    return c\n",
        "\n",
        "def _basename(pathlike) -> str:\n",
        "    s = str(pathlike).replace(\"\\\\\", \"/\")\n",
        "    return s.split(\"/\")[-1]\n",
        "\n",
        "def _file_base_lower(pathlike):\n",
        "    return os.path.splitext(os.path.basename(str(pathlike)))[0].lower()\n",
        "\n",
        "def _to_num(s):\n",
        "    return pd.to_numeric(s, errors=\"coerce\")\n",
        "\n",
        "def _count_events(df, label: str) -> int:\n",
        "    if \"Event\" not in df.columns:\n",
        "        return 0\n",
        "    return int((df[\"Event\"].astype(str) == label).sum())\n",
        "\n",
        "def _get_timestamp_series(df, ts_col=\"MM:DD:YYYY hh:mm:ss\"):\n",
        "    \"\"\"\n",
        "    Match Bandit: FED3 default timestamp format is '%m:%d:%Y %H:%M:%S'\n",
        "    \"\"\"\n",
        "    if ts_col in df.columns:\n",
        "        ts = pd.to_datetime(df[ts_col], format=\"%m:%d:%Y %H:%M:%S\", errors=\"coerce\")\n",
        "        return pd.Series(ts, index=df.index)\n",
        "    for cand in [\"DateTime\", \"Datetime\", \"Timestamp\", \"timestamp\", \"datetime\"]:\n",
        "        if cand in df.columns:\n",
        "            ts = pd.to_datetime(df[cand], errors=\"coerce\")\n",
        "            return pd.Series(ts, index=df.index)\n",
        "    idx = df.index\n",
        "    if isinstance(idx, pd.DatetimeIndex):\n",
        "        return pd.Series(idx, index=df.index)\n",
        "    return pd.to_datetime(pd.Series(idx, index=df.index), errors=\"coerce\")\n",
        "\n",
        "def _estimate_daily_pellets(df):\n",
        "    ts = _get_timestamp_series(df)\n",
        "    valid_ts = ts.dropna()\n",
        "    if valid_ts.size < 2:\n",
        "        return np.nan\n",
        "    duration_hours = (valid_ts.max() - valid_ts.min()).total_seconds() / 3600.0\n",
        "    if duration_hours <= 0:\n",
        "        return np.nan\n",
        "\n",
        "    pellet_events = np.nan\n",
        "    if \"Pellet_Count\" in df.columns and df[\"Pellet_Count\"].notna().any():\n",
        "        pc = pd.to_numeric(df[\"Pellet_Count\"], errors=\"coerce\")\n",
        "        if pc.notna().any():\n",
        "            diffs = pc.diff().fillna(0).clip(lower=0)\n",
        "            pellet_events = float(diffs.sum())\n",
        "            if pellet_events == 0 and pc.iloc[-1] >= pc.iloc[0]:\n",
        "                pellet_events = float(pc.iloc[-1] - pc.iloc[0])\n",
        "\n",
        "    if (pd.isna(pellet_events)) and (\"Event\" in df.columns):\n",
        "        pellet_events = float((df[\"Event\"].astype(str) == \"Pellet\").sum())\n",
        "\n",
        "    if pd.isna(pellet_events):\n",
        "        return np.nan\n",
        "    return (pellet_events / duration_hours) * 24.0\n",
        "\n",
        "def breakpoint_and_runs(df):\n",
        "    \"\"\"\n",
        "    Returns: (median_breakpoint, max_breakpoint, runs_count)\n",
        "    end-of-block: BPC > 0 now and next BPC == 0\n",
        "    \"\"\"\n",
        "    if \"Block_Pellet_Count\" not in df.columns or df[\"Block_Pellet_Count\"].empty:\n",
        "        return np.nan, np.nan, 0\n",
        "\n",
        "    bpc = _to_num(df[\"Block_Pellet_Count\"])\n",
        "    end_mask = (bpc > 0) & (bpc.shift(-1) == 0)\n",
        "    end_vals = bpc[end_mask]\n",
        "\n",
        "    med_bp = float(end_vals.median()) if len(end_vals) else np.nan\n",
        "    max_bp = float(bpc.max()) if bpc.notna().any() else np.nan\n",
        "    runs   = int(end_mask.sum())\n",
        "    return med_bp, max_bp, runs\n",
        "\n",
        "# --------------------\n",
        "# 1) Build cleaned metadata_df from Key_Df (Bandit-style)\n",
        "# --------------------\n",
        "metadata_df = Key_Df.copy().reset_index(drop=True)\n",
        "metadata_df.columns = [_clean_colname(c) for c in metadata_df.columns]\n",
        "\n",
        "if \"filename\" in metadata_df.columns:\n",
        "    metadata_df[\"filename\"] = metadata_df[\"filename\"].astype(str).map(os.path.basename).map(_basename)\n",
        "if \"Mouse_ID\" in metadata_df.columns:\n",
        "    metadata_df[\"Mouse_ID\"] = metadata_df[\"Mouse_ID\"].astype(str).str.strip()\n",
        "\n",
        "wanted7 = [\"Genotype\", \"Gene\", \"Strain\", \"Sex\", \"Diet\", \"Treatment\", \"Condition\"]\n",
        "\n",
        "lower_map = {c.lower(): c for c in metadata_df.columns}\n",
        "rename_map = {}\n",
        "for w in wanted7:\n",
        "    c = lower_map.get(w.lower(), None)\n",
        "    if c is not None and c != w:\n",
        "        rename_map[c] = w\n",
        "metadata_df = metadata_df.rename(columns=rename_map)\n",
        "\n",
        "merge_cols  = [c for c in [\"filename\", \"Mouse_ID\"] if c in metadata_df.columns]\n",
        "naming_cols = [c for c in [\"Session_type\", \"Gene_ID\", \"Strain_ID\"] if c in metadata_df.columns]\n",
        "keep_cols   = list(dict.fromkeys(merge_cols + [c for c in wanted7 if c in metadata_df.columns] + naming_cols))\n",
        "metadata_df = metadata_df.loc[:, keep_cols].copy()\n",
        "\n",
        "md = metadata_df.copy()\n",
        "if \"filename\" in md.columns:\n",
        "    md[\"filename\"] = md[\"filename\"].astype(str).map(_basename)\n",
        "if \"Mouse_ID\" in md.columns:\n",
        "    md[\"Mouse_ID\"] = md[\"Mouse_ID\"].astype(str).str.strip()\n",
        "\n",
        "# --------------------\n",
        "# 2) Decide output ID mode (Bandit-style)\n",
        "# --------------------\n",
        "match_mode = globals().get('KEY_MATCH_MODE', None)\n",
        "if match_mode == 'filename':\n",
        "    id_col = 'filename'\n",
        "elif match_mode == 'mouse_id':\n",
        "    id_col = 'Mouse_ID'\n",
        "else:\n",
        "    id_col = 'Mouse_ID' if 'Mouse_ID' in md.columns else 'filename'\n",
        "other_id = \"Mouse_ID\" if id_col == \"filename\" else \"filename\"\n",
        "\n",
        "# --------------------\n",
        "# 3) Compute PR metrics (file-level)\n",
        "# --------------------\n",
        "rows = []\n",
        "for idx, c_df in enumerate(_tqdm(feds, desc=\"Computing PR metrics\")):\n",
        "    file_name = _basename(getattr(c_df, \"name\", f\"File_{idx}\"))\n",
        "\n",
        "    pellets = _count_events(c_df, \"Pellet\")\n",
        "    left    = _count_events(c_df, \"Left\")\n",
        "    right   = _count_events(c_df, \"Right\")\n",
        "    total   = left + right\n",
        "\n",
        "    acc = (left / total * 100.0) if total > 0 else np.nan\n",
        "    ppp = (total / pellets) if pellets > 0 else np.nan\n",
        "\n",
        "    med_bp, max_bp, runs = breakpoint_and_runs(c_df)\n",
        "    daily = _estimate_daily_pellets(c_df)\n",
        "\n",
        "    rows.append({\n",
        "        \"filename\": file_name,\n",
        "        \"Left_Poke\": left,\n",
        "        \"Right_Poke\": right,\n",
        "        \"Total_Pokes\": total,\n",
        "        \"Accuracy\": acc,\n",
        "        \"PokesPerPellet\": ppp,\n",
        "        \"MedianBreakPoint\": med_bp,\n",
        "        \"MaxBreakPoint\": max_bp,\n",
        "        \"Numberofblocks\": runs,\n",
        "        \"Daily_Pellets\": daily,   # use consistent name (recommended)\n",
        "    })\n",
        "\n",
        "PRmetrics = pd.DataFrame(rows)\n",
        "if PRmetrics.empty:\n",
        "    display(HTML(\"<b style='color:#b00'>No files to analyze.</b>\"))\n",
        "    raise SystemExit\n",
        "\n",
        "# --------------------\n",
        "# 4) Demand fits (mouse-level) — keep your logic, but ensure Mouse_ID exists first\n",
        "#     We attach Mouse_ID from md now so demand fitting can group by mouse.\n",
        "# --------------------\n",
        "if \"filename\" in md.columns and \"Mouse_ID\" in md.columns:\n",
        "    mouse_map = md.dropna(subset=[\"filename\"]).drop_duplicates(\"filename\").set_index(\"filename\")[\"Mouse_ID\"]\n",
        "    PRmetrics[\"Mouse_ID\"] = PRmetrics[\"filename\"].map(mouse_map)\n",
        "else:\n",
        "    PRmetrics[\"Mouse_ID\"] = np.nan\n",
        "\n",
        "# substring fallback if still missing\n",
        "if PRmetrics[\"Mouse_ID\"].isna().any() and (\"Mouse_ID\" in md.columns):\n",
        "    known_ids = md[\"Mouse_ID\"].dropna().unique().tolist()\n",
        "    for i, r in PRmetrics.loc[PRmetrics[\"Mouse_ID\"].isna()].iterrows():\n",
        "        base = _file_base_lower(r[\"filename\"])\n",
        "        hits = [mid for mid in known_ids if str(mid).lower() in base]\n",
        "        if len(hits) == 1:\n",
        "            PRmetrics.at[i, \"Mouse_ID\"] = hits[0]\n",
        "        elif len(hits) > 1:\n",
        "            longest = max(len(str(h)) for h in hits)\n",
        "            best = [h for h in hits if len(str(h)) == longest]\n",
        "            if len(best) == 1:\n",
        "                PRmetrics.at[i, \"Mouse_ID\"] = best[0]\n",
        "\n",
        "# build filename->df map\n",
        "file_names = [_basename(getattr(df, \"name\", f\"File_{i}\")) for i, df in enumerate(feds)]\n",
        "file_to_df = dict(zip(file_names, feds))\n",
        "\n",
        "MIN_RUNS_PER_MOUSE = 10\n",
        "runs_per_mouse = (\n",
        "    PRmetrics.dropna(subset=[\"Mouse_ID\"])\n",
        "    .groupby(\"Mouse_ID\")[\"Numberofblocks\"]\n",
        "    .sum()\n",
        "    .to_dict()\n",
        ")\n",
        "keep_mice = {m for m, r in runs_per_mouse.items() if r >= MIN_RUNS_PER_MOUSE}\n",
        "for m, r in runs_per_mouse.items():\n",
        "    if m not in keep_mice:\n",
        "        print(f\"[{m}] skipped: only {r} blocks (< {MIN_RUNS_PER_MOUSE}).\")\n",
        "\n",
        "raw_rows = []\n",
        "for fn, df in file_to_df.items():\n",
        "    mouse = PRmetrics.loc[PRmetrics[\"filename\"] == fn, \"Mouse_ID\"]\n",
        "    mouse = mouse.iloc[0] if len(mouse) else None\n",
        "    if (mouse is None) or (mouse not in keep_mice):\n",
        "        continue\n",
        "    if {\"Event\", \"Block_Pellet_Count\"} - set(df.columns):\n",
        "        print(f\"[{mouse}] file {fn} missing Event/BPC columns; skipped.\")\n",
        "        continue\n",
        "\n",
        "    pellets_df = df[df[\"Event\"].astype(str) == \"Pellet\"].copy()\n",
        "    if pellets_df.empty:\n",
        "        print(f\"[{mouse}] file {fn} has 0 pellet rows; skipped.\")\n",
        "        continue\n",
        "\n",
        "    bpc = pd.to_numeric(pellets_df[\"Block_Pellet_Count\"], errors=\"coerce\")\n",
        "    counts = (\n",
        "        pellets_df.assign(Block_Pellet_Count=bpc)\n",
        "        .dropna(subset=[\"Block_Pellet_Count\"])\n",
        "        .groupby(\"Block_Pellet_Count\")\n",
        "        .size()\n",
        "    )\n",
        "    if counts.empty:\n",
        "        print(f\"[{mouse}] file {fn} produced no valid price bins; skipped.\")\n",
        "        continue\n",
        "\n",
        "    for price, cnt in counts.items():\n",
        "        raw_rows.append({\"Mouse_ID\": mouse, \"PricePaid\": int(price), \"PelletCount\": int(cnt)})\n",
        "\n",
        "demand_raw_df = pd.DataFrame(raw_rows)\n",
        "\n",
        "if demand_raw_df.empty:\n",
        "    demand_metrics = pd.DataFrame(columns=[\n",
        "        \"Mouse_ID\", \"Demand_Q0_raw\", \"Demand_alpha_raw\", \"Demand_beta_raw\",\n",
        "        \"Demand_alpha_FR\", \"Demand_beta_FR\",\n",
        "    ])\n",
        "else:\n",
        "    demand_mouse = (demand_raw_df\n",
        "                    .groupby([\"Mouse_ID\", \"PricePaid\"], as_index=False)[\"PelletCount\"]\n",
        "                    .sum())\n",
        "\n",
        "    def _baseline_B(sub):\n",
        "        if sub.empty:\n",
        "            return np.nan\n",
        "        mprice = sub[\"PricePaid\"].min()\n",
        "        return sub.loc[sub[\"PricePaid\"] == mprice, \"PelletCount\"].mean()\n",
        "\n",
        "    B = (demand_mouse.groupby(\"Mouse_ID\")\n",
        "         .apply(_baseline_B).rename(\"B\").reset_index())\n",
        "    B[\"q\"] = 100.0 / B[\"B\"]\n",
        "\n",
        "    normalized = (\n",
        "        demand_mouse\n",
        "        .merge(B[[\"Mouse_ID\", \"q\"]], on=\"Mouse_ID\", how=\"left\")\n",
        "        .assign(P_FR=lambda d: d[\"PricePaid\"],\n",
        "                Q_norm=lambda d: d[\"PelletCount\"] * d[\"q\"])\n",
        "    )\n",
        "\n",
        "    def loglog3(P, Q0, a, b):\n",
        "        return Q0 / (1.0 + (P / a) ** b)\n",
        "\n",
        "    def loglogN(P, a, b):\n",
        "        return 100.0 / (1.0 + (P / a) ** b)\n",
        "\n",
        "    raw_fit_rows = []\n",
        "    for mouse, sub in demand_mouse.groupby(\"Mouse_ID\"):\n",
        "        P = sub[\"PricePaid\"].to_numpy(float)\n",
        "        Q = sub[\"PelletCount\"].to_numpy(float)\n",
        "        if np.unique(P).size < 3:\n",
        "            print(f\"[{mouse}] demand fit skipped (raw): need ≥3 unique FR levels.\")\n",
        "            continue\n",
        "        try:\n",
        "            popt, _ = curve_fit(\n",
        "                loglog3, P, Q,\n",
        "                p0=[Q.max(), float(np.median(P)), 2.0],\n",
        "                bounds=([1e-6, 1e-6, 1e-3], [np.inf, np.inf, 50.0]),\n",
        "                maxfev=20000\n",
        "            )\n",
        "            raw_fit_rows.append({\n",
        "                \"Mouse_ID\": mouse,\n",
        "                \"Demand_Q0_raw\": float(popt[0]),\n",
        "                \"Demand_alpha_raw\": float(popt[1]),\n",
        "                \"Demand_beta_raw\": float(popt[2]),\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"[{mouse}] demand curve fit failed (raw): {e}\")\n",
        "    demand_fit_raw = pd.DataFrame(raw_fit_rows)\n",
        "\n",
        "    fr_fit_rows = []\n",
        "    for mouse, sub in normalized.groupby(\"Mouse_ID\"):\n",
        "        P = sub[\"P_FR\"].to_numpy(float)\n",
        "        Q = sub[\"Q_norm\"].to_numpy(float)\n",
        "        if np.unique(P).size < 3:\n",
        "            print(f\"[{mouse}] demand fit skipped (norm-Q): need ≥3 unique FR levels.\")\n",
        "            continue\n",
        "        try:\n",
        "            popt, _ = curve_fit(\n",
        "                loglogN, P, Q,\n",
        "                p0=[float(np.median(P)), 2.0],\n",
        "                bounds=([1e-6, 1e-3], [np.inf, 50.0]),\n",
        "                maxfev=20000\n",
        "            )\n",
        "            fr_fit_rows.append({\n",
        "                \"Mouse_ID\": mouse,\n",
        "                \"Demand_alpha_FR\": float(popt[0]),\n",
        "                \"Demand_beta_FR\": float(popt[1]),\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"[{mouse}] demand curve fit failed (norm-Q): {e}\")\n",
        "    demand_fit_fr = pd.DataFrame(fr_fit_rows)\n",
        "\n",
        "    demand_metrics = demand_fit_raw.merge(demand_fit_fr, on=\"Mouse_ID\", how=\"outer\")\n",
        "\n",
        "# merge demand into PRmetrics (Mouse-level)\n",
        "PRmetrics = PRmetrics.merge(demand_metrics, on=\"Mouse_ID\", how=\"left\")\n",
        "\n",
        "# --------------------\n",
        "# 5) Attach metadata robustly (Bandit-style merge)\n",
        "# --------------------\n",
        "pm = PRmetrics.copy()\n",
        "\n",
        "# If md has filename<->Mouse_ID, map Mouse_ID from filename (already done), but keep Bandit pattern\n",
        "if \"filename\" in md.columns and \"Mouse_ID\" in md.columns and pm[\"Mouse_ID\"].isna().any():\n",
        "    mouse_map = md.dropna(subset=[\"filename\"]).drop_duplicates(\"filename\").set_index(\"filename\")[\"Mouse_ID\"]\n",
        "    pm[\"Mouse_ID\"] = pm[\"filename\"].map(mouse_map)\n",
        "\n",
        "# Merge metadata on best available key\n",
        "if (id_col == \"Mouse_ID\") and (\"Mouse_ID\" in pm.columns) and (\"Mouse_ID\" in md.columns) and pm[\"Mouse_ID\"].notna().any():\n",
        "    md_unique = md.drop_duplicates(subset=[\"Mouse_ID\"], keep=\"first\")\n",
        "    pm = pm.merge(md_unique, on=\"Mouse_ID\", how=\"left\", suffixes=(\"\", \"_md\"))\n",
        "else:\n",
        "    if \"filename\" not in md.columns:\n",
        "        raise ValueError(\"metadata_df has no 'filename' column, but filename is needed for this mode.\")\n",
        "    md_unique = md.drop_duplicates(subset=[\"filename\"], keep=\"first\")\n",
        "    pm = pm.merge(md_unique, on=\"filename\", how=\"left\", suffixes=(\"\", \"_md\"))\n",
        "\n",
        "PRmetrics_merged = pm.copy()\n",
        "\n",
        "# --------------------\n",
        "# 6) Session-type suffixing (Bandit-style)\n",
        "# --------------------\n",
        "metric_cols = [\n",
        "    \"Left_Poke\", \"Right_Poke\", \"Total_Pokes\", \"Accuracy\", \"PokesPerPellet\",\n",
        "    \"MedianBreakPoint\", \"Numberofblocks\", \"Daily_Pellets\",\n",
        "    \"Demand_Q0_raw\", \"Demand_alpha_raw\", \"Demand_beta_raw\",\n",
        "    \"Demand_alpha_FR\", \"Demand_beta_FR\",\n",
        "]\n",
        "metric_cols = [c for c in metric_cols if c in PRmetrics_merged.columns]\n",
        "\n",
        "bm = PRmetrics_merged.copy()\n",
        "\n",
        "if \"Session_type\" in bm.columns:\n",
        "    session_series = bm[\"Session_type\"].astype(str).str.strip()\n",
        "else:\n",
        "    sess_map = {\n",
        "        _basename(getattr(feds[i], \"name\", f\"File_{i}\")):\n",
        "        (getattr(feds[i], \"attrs\", {}).get(\"Session_type\") or \"Unknown\")\n",
        "        for i in range(len(feds))\n",
        "    }\n",
        "    session_series = bm[\"filename\"].map(sess_map).fillna(\"Unknown\").astype(str)\n",
        "\n",
        "session_series = session_series.str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "bm[\"_Session_type_for_csv\"] = session_series\n",
        "\n",
        "def with_session_suffix_for_csv(df, metrics=metric_cols, session_col=\"_Session_type_for_csv\"):\n",
        "    df = df.copy()\n",
        "    for m in metrics:\n",
        "        if m not in df.columns:\n",
        "            continue\n",
        "        for sess in df[session_col].dropna().unique():\n",
        "            mask = df[session_col] == sess\n",
        "            col_name = f\"{m}_{sess}\"\n",
        "            if col_name not in df.columns:\n",
        "                df[col_name] = np.nan\n",
        "            df.loc[mask, col_name] = df.loc[mask, m]\n",
        "        df.drop(columns=[m], inplace=True)\n",
        "    return df.drop(columns=[session_col])\n",
        "\n",
        "PRmetrics_csv = with_session_suffix_for_csv(bm)\n",
        "\n",
        "# --------------------\n",
        "# 7) Final export columns (Bandit-style): [id_col] + wanted7 + suffixed metrics only\n",
        "# --------------------\n",
        "def _metric_match(col: str) -> bool:\n",
        "    return any(col.startswith(base + \"_\") for base in metric_cols)\n",
        "\n",
        "metric_keep = [c for c in PRmetrics_csv.columns if _metric_match(c)]\n",
        "if not metric_keep:\n",
        "    raise RuntimeError(\"No session-suffixed metric columns matched; check 'metric_cols'.\")\n",
        "\n",
        "meta_keep = [c for c in wanted7 if c in PRmetrics_csv.columns]\n",
        "\n",
        "drop_cols = []\n",
        "if other_id in PRmetrics_csv.columns:\n",
        "    drop_cols.append(other_id)\n",
        "for c in [\"File\", \"FileIndex\"]:\n",
        "    if c in PRmetrics_csv.columns:\n",
        "        drop_cols.append(c)\n",
        "\n",
        "PRmetrics_csv = PRmetrics_csv.drop(columns=drop_cols, errors=\"ignore\")\n",
        "\n",
        "cols_out = [id_col] + meta_keep + metric_keep\n",
        "cols_out = [c for c in cols_out if c in PRmetrics_csv.columns]\n",
        "PRmetrics_csv = PRmetrics_csv.loc[:, cols_out].copy()\n",
        "\n",
        "# --------------------\n",
        "# 8) Save & present (same naming scheme as Bandit)\n",
        "# --------------------\n",
        "example = PRmetrics_merged.iloc[0]\n",
        "\n",
        "strain_name = str(example.get(\"Gene\", example.get(\"Strain\", \"PR\"))).replace(\" \", \"_\")\n",
        "strain_num_raw = example.get(\"Gene_ID\", example.get(\"Strain_ID\", \"NA\"))\n",
        "try:\n",
        "    strain_num = f\"{int(strain_num_raw):03d}\"\n",
        "except Exception:\n",
        "    strain_num = str(strain_num_raw).zfill(3)\n",
        "\n",
        "task_name = str(example.get(\"Session_type\", \"Unknown\")).replace(\" \", \"_\")\n",
        "fname = f\"{strain_name}_{strain_num}_{task_name}_L3.csv\"\n",
        "\n",
        "PRmetrics_csv.to_csv(fname, index=False)\n",
        "display(HTML(f\"<b>✓ Saved PR metrics CSV to:</b> <code>{fname}</code>\"))\n",
        "try:\n",
        "    display(FileLink(fname))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(fname)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "\n",
        "def _on_click(_):\n",
        "    clear_output(wait=True)\n",
        "    display(btn, status)\n",
        "    if not os.path.exists(fname):\n",
        "        status.value = f\"<b style='color:#b00'>File not found:</b> {fname}\"\n",
        "        return\n",
        "    try:\n",
        "        from google.colab import files as gfiles\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(fname)}</code>…\"\n",
        "        gfiles.download(fname)\n",
        "    except Exception:\n",
        "        status.value = f\"Not running in Colab. File saved locally at:<br><code>{fname}</code>\"\n",
        "\n",
        "display(btn, status)\n",
        "btn.on_click(_on_click)\n",
        "\n"
      ],
      "metadata": {
        "id": "hrdyJUDo0QPh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot normalized demand curves\n",
        "try:\n",
        "    if isinstance(globals().get(\"demand_ui\"), dict):\n",
        "        for k in [\"slider\", \"save_btn\", \"status\", \"out\", \"box\"]:\n",
        "            w = demand_ui.get(k)\n",
        "            if hasattr(w, \"close\"):\n",
        "                w.close()\n",
        "        del demand_ui\n",
        "except Exception:\n",
        "    pass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import curve_fit\n",
        "from ipywidgets import interact, IntSlider\n",
        "from IPython.display import clear_output\n",
        "from matplotlib.lines import Line2D\n",
        "import matplotlib.patheffects as pe\n",
        "from matplotlib.colors import LogNorm, Normalize\n",
        "\n",
        "\n",
        "# ---------- Build normalized_demand_df if missing ----------\n",
        "def _basename(p):\n",
        "    s = str(p).replace(\"\\\\\", \"/\")\n",
        "    return s.split(\"/\")[-1]\n",
        "\n",
        "if \"normalized_demand_df\" not in globals() or not isinstance(globals().get(\"normalized_demand_df\"), pd.DataFrame) or globals()[\"normalized_demand_df\"].empty:\n",
        "    assert 'feds' in globals() and isinstance(feds, (list, tuple)) and len(feds) > 0, \\\n",
        "        \"Need `feds` (list of FED3 DataFrames) to derive normalized_demand_df.\"\n",
        "\n",
        "    file_names = [_basename(getattr(df, \"name\", f\"File_{i}\")) for i, df in enumerate(feds)]\n",
        "    file_to_df = dict(zip(file_names, feds))\n",
        "\n",
        "    rows = []\n",
        "    for fn, df in file_to_df.items():\n",
        "        if {\"Event\", \"Block_Pellet_Count\"} - set(df.columns):\n",
        "            continue\n",
        "        pellets = df[df[\"Event\"].astype(str) == \"Pellet\"].copy()\n",
        "        if pellets.empty:\n",
        "            continue\n",
        "        bpc = pd.to_numeric(pellets[\"Block_Pellet_Count\"], errors=\"coerce\")\n",
        "        counts = (\n",
        "            pellets.assign(Block_Pellet_Count=bpc)\n",
        "                   .dropna(subset=[\"Block_Pellet_Count\"])\n",
        "                   .groupby(\"Block_Pellet_Count\")\n",
        "                   .size()\n",
        "        )\n",
        "        for price, cnt in counts.items():\n",
        "            rows.append({\"File\": str(fn), \"PricePaid\": int(price), \"PelletCount\": int(cnt)})\n",
        "\n",
        "    normalized_demand_df = pd.DataFrame(rows)\n",
        "    if not normalized_demand_df.empty:\n",
        "        keep = normalized_demand_df.groupby(\"File\")[\"PricePaid\"].nunique()\n",
        "        normalized_demand_df = normalized_demand_df[\n",
        "            normalized_demand_df[\"File\"].isin(keep[keep >= 3].index)\n",
        "        ].reset_index(drop=True)\n",
        "\n",
        "# Preconditions\n",
        "if \"normalized_demand_df\" not in globals() or not isinstance(normalized_demand_df, pd.DataFrame) or normalized_demand_df.empty:\n",
        "    raise RuntimeError(\"normalized_demand_df is missing or empty. Expect columns: File, PricePaid, PelletCount.\")\n",
        "\n",
        "for col in [\"File\", \"PricePaid\", \"PelletCount\"]:\n",
        "    if col not in normalized_demand_df.columns:\n",
        "        raise RuntimeError(f\"normalized_demand_df must contain '{col}'\")\n",
        "\n",
        "# ---------- Prepare data (Q-only normalization) ----------\n",
        "data = normalized_demand_df.copy()\n",
        "data[\"File\"]        = data[\"File\"].astype(str)\n",
        "data[\"PricePaid\"]   = pd.to_numeric(data[\"PricePaid\"], errors=\"coerce\")\n",
        "data[\"PelletCount\"] = pd.to_numeric(data[\"PelletCount\"], errors=\"coerce\")\n",
        "data = data.dropna(subset=[\"PricePaid\", \"PelletCount\"])\n",
        "\n",
        "def _baseline_B(sub):\n",
        "    return sub.loc[sub[\"PricePaid\"].eq(sub[\"PricePaid\"].min()), \"PelletCount\"].mean()\n",
        "\n",
        "B_by_file = data.groupby(\"File\", as_index=True).apply(_baseline_B).rename(\"B\")\n",
        "q_by_file = (100.0 / B_by_file).rename(\"q\")\n",
        "\n",
        "data = data.merge(q_by_file, left_on=\"File\", right_index=True, how=\"left\")\n",
        "data[\"Q_norm\"] = data[\"PelletCount\"] * data[\"q\"]\n",
        "\n",
        "files = sorted(data[\"File\"].unique().tolist())\n",
        "\n",
        "# Optional mouse labels\n",
        "fname_to_mouse = {}\n",
        "if \"metadata_df\" in globals() and isinstance(metadata_df, pd.DataFrame):\n",
        "    md_tmp = metadata_df.copy()\n",
        "    if \"filename\" in md_tmp.columns and \"Mouse_ID\" in md_tmp.columns:\n",
        "        md_tmp[\"filename\"] = md_tmp[\"filename\"].astype(str).apply(lambda p: p.split(\"/\")[-1])\n",
        "        fname_to_mouse = md_tmp.set_index(\"filename\")[\"Mouse_ID\"].astype(str).to_dict()\n",
        "\n",
        "# ---------- Model ----------\n",
        "def log_logistic_norm(P, alpha, beta):\n",
        "    return 100.0 / (1.0 + (P / alpha)**beta)\n",
        "\n",
        "# ---------- Plotter (returns fig; does not call plt.show) ----------\n",
        "def plot_mouse(idx: int):\n",
        "    if len(files) == 0:\n",
        "        raise RuntimeError(\"No files in data.\")\n",
        "    if not (0 <= idx < len(files)):\n",
        "        raise RuntimeError(f\"Out-of-range index {idx} for {len(files)} files.\")\n",
        "\n",
        "    file = files[idx]\n",
        "    sub = data[data[\"File\"] == file].copy().sort_values(\"PricePaid\")\n",
        "\n",
        "    FR = sub[\"PricePaid\"].to_numpy(dtype=float)\n",
        "    Qn = sub[\"Q_norm\"].to_numpy(dtype=float)\n",
        "\n",
        "    # Fit in FR space\n",
        "    alpha_FR = beta_FR = np.nan\n",
        "    if np.isfinite(FR).any() and np.isfinite(Qn).any():\n",
        "        try:\n",
        "            alpha0_fr = float(np.nanmedian(FR))\n",
        "            popt_fr, _ = curve_fit(\n",
        "                log_logistic_norm, FR, Qn,\n",
        "                p0=[max(alpha0_fr, 1.0), 2.0],\n",
        "                bounds=([1e-6, 1e-3], [np.inf, 50.0]),\n",
        "                maxfev=20000\n",
        "            )\n",
        "            alpha_FR, beta_FR = [float(p) for p in popt_fr]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Figure\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "\n",
        "    # choose bounds from your finite, positive FR values\n",
        "    FR_pos = FR[np.isfinite(FR) & (FR > 0)]\n",
        "    vmin = float(np.nanmin(FR_pos)) if FR_pos.size else 1.0\n",
        "    vmax = float(np.nanmax(FR_pos)) if FR_pos.size else 100.0\n",
        "\n",
        "    # Log-normalize so equal ratios get equal color steps (matches log x-axis)\n",
        "    norm = LogNorm(vmin=vmin, vmax=vmax)\n",
        "\n",
        "\n",
        "    sc = ax.scatter(FR, Qn, c=FR, cmap=\"spring\", norm=norm, s=30, edgecolor=\"none\", alpha=0.9)\n",
        "\n",
        "\n",
        "    FR_pos = FR[np.isfinite(FR) & (FR > 0)]\n",
        "    xmin = float(np.nanmin(FR_pos)) if FR_pos.size else 1.0\n",
        "    xmin = max(1.0, xmin)\n",
        "    xmax_data = float(np.nanmax(FR_pos)) if FR_pos.size else 100.0\n",
        "    xmax = max(100.0, xmax_data)\n",
        "    if xmax <= xmin:\n",
        "        xmax = xmin * 1.5\n",
        "    ax.set_xlim(xmin, xmax)\n",
        "    ax.set_xscale(\"log\")\n",
        "\n",
        "    if np.isfinite(alpha_FR) and np.isfinite(beta_FR):\n",
        "        Pfit = np.logspace(np.log10(xmin), np.log10(xmax), 400)\n",
        "        Qfit = log_logistic_norm(Pfit, alpha_FR, beta_FR)\n",
        "        ax.plot(Pfit, Qfit, linewidth=2)\n",
        "\n",
        "        ax.vlines(alpha_FR, ymin=0, ymax=50, linestyle=\"--\", linewidth=1.5)\n",
        "        ax.scatter([alpha_FR], [50], zorder=5)\n",
        "        ax.annotate(\n",
        "            rf\"$\\alpha_{{FR}}$ = {alpha_FR:.3g}\",\n",
        "            xy=(alpha_FR, 50),\n",
        "            xytext=(8, 8), textcoords=\"offset points\",\n",
        "            fontsize=11, ha=\"left\", va=\"bottom\",\n",
        "            path_effects=[pe.withStroke(linewidth=2, foreground=\"white\", alpha=0.7)]\n",
        "        )\n",
        "\n",
        "        dQdP_alpha = -(100.0 * beta_FR) / (4.0 * alpha_FR)\n",
        "        dx = alpha_FR * 0.2\n",
        "        x1, x2 = alpha_FR - dx/2.0, alpha_FR + dx/2.0\n",
        "        y1 = 50 + dQdP_alpha * (x1 - alpha_FR)\n",
        "        y2 = 50 + dQdP_alpha * (x2 - alpha_FR)\n",
        "        ax.plot([x1, x2], [y1, y2], linewidth=4)\n",
        "        xm, ym = (x1 + x2) / 2.0, (y1 + y2) / 2.0\n",
        "        ax.annotate(\n",
        "            rf\"$\\beta_{{FR}}$ = {beta_FR:.3g}\",\n",
        "            xy=(xm, ym), xytext=(8, 0), textcoords=\"offset points\",\n",
        "            fontsize=11, ha=\"left\", va=\"center\",\n",
        "            path_effects=[pe.withStroke(linewidth=2, foreground=\"white\", alpha=0.7)]\n",
        "        )\n",
        "\n",
        "    title = fname_to_mouse.get(file, None) or os.path.basename(str(file))\n",
        "    ax.set_title(str(title))\n",
        "    ax.set_xlabel(\"Food Price (FR)\")\n",
        "    ax.set_ylabel(\"Consumption\")\n",
        "    ax.set_yticks(np.arange(0, 101, 20))\n",
        "    ax.set_ylim(0, 105)\n",
        "\n",
        "    fr_ticks = np.array([1, 5, 10, 20, 30, 50, 75, 100], dtype=float)\n",
        "    fr_ticks = fr_ticks[(fr_ticks >= xmin) & (fr_ticks <= xmax)]\n",
        "    if fr_ticks.size:\n",
        "        ax.set_xticks(fr_ticks)\n",
        "        ax.set_xticklabels([str(int(fr)) for fr in fr_ticks])\n",
        "        for fr in fr_ticks:\n",
        "            ax.axvline(fr, alpha=0.12, lw=1, zorder=0)\n",
        "\n",
        "    ax.grid(False)\n",
        "    fig.tight_layout()\n",
        "    return fig, file\n",
        "\n",
        "# ---------- UI: single instance ----------\n",
        "demand_ui = {}\n",
        "\n",
        "def _sanitize_filename(s: str) -> str:\n",
        "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", s)\n",
        "\n",
        "slider = widgets.IntSlider(\n",
        "    min=0, max=max(0, len(files)-1), step=1, value=0,\n",
        "    description=\"File\", continuous_update=True\n",
        ")\n",
        "save_btn = widgets.Button(description=\"Save PDF\")\n",
        "status = widgets.HTML()\n",
        "out = widgets.Output()\n",
        "\n",
        "def _update_plot(change=None):\n",
        "    with out:\n",
        "        out.clear_output(wait=True)\n",
        "        fig, file = plot_mouse(slider.value)\n",
        "        display(fig)      # show exactly one figure\n",
        "        plt.close(fig)    # close backend handle to prevent accumulation\n",
        "        demand_ui[\"_last_fig\"] = fig\n",
        "        demand_ui[\"_last_file\"] = file\n",
        "        status.value = \"\" # clear old status\n",
        "\n",
        "def _save_pdf(_):\n",
        "    fig = demand_ui.get(\"_last_fig\")\n",
        "    file = demand_ui.get(\"_last_file\", \"figure\")\n",
        "    if fig is None:\n",
        "        status.value = \"<span style='color:#b00'>No figure to save.</span>\"\n",
        "        return\n",
        "    base = _sanitize_filename(os.path.splitext(os.path.basename(str(file)))[0])\n",
        "    fname = f\"demand_{base}_{datetime.now():%Y%m%d_%H%M%S}.pdf\"\n",
        "    # Save from the stored fig object; even though we closed the backend handle,\n",
        "    # the Figure instance retains its artists and can still be saved.\n",
        "    fig.savefig(fname, format=\"pdf\", bbox_inches=\"tight\")\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Preparing download: <code>{fname}</code>…\"\n",
        "        gfiles.download(fname)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{os.path.abspath(fname)}</code>.\"\n",
        "\n",
        "slider.observe(_update_plot, names=\"value\")\n",
        "save_btn.on_click(_save_pdf)\n",
        "\n",
        "# Keep references so we can close them next run\n",
        "box = widgets.HBox([slider, save_btn])\n",
        "demand_ui.update({\"slider\": slider, \"save_btn\": save_btn, \"status\": status, \"out\": out, \"box\": box})\n",
        "\n",
        "# Initial render\n",
        "_update_plot()\n",
        "display(box)\n",
        "display(status)\n",
        "display(out)"
      ],
      "metadata": {
        "id": "70luc0dCbFaq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Group for plotting\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# --- sanity ---\n",
        "if 'metadata_df' not in globals() or metadata_df is None or metadata_df.empty:\n",
        "    raise RuntimeError(\"metadata_df is missing or empty. Build metadata_df (copy of Key_Df) first.\")\n",
        "\n",
        "EXCLUDE_LOWER = {\"match_status\"}   # everything else is allowed\n",
        "\n",
        "def _build_file_column(df):\n",
        "    if \"filename\" in df.columns:\n",
        "        return df[\"filename\"].apply(lambda p: os.path.basename(str(p)))\n",
        "    if \"FED3_from_file\" in df.columns and \"Date_from_file\" in df.columns:\n",
        "        return \"FED\" + df[\"FED3_from_file\"].astype(str) + \"_\" + df[\"Date_from_file\"].astype(str)\n",
        "    if \"FED3_from_file\" in df.columns:\n",
        "        return \"FED\" + df[\"FED3_from_file\"].astype(str)\n",
        "    return df.index.astype(str)\n",
        "\n",
        "def _norm_val(x):\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s.lower() in {\"nan\", \"none\"}:\n",
        "        return \"UNK\"\n",
        "    return s.upper()\n",
        "\n",
        "def _build_group_row(row, ordered_cols):\n",
        "    if not ordered_cols:\n",
        "        return \"ALL\"\n",
        "    return \" | \".join(_norm_val(row[c]) for c in ordered_cols)\n",
        "\n",
        "def build_mapping(ordered_cols):\n",
        "    _meta = metadata_df.copy()\n",
        "    _meta[\"filename\"] = _build_file_column(_meta)\n",
        "    _meta[\"Group\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols), axis=1)\n",
        "    mapping = (\n",
        "        _meta[[\"filename\", \"Group\"]]\n",
        "        .dropna(subset=[\"filename\"])\n",
        "        .drop_duplicates()\n",
        "        .sort_values([\"Group\", \"filename\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    return mapping\n",
        "\n",
        "def _unique_keep_order(seq):\n",
        "    seen = set(); out = []\n",
        "    for x in seq:\n",
        "        if x not in seen:\n",
        "            seen.add(x); out.append(x)\n",
        "    return out\n",
        "\n",
        "# ---------- UI (fixed sizes + grid) ----------\n",
        "PX_W = \"260px\"   # list box width\n",
        "PX_H = \"160px\"   # list box height\n",
        "BTN_W = \"160px\"  # button column width\n",
        "HDR_H = \"28px\"   # header cell height (consistent across all headers)\n",
        "\n",
        "title = widgets.HTML(\"<h3>Select columns to group by for X and Hue, then reorder X to set hierarchy</h3>\")\n",
        "\n",
        "all_cols = sorted((c for c in metadata_df.columns if str(c).lower() not in EXCLUDE_LOWER), key=str.lower)\n",
        "\n",
        "def header(text):\n",
        "    # Normalize header height/margins so they align perfectly in the grid row\n",
        "    return widgets.HTML(\n",
        "        f\"<div style='height:{HDR_H};display:flex;align-items:flex-end;'>\"\n",
        "        f\"<h4 style=\\\"margin:0;\\\">{text}</h4></div>\"\n",
        "    )\n",
        "\n",
        "# Headers (row 1 of grid)\n",
        "available_hdr = header(\"Available\")\n",
        "actions_hdr   = header(\"Actions\")\n",
        "x_hdr         = header(\"X grouping\")\n",
        "hue_hdr       = header(\"Hue grouping\")\n",
        "\n",
        "# Widgets (row 2 of grid)\n",
        "available = widgets.SelectMultiple(\n",
        "    options=all_cols, value=tuple(), rows=14,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "right_x = widgets.Select(\n",
        "    options=[], value=None, rows=8,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "right_hue = widgets.Select(\n",
        "    options=[], value=None, rows=8,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Buttons\n",
        "btn_add_x    = widgets.Button(description=\"Add to X ▶\", button_style='primary', layout=widgets.Layout(width=BTN_W))\n",
        "btn_add_hue  = widgets.Button(description=\"Add to Hue ▶\",button_style='primary', layout=widgets.Layout(width=BTN_W))\n",
        "btn_clear    = widgets.Button(description=\"Clear\", button_style='danger', layout=widgets.Layout(width=BTN_W))\n",
        "btn_up       = widgets.Button(description=\"↑ Up (X only)\", layout=widgets.Layout(width=BTN_W))\n",
        "btn_down     = widgets.Button(description=\"↓ Down (X only)\", layout=widgets.Layout(width=BTN_W))\n",
        "btn_build    = widgets.Button(description=\"Build Groups\", button_style='success', layout=widgets.Layout(width=\"160px\"))\n",
        "\n",
        "controls_col = widgets.VBox(\n",
        "    [btn_add_x, btn_add_hue, btn_clear, btn_up, btn_down],\n",
        "    layout=widgets.Layout(\n",
        "        align_items=\"center\",\n",
        "        width=BTN_W, min_width=BTN_W, max_width=BTN_W,\n",
        "        height=PX_H, min_height=PX_H, max_height=PX_H,\n",
        "        flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "btn_build = widgets.Button(description=\"Build Groups\", button_style='success', layout=widgets.Layout(width=\"160px\"))\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- Callbacks ---\n",
        "def on_add_x(_):\n",
        "    sel = list(available.value)\n",
        "    if not sel: return\n",
        "    new_opts = _unique_keep_order(list(right_x.options) + sel)\n",
        "    right_x.value = None\n",
        "    right_x.options = new_opts\n",
        "    right_x.value = new_opts[-1] if new_opts else None\n",
        "\n",
        "def on_add_hue(_):\n",
        "    sel = list(available.value)\n",
        "    if not sel: return\n",
        "    new_opts = _unique_keep_order(list(right_hue.options) + sel)\n",
        "    right_hue.value = None\n",
        "    right_hue.options = new_opts\n",
        "    right_hue.value = new_opts[-1] if new_opts else None\n",
        "\n",
        "def on_clear(_):\n",
        "    right_x.value = None; right_x.options = []\n",
        "    right_hue.value = None; right_hue.options = []\n",
        "\n",
        "def on_up(_):\n",
        "    item = right_x.value\n",
        "    if item is None: return\n",
        "    opts = list(right_x.options)\n",
        "    i = opts.index(item)\n",
        "    if i > 0:\n",
        "        opts[i-1], opts[i] = opts[i], opts[i-1]\n",
        "        right_x.value = None; right_x.options = opts; right_x.value = item\n",
        "\n",
        "def on_down(_):\n",
        "    item = right_x.value\n",
        "    if item is None: return\n",
        "    opts = list(right_x.options)\n",
        "    i = opts.index(item)\n",
        "    if i < len(opts) - 1:\n",
        "        opts[i+1], opts[i] = opts[i], opts[i+1]\n",
        "        right_x.value = None; right_x.options = opts; right_x.value = item\n",
        "\n",
        "def on_build(_):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        ordered_cols_x = list(right_x.options)\n",
        "        ordered_cols_hue = list(right_hue.options)\n",
        "\n",
        "        mapping_x = build_mapping(ordered_cols_x)\n",
        "        mapping_hue = build_mapping(ordered_cols_hue)\n",
        "\n",
        "        _meta = metadata_df.copy()\n",
        "        _meta[\"filename\"] = _build_file_column(_meta)\n",
        "        _meta[\"XGroup\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols_x), axis=1)\n",
        "        _meta[\"HueGroup\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols_hue), axis=1)\n",
        "        mapping_both = (\n",
        "            _meta[[\"filename\", \"XGroup\", \"HueGroup\"]]\n",
        "            .dropna(subset=[\"filename\"])\n",
        "            .drop_duplicates()\n",
        "            .sort_values([\"XGroup\", \"HueGroup\", \"filename\"])\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        globals()['files_to_group_x'] = mapping_x.copy()\n",
        "        globals()['files_to_group_hue'] = mapping_hue.copy()\n",
        "        globals()['files_to_group_both'] = mapping_both.copy()\n",
        "        globals()['selected_group_cols_x'] = ordered_cols_x.copy()\n",
        "        globals()['selected_group_cols_hue'] = ordered_cols_hue.copy()\n",
        "\n",
        "        print(\"X-axis grouping (hierarchy):\", ordered_cols_x if ordered_cols_x else [\"ALL\"])\n",
        "        print(f\"Total unique files (X map): {mapping_x['filename'].nunique()}\")\n",
        "        display(widgets.HTML(\"<b>X-group summary</b>\"))\n",
        "        display((mapping_x.groupby(\"Group\", dropna=False)[\"filename\"]\n",
        "                 .nunique().sort_values(ascending=False)\n",
        "                 .rename(\"UniqueFiles\").to_frame()))\n",
        "\n",
        "        print(\"\\nHue grouping:\", ordered_cols_hue if ordered_cols_hue else [\"ALL\"])\n",
        "        print(f\"Total unique files (Hue map): {mapping_hue['filename'].nunique()}\")\n",
        "        display(widgets.HTML(\"<b>Hue-group summary</b>\"))\n",
        "        display((mapping_hue.groupby(\"Group\", dropna=False)[\"filename\"]\n",
        "                 .nunique().sort_values(ascending=False)\n",
        "                 .rename(\"UniqueFiles\").to_frame()))\n",
        "        print(\"\\nCombined mapping available as `files_to_group_both` (filename, XGroup, HueGroup)\")\n",
        "\n",
        "# Wire up\n",
        "btn_add_x.on_click(on_add_x)\n",
        "btn_add_hue.on_click(on_add_hue)\n",
        "btn_clear.on_click(on_clear)\n",
        "btn_up.on_click(on_up)\n",
        "btn_down.on_click(on_down)\n",
        "btn_build.on_click(on_build)\n",
        "\n",
        "# ----- Grid layout -----\n",
        "grid = widgets.GridBox(\n",
        "    children=[\n",
        "        available_hdr, actions_hdr, x_hdr, hue_hdr,     # row 1: headers\n",
        "        available,     controls_col, right_x, right_hue # row 2: widgets\n",
        "    ],\n",
        "    layout=widgets.Layout(\n",
        "        grid_template_columns=f\"{PX_W} {BTN_W} {PX_W} {PX_W}\",\n",
        "        grid_template_rows=\"auto auto\",\n",
        "        grid_gap=\"6px 16px\",\n",
        "        align_items=\"flex-start\",\n",
        "        justify_items=\"flex-start\",\n",
        "        width=\"100%\"\n",
        "    )\n",
        ")\n",
        "\n",
        "ui = widgets.VBox([title, grid, widgets.HBox([btn_build]), output])\n",
        "display(ui)"
      ],
      "metadata": {
        "id": "DNxvpj9ZrC8b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot Metrics!\n",
        "\n",
        "import os, time, shutil, re, itertools\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Optional stats libs\n",
        "try:\n",
        "    import pingouin as pg\n",
        "except Exception:\n",
        "    pg = None\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    from statsmodels.formula.api import ols\n",
        "except Exception:\n",
        "    sm, ols = None, None\n",
        "\n",
        "# Optional Colab download\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "except Exception:\n",
        "    colab_files = None\n",
        "\n",
        "ALPHA = 0.6  # bar & dot alpha\n",
        "\n",
        "# -----------------------\n",
        "# 0) Preconditions & base table\n",
        "# -----------------------\n",
        "if \"PRmetrics\" not in globals() or not isinstance(PRmetrics, pd.DataFrame) or PRmetrics.empty:\n",
        "    raise RuntimeError(\"PRmetrics not found/empty. Run the PR metrics cell first.\")\n",
        "\n",
        "if \"files_to_group_both\" not in globals() or not isinstance(files_to_group_both, pd.DataFrame) or files_to_group_both.empty:\n",
        "    raise RuntimeError(\"files_to_group_both not found/empty. Run the 'Group for plotting' cell and Build Groups.\")\n",
        "\n",
        "bm = PRmetrics.copy()\n",
        "if \"File\" not in bm.columns:\n",
        "    if \"filename\" in bm.columns:\n",
        "        bm[\"File\"] = bm[\"filename\"].apply(lambda p: os.path.basename(str(p)))\n",
        "    else:\n",
        "        raise RuntimeError(\"PRmetrics must contain 'File' or 'filename'.\")\n",
        "\n",
        "# -----------------------\n",
        "# -----------------------\n",
        "# Merge in XGroup/HueGroup from grouping widget\n",
        "# -----------------------\n",
        "def _basename_col(s):\n",
        "    return os.path.basename(str(s))\n",
        "\n",
        "def _src_name(df):\n",
        "    if \"filename\" in df.columns:\n",
        "        return \"filename\"\n",
        "    return None\n",
        "\n",
        "if (\"XGroup\" not in bm.columns) or (\"HueGroup\" not in bm.columns):\n",
        "    if 'files_to_group_both' in globals() and files_to_group_both is not None and not files_to_group_both.empty:\n",
        "        m = files_to_group_both.copy()\n",
        "        m_src = _src_name(m)\n",
        "        if m_src is None:\n",
        "            raise RuntimeError(\"Grouping table must include 'filename' (or legacy 'File').\")\n",
        "        m[\"file_base\"]  = m[m_src].apply(_basename_col)\n",
        "        bm[\"file_base\"] = bm[\"filename\"].apply(_basename_col)\n",
        "        bm = bm.merge(m[[\"file_base\",\"XGroup\",\"HueGroup\"]], on=\"file_base\", how=\"left\").drop(columns=[\"file_base\"])\n",
        "        bm[\"XGroup\"]   = bm[\"XGroup\"].fillna(\"UNASSIGNED\")\n",
        "        bm[\"HueGroup\"] = bm[\"HueGroup\"].fillna(\"UNASSIGNED\")\n",
        "    elif 'files_to_group' in globals() and files_to_group is not None and not files_to_group.empty:\n",
        "        m = files_to_group.copy()\n",
        "        m_src = _src_name(m)\n",
        "        if m_src is None:\n",
        "            raise RuntimeError(\"Grouping table must include 'filename' (or legacy 'File').\")\n",
        "        m[\"file_base\"]  = m[m_src].apply(_basename_col)\n",
        "        bm[\"file_base\"] = bm[\"filename\"].apply(_basename_col)\n",
        "        bm = bm.merge(m[[\"file_base\",\"Group\"]], on=\"file_base\", how=\"left\").drop(columns=[\"file_base\"])\n",
        "        bm[\"Group\"] = m[\"Group\"].fillna(\"UNASSIGNED\")\n",
        "        bm[\"XGroup\"] = bm[\"Group\"]\n",
        "        bm[\"HueGroup\"] = \"ALL\"\n",
        "    else:\n",
        "        raise RuntimeError(\"Missing X/Hue mapping. Run the grouping widget (Build Groups) first.\")\n",
        "def export_group_colors_to_global():\n",
        "    # Publish widget colors to a plain dict for other cells\n",
        "    globals()[\"group_colors\"] = {\n",
        "        g: (x_colors[g].value.strip() or \"tab:blue\")\n",
        "        for g in x_colors\n",
        "    }\n",
        "\n",
        "# -----------------------\n",
        "# 1) Melt to long format (numeric PR1 + fit params)\n",
        "# -----------------------\n",
        "base_metric_names = [\n",
        "    \"Left_Poke\", \"Right_Poke\", \"Total_Pokes\", \"Accuracy\", \"PokesPerPellet\",\n",
        "    \"MedianBreakPoint\", \"Numberofblocks\", \"daily pellets\",\n",
        "    \"Demand_Q0_raw\", \"Demand_alpha_raw\", \"Demand_beta_raw\",\n",
        "    \"Demand_alpha_FR\",\"Demand_beta_FR\",\n",
        "]\n",
        "\n",
        "metric_cols = []\n",
        "for c in bm.columns:\n",
        "    if pd.api.types.is_numeric_dtype(bm[c]):\n",
        "        for base in base_metric_names:\n",
        "            if c == base or c.startswith(base + \"_\"):\n",
        "                metric_cols.append(c); break\n",
        "seen = set(); metric_cols = [c for c in metric_cols if not (c in seen or seen.add(c))]\n",
        "if not metric_cols:\n",
        "    raise RuntimeError(\"No numeric metric columns found among expected Bandit metrics.\")\n",
        "\n",
        "candidate_id_vars = [\"Genotype\",\"Sex\",\"Strain\",\"Start_Date\",\"filename\",\"Mouse_ID\",\"Session_type\",\"XGroup\",\"HueGroup\"]\n",
        "id_vars = [c for c in candidate_id_vars if c in bm.columns]\n",
        "for need in [\"XGroup\",\"HueGroup\",\"filename\"]:\n",
        "    if need not in id_vars: id_vars.append(need)\n",
        "\n",
        "long_df = pd.melt(\n",
        "    bm,\n",
        "    id_vars=id_vars,\n",
        "    value_vars=metric_cols,\n",
        "    var_name=\"variable\",\n",
        "    value_name=\"value\"\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 2) Ordering helpers (hierarchical)\n",
        "# -----------------------\n",
        "def _is_wt_group(g):\n",
        "    \"\"\"\n",
        "    Returns True if the label includes any wildtype/control alias\n",
        "    (case-insensitive, robust to punctuation and composites).\n",
        "    \"\"\"\n",
        "    u = str(g).strip().upper()\n",
        "    tokens = [t for t in re.split(r'[^A-Z0-9]+', u) if t]\n",
        "    WT_ALIASES = {\"WT\", \"WILDTYPE\", \"CONTROL\", \"CTRL\"}  # add more if needed\n",
        "    return any(t in WT_ALIASES for t in tokens)\n",
        "\n",
        "def _hier_sort_key(g):\n",
        "    lv = _x_levels(g)\n",
        "    norm = []\n",
        "    for tok in lv:\n",
        "        is_blank = 1 if _is_unassigned_token(tok) else 0\n",
        "        norm.append((is_blank, str(tok).upper()))\n",
        "    wt_present = any(_is_wt_group(tok) for tok in lv) or _is_wt_group(g)\n",
        "    wt_rank = 0 if wt_present else 1\n",
        "    # make WT/CONTROL primary\n",
        "    return (wt_rank,) + tuple(norm) + (str(g).upper(),)\n",
        "\n",
        "def _is_unassigned_token(s):\n",
        "    return (str(s).strip().upper() in {\"\", \"UNASSIGNED\", \"NONE\", \"NA\", \"N/A\"})\n",
        "\n",
        "def _x_levels(xname):\n",
        "    s = str(xname)\n",
        "    parts = [p.strip() for p in s.split(\"|\")]\n",
        "    wanted = globals().get(\"selected_group_cols_x\", None)\n",
        "    if isinstance(wanted, (list, tuple)) and wanted:\n",
        "        if len(parts) < len(wanted):\n",
        "            parts += [\"\"] * (len(wanted) - len(parts))\n",
        "        else:\n",
        "            parts = parts[:len(wanted)]\n",
        "    return parts\n",
        "\n",
        "def _order_x_groups(groups):\n",
        "    return sorted(groups, key=_hier_sort_key)\n",
        "\n",
        "def _choose_ref_group(order):\n",
        "    for g in order:\n",
        "        if _is_wt_group(g):\n",
        "            return g\n",
        "    return order[0] if order else None\n",
        "\n",
        "all_x = [g for g in sorted(long_df[\"XGroup\"].dropna().unique().tolist()) if g != \"UNASSIGNED\"] or [\"UNASSIGNED\"]\n",
        "ordered_x = _order_x_groups(all_x)\n",
        "\n",
        "# -----------------------\n",
        "# 3) Controls (left column: groups & colors)\n",
        "# -----------------------\n",
        "named_defaults = [\n",
        "    \"dodgerblue\", \"red\", \"green\", \"orange\", \"purple\",\n",
        "    \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
        "\n",
        "x_checks, x_colors = {}, {}\n",
        "group_rows = []\n",
        "for i, g in enumerate(ordered_x):\n",
        "    chk = widgets.Checkbox(value=True, description=g, indent=False, layout=widgets.Layout(width=\"260px\"))\n",
        "    col = widgets.Text(value=named_defaults[i % len(named_defaults)],\n",
        "                       layout=widgets.Layout(width=\"120px\"))\n",
        "    x_checks[g] = chk\n",
        "    x_colors[g] = col\n",
        "    # more compact row\n",
        "    group_rows.append(widgets.HBox([chk, widgets.Label(\"\"), col],\n",
        "                                   layout=widgets.Layout(align_items=\"center\", height=\"28px\")))\n",
        "\n",
        "picker = widgets.VBox(group_rows, layout=widgets.Layout(gap=\"2px\"))\n",
        "\n",
        "btn_all  = widgets.Button(description=\"Select all\", layout=widgets.Layout(width=\"140px\"))\n",
        "btn_none = widgets.Button(description=\"Clear\", layout=widgets.Layout(width=\"140px\"))\n",
        "def _set_all(val):\n",
        "    for c in x_checks.values(): c.value = val\n",
        "btn_all.on_click(lambda _: _set_all(True))\n",
        "btn_none.on_click(lambda _: _set_all(False))\n",
        "\n",
        "# scrollable container for the (possibly long) group list\n",
        "picker_container = widgets.Box([picker],\n",
        "    layout=widgets.Layout(overflow=\"auto\", max_height=\"420px\",\n",
        "                          border=\"1px solid #ddd\", padding=\"6px\", width=\"360px\"))\n",
        "\n",
        "left_col = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Groups & Colors</b>\"),\n",
        "    widgets.HBox([btn_all, btn_none], layout=widgets.Layout(gap=\"8px\")),\n",
        "    picker_container\n",
        "], layout=widgets.Layout(width=\"380px\"))\n",
        "\n",
        "# -----------------------\n",
        "# 4) Comparison controls (right column)\n",
        "# -----------------------\n",
        "mode_radio = widgets.ToggleButtons(\n",
        "    options=[(\"Reference group\", \"ref\"), (\"Select Pairs\", \"pairs\")],\n",
        "    value=\"ref\", description=\"\", style={\"button_width\":\"150px\"},\n",
        "    layout=widgets.Layout(width=\"320px\")\n",
        ")\n",
        "show_stats_chk = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description=\"Show stats/ANOVA panel\",\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width=\"320px\")\n",
        ")\n",
        "ref_dropdown = widgets.Dropdown(\n",
        "    options=ordered_x, value=_choose_ref_group(ordered_x),\n",
        "    description=\"Reference:\", layout=widgets.Layout(width=\"320px\")\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "def _pair_label(a,b): return f\"{a} ⟷ {b}\"\n",
        "def _pair_value(a,b): return (a,b) if a <= b else (b,a)\n",
        "\n",
        "pairs_select = widgets.SelectMultiple(\n",
        "    options=[], value=[], description=\"Pairs\",\n",
        "    layout=widgets.Layout(width=\"360px\", height=\"320px\")\n",
        ")\n",
        "\n",
        "def _selected_x():\n",
        "    return _order_x_groups([g for g, cb in x_checks.items() if cb.value])\n",
        "\n",
        "def _pair_sort_key(a, b):\n",
        "    \"\"\"Rank pairs by level of first difference: later differences (within-group) rank first.\"\"\"\n",
        "    A = _x_levels(a); B = _x_levels(b)\n",
        "    L = max(len(A), len(B))\n",
        "    if len(A) < L: A += [\"\"] * (L - len(A))\n",
        "    if len(B) < L: B += [\"\"] * (L - len(B))\n",
        "    first_diff = next((i for i, (x, y) in enumerate(zip(A, B)) if x != y), L)\n",
        "    prefix = tuple(A[:first_diff])\n",
        "    return (-first_diff, prefix, tuple(A), tuple(B))\n",
        "\n",
        "def _update_ref_and_pairs(*_):\n",
        "    sel = _selected_x()\n",
        "    ref_dropdown.options = sel or [\"—\"]\n",
        "    if sel:\n",
        "        if ref_dropdown.value not in sel:\n",
        "            ref_dropdown.value = _choose_ref_group(sel)\n",
        "    else:\n",
        "        ref_dropdown.value = None\n",
        "\n",
        "    opts = []\n",
        "    for a, b in itertools.combinations(sel, 2):\n",
        "        lbl = _pair_label(a, b)\n",
        "        val = _pair_value(a, b)\n",
        "        opts.append((lbl, val))\n",
        "    opts.sort(key=lambda kv: _pair_sort_key(*kv[1]))\n",
        "    pairs_select.options = opts\n",
        "\n",
        "for cb in x_checks.values():\n",
        "    cb.observe(_update_ref_and_pairs, names=\"value\")\n",
        "_update_ref_and_pairs()\n",
        "\n",
        "# action buttons\n",
        "plot_btn = widgets.Button(description=\"Plot\", button_style=\"primary\",\n",
        "                          layout=widgets.Layout(width=\"160px\"))\n",
        "save_btn = widgets.Button(description=\"Save Plots\", button_style=\"success\",\n",
        "                          layout=widgets.Layout(width=\"160px\"))\n",
        "\n",
        "# Right column layout\n",
        "right_col = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Statistical comparisons</b>\"),\n",
        "    mode_radio,\n",
        "    show_stats_chk,\n",
        "    ref_dropdown,\n",
        "    pairs_select,\n",
        "    widgets.HBox([plot_btn, save_btn], layout=widgets.Layout(gap=\"8px\"))\n",
        "], layout=widgets.Layout(width=\"360px\"))\n",
        "# -----------------------\n",
        "# 5) Labels for stats/legend\n",
        "# -----------------------\n",
        "def _grouping_label(which=\"X\"):\n",
        "    if which.lower().startswith(\"x\"):\n",
        "        cols = globals().get(\"selected_group_cols_x\", [])\n",
        "        default = \"XGroup\"\n",
        "    else:\n",
        "        cols = globals().get(\"selected_group_cols_hue\", [])\n",
        "        default = \"HueGroup\"\n",
        "    cols = [str(c).strip() for c in (cols or []) if str(c).strip()]\n",
        "    return \" | \".join(cols) if cols else default\n",
        "\n",
        "# -----------------------\n",
        "# 6) Stats helpers (ANOVA with Hue)\n",
        "# -----------------------\n",
        "def _fmt_p(p):\n",
        "    if not np.isfinite(p): return \"n/a\"\n",
        "    return f\"p = {p:.3f}\" if p >= 0.001 else \"p < 0.001\"\n",
        "\n",
        "def _anova_subset(df):\n",
        "    \"\"\"\n",
        "    If >=2 Hue levels: two-way ANOVA (XGroup, HueGroup, interaction)\n",
        "    Else: one-way ANOVA (XGroup).\n",
        "    \"\"\"\n",
        "    out = {\"p_x\": np.nan, \"p_h\": np.nan, \"p_int\": np.nan, \"n_h\": 0, \"ok\": False, \"err\": None}\n",
        "    d = df.dropna(subset=[\"value\",\"XGroup\"])\n",
        "    if d.empty or d[\"XGroup\"].nunique() < 2:\n",
        "        out[\"err\"] = \"Too few groups\"; return out\n",
        "    n_h = d[\"HueGroup\"].nunique(dropna=True); out[\"n_h\"] = n_h\n",
        "    try:\n",
        "        if n_h >= 2:\n",
        "            model = ols('value ~ C(XGroup) + C(HueGroup) + C(XGroup):C(HueGroup)', data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            out[\"p_x\"]   = float(an.loc['C(XGroup)','PR(>F)'])\n",
        "            out[\"p_h\"]   = float(an.loc['C(HueGroup)','PR(>F)'])\n",
        "            out[\"p_int\"] = float(an.loc['C(XGroup):C(HueGroup)','PR(>F)'])\n",
        "            out[\"ok\"] = True\n",
        "        else:\n",
        "            model = ols('value ~ C(XGroup)', data=d).fit()\n",
        "            out[\"p_x\"] = float(model.f_pvalue); out[\"ok\"] = True\n",
        "    except Exception as e:\n",
        "        out[\"err\"] = str(e)\n",
        "    return out\n",
        "\n",
        "def _stats_text(dfm, x_label, hue_label, *, mode=\"ref\", ref_group=None, pair_list=None):\n",
        "    df = dfm.dropna(subset=[\"value\"]).copy()\n",
        "    g_n = df[\"XGroup\"].nunique(dropna=True)\n",
        "    h_n = df[\"HueGroup\"].nunique(dropna=True)\n",
        "\n",
        "    if mode == \"pairs\" and pair_list:\n",
        "        lines = [\"Selected pairwise ANOVA tests:\"]\n",
        "        for a,b in pair_list:\n",
        "            sub = df[df[\"XGroup\"].isin([a,b])]\n",
        "            res = _anova_subset(sub)\n",
        "            if not res[\"ok\"]:\n",
        "                lines.append(f\"{a} vs {b}: {res['err'] or 'failed'}\"); continue\n",
        "            if res[\"n_h\"] >= 2:\n",
        "                lines.append(\n",
        "                    f\"{a} vs {b} (Two-way: {x_label}, {hue_label})  \"\n",
        "                    f\"{x_label}: {_fmt_p(res['p_x'])} | {hue_label}: {_fmt_p(res['p_h'])} | \"\n",
        "                    f\"{x_label}×{hue_label}: {_fmt_p(res['p_int'])}\"\n",
        "                )\n",
        "            else:\n",
        "                lines.append(f\"{a} vs {b} (One-way {x_label}): {_fmt_p(res['p_x'])}\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def fmt(p): return _fmt_p(p)\n",
        "    if g_n == 2 and h_n <= 1:\n",
        "        g1, g2 = sorted(df[\"XGroup\"].unique())\n",
        "        v1 = df[df[\"XGroup\"] == g1][\"value\"].dropna()\n",
        "        v2 = df[df[\"XGroup\"] == g2][\"value\"].dropna()\n",
        "        if len(v1) > 1 and len(v2) > 1:\n",
        "            p = pg.ttest(v1, v2, paired=False)[\"p-val\"].values[0]\n",
        "            return f\"t-test ({x_label}): {fmt(p)}\\n{g1} vs {g2}\"\n",
        "        return \"t-test: not enough data\"\n",
        "\n",
        "    if g_n >= 2 and h_n >= 2:\n",
        "        try:\n",
        "            model = ols('value ~ C(XGroup) + C(HueGroup) + C(XGroup):C(HueGroup)', data=df).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            return (\n",
        "                \"Two-way ANOVA\\n\"\n",
        "                f\"{x_label}: {fmt(float(an.loc['C(XGroup)','PR(>F)']))}\\n\"\n",
        "                f\"{hue_label}: {fmt(float(an.loc['C(HueGroup)','PR(>F)']))}\\n\"\n",
        "                f\"{x_label}×{hue_label}: {fmt(float(an.loc['C(XGroup):C(HueGroup)','PR(>F)']))}\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            return f\"ANOVA failed: {e}\"\n",
        "\n",
        "    if g_n >= 2:\n",
        "        try:\n",
        "            model = ols('value ~ C(XGroup)', data=df).fit()\n",
        "            return f\"One-way ANOVA ({x_label}): {fmt(float(model.f_pvalue))}\"\n",
        "        except Exception as e:\n",
        "            return f\"One-way ANOVA failed: {e}\"\n",
        "    return \"Too few groups for stats\"\n",
        "def _fmt_p_num(p):\n",
        "    if p is None or (isinstance(p, float) and (not np.isfinite(p))):\n",
        "        return \"n/a\"\n",
        "    p = float(p)\n",
        "    return f\"{p:.4f}\" if p >= 0.0001 else \"<0.0001\"\n",
        "\n",
        "def _fmt_F(df_num, df_den, F):\n",
        "    if df_num is None or df_den is None or F is None:\n",
        "        return \"n/a\"\n",
        "    if not np.isfinite(F):\n",
        "        return \"n/a\"\n",
        "    return f\"F({int(df_num)}, {int(df_den)}) = {float(F):.3f}\"\n",
        "\n",
        "def _twoway_anova_full(df):\n",
        "    \"\"\"\n",
        "    Returns dict with full ANOVA stats including df for F.\n",
        "    If HueGroup has <2 levels -> one-way (XGroup only).\n",
        "    \"\"\"\n",
        "    d = df.dropna(subset=[\"value\",\"XGroup\"]).copy()\n",
        "    if d.empty or d[\"XGroup\"].nunique() < 2:\n",
        "        return {\"ok\": False, \"err\": \"Too few groups\"}\n",
        "\n",
        "    n_h = d[\"HueGroup\"].nunique(dropna=True) if \"HueGroup\" in d.columns else 0\n",
        "\n",
        "    try:\n",
        "        if n_h >= 2:\n",
        "            model = ols(\"value ~ C(XGroup) + C(HueGroup) + C(XGroup):C(HueGroup)\", data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            df_den = int(model.df_resid)\n",
        "\n",
        "            return {\n",
        "                \"ok\": True,\n",
        "                \"test\": \"Two-way ANOVA\",\n",
        "\n",
        "                \"F_x\": float(an.loc[\"C(XGroup)\", \"F\"]),\n",
        "                \"df_x_num\": int(an.loc[\"C(XGroup)\", \"df\"]),\n",
        "                \"df_x_den\": df_den,\n",
        "                \"p_x\": float(an.loc[\"C(XGroup)\", \"PR(>F)\"]),\n",
        "\n",
        "                \"F_h\": float(an.loc[\"C(HueGroup)\", \"F\"]),\n",
        "                \"df_h_num\": int(an.loc[\"C(HueGroup)\", \"df\"]),\n",
        "                \"df_h_den\": df_den,\n",
        "                \"p_h\": float(an.loc[\"C(HueGroup)\", \"PR(>F)\"]),\n",
        "\n",
        "                \"F_int\": float(an.loc[\"C(XGroup):C(HueGroup)\", \"F\"]),\n",
        "                \"df_int_num\": int(an.loc[\"C(XGroup):C(HueGroup)\", \"df\"]),\n",
        "                \"df_int_den\": df_den,\n",
        "                \"p_int\": float(an.loc[\"C(XGroup):C(HueGroup)\", \"PR(>F)\"]),\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            model = ols(\"value ~ C(XGroup)\", data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "            return {\n",
        "                \"ok\": True,\n",
        "                \"test\": \"One-way ANOVA\",\n",
        "                \"F_x\": float(an.loc[\"C(XGroup)\", \"F\"]),\n",
        "                \"df_x_num\": int(an.loc[\"C(XGroup)\", \"df\"]),\n",
        "                \"df_x_den\": int(model.df_resid),\n",
        "                \"p_x\": float(an.loc[\"C(XGroup)\", \"PR(>F)\"]),\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"ok\": False, \"err\": str(e)}\n",
        "\n",
        "def _subjects_n_per_group(dfm):\n",
        "    # n per XGroup (counts non-NaN values)\n",
        "    n_per_x = dfm.groupby(\"XGroup\")[\"value\"].apply(lambda s: int(s.dropna().shape[0]))\n",
        "    groups = _order_x_groups(dfm[\"XGroup\"].dropna().unique().tolist())\n",
        "    parts = [f\"{g}: n={n_per_x.get(g, 0)}\" for g in groups]\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "def _posthoc_ref_ttests(dfm, ref_group):\n",
        "    d = dfm.dropna(subset=[\"value\",\"XGroup\"]).copy()\n",
        "    ref = d[d[\"XGroup\"] == ref_group][\"value\"].dropna().to_numpy()\n",
        "    if ref_group is None or ref_group not in d[\"XGroup\"].unique():\n",
        "        return \"n/a\"\n",
        "\n",
        "    groups = [g for g in d[\"XGroup\"].unique().tolist() if g != ref_group]\n",
        "    lines = []\n",
        "    for g in _order_x_groups(groups):\n",
        "        vals = d[d[\"XGroup\"] == g][\"value\"].dropna().to_numpy()\n",
        "        if len(vals) < 2 or len(ref) < 2:\n",
        "            lines.append(f\"{g} vs {ref_group}: not enough data\")\n",
        "            continue\n",
        "        try:\n",
        "            p = float(pg.ttest(vals, ref, paired=False)[\"p-val\"].values[0])\n",
        "        except Exception:\n",
        "            p = np.nan\n",
        "        lines.append(f\"{g} vs {ref_group}: p={_fmt_p_num(p)} {_p_to_stars(p)}\")\n",
        "    return \" | \".join(lines) if lines else \"n/a\"\n",
        "\n",
        "def _posthoc_pairs_anova(dfm, pair_list, x_label, hue_label):\n",
        "    if not pair_list:\n",
        "        return \"n/a\"\n",
        "    lines = []\n",
        "    for a, b in pair_list:\n",
        "        sub = dfm[dfm[\"XGroup\"].isin([a, b])].dropna(subset=[\"value\"])\n",
        "        if sub[\"XGroup\"].nunique() < 2:\n",
        "            continue\n",
        "        res = _anova_subset(sub)  # uses your existing helper (p only)\n",
        "        if not res[\"ok\"]:\n",
        "            lines.append(f\"{a} vs {b}: {res['err'] or 'failed'}\")\n",
        "            continue\n",
        "        if res[\"n_h\"] >= 2:\n",
        "            lines.append(\n",
        "                f\"{a} vs {b}: {x_label} p={_fmt_p_num(res['p_x'])} {_p_to_stars(res['p_x'])}, \"\n",
        "                f\"{hue_label} p={_fmt_p_num(res['p_h'])}, int p={_fmt_p_num(res['p_int'])}\"\n",
        "            )\n",
        "        else:\n",
        "            lines.append(f\"{a} vs {b}: p={_fmt_p_num(res['p_x'])} {_p_to_stars(res['p_x'])}\")\n",
        "    return \" | \".join(lines) if lines else \"n/a\"\n",
        "\n",
        "def build_stats_table(long_df, metrics, sel_x, mode, ref_group=None, pair_list=None):\n",
        "    x_label_name   = _grouping_label(\"X\")\n",
        "    hue_label_name = _grouping_label(\"Hue\")\n",
        "\n",
        "    rows = []\n",
        "    for metric in metrics:\n",
        "        dfm = long_df[(long_df[\"variable\"] == metric) & (long_df[\"XGroup\"].isin(sel_x))].copy()\n",
        "        dfm = dfm.dropna(subset=[\"value\"])\n",
        "        if dfm.empty:\n",
        "            continue\n",
        "\n",
        "        stats = _twoway_anova_full(dfm)\n",
        "        subjects = _subjects_n_per_group(dfm)\n",
        "\n",
        "        if not stats.get(\"ok\", False):\n",
        "            rows.append({\n",
        "                \"Figure\": metric,\n",
        "                \"Test\": \"ANOVA failed\",\n",
        "                \"Subjects\": subjects,\n",
        "                \"F value interaction\": \"n/a\",\n",
        "                \"p value interaction\": \"n/a\",\n",
        "                \"Main effects\": stats.get(\"err\", \"unknown error\"),\n",
        "                \"Post hoc test\": \"n/a\",\n",
        "                \"Post hoc results\": \"n/a\",\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        if stats[\"test\"] == \"Two-way ANOVA\":\n",
        "            main_effects = (\n",
        "                f\"{x_label_name}: {_fmt_F(stats['df_x_num'], stats['df_x_den'], stats['F_x'])}, \"\n",
        "                f\"p={_fmt_p_num(stats['p_x'])} {_p_to_stars(stats['p_x'])}; \"\n",
        "                f\"{hue_label_name}: {_fmt_F(stats['df_h_num'], stats['df_h_den'], stats['F_h'])}, \"\n",
        "                f\"p={_fmt_p_num(stats['p_h'])} {_p_to_stars(stats['p_h'])}\"\n",
        "            )\n",
        "            f_int = _fmt_F(stats[\"df_int_num\"], stats[\"df_int_den\"], stats[\"F_int\"])\n",
        "            p_int = _fmt_p_num(stats[\"p_int\"]) + (f\" {_p_to_stars(stats['p_int'])}\" if np.isfinite(stats[\"p_int\"]) else \"\")\n",
        "        else:\n",
        "            main_effects = (\n",
        "                f\"{x_label_name}: {_fmt_F(stats['df_x_num'], stats['df_x_den'], stats['F_x'])}, \"\n",
        "                f\"p={_fmt_p_num(stats['p_x'])} {_p_to_stars(stats['p_x'])}\"\n",
        "            )\n",
        "            f_int, p_int = \"n/a\", \"n/a\"\n",
        "\n",
        "        if mode == \"ref\":\n",
        "            posthoc_test = \"Unpaired t-tests vs reference\"\n",
        "            posthoc_res  = _posthoc_ref_ttests(dfm, ref_group)\n",
        "        else:\n",
        "            posthoc_test = \"Selected pairwise ANOVA\"\n",
        "            posthoc_res  = _posthoc_pairs_anova(dfm, pair_list or [], x_label_name, hue_label_name)\n",
        "\n",
        "        rows.append({\n",
        "            \"Figure\": metric,\n",
        "            \"Test\": stats[\"test\"],\n",
        "            \"Subjects\": subjects,\n",
        "            \"F value interaction\": f_int,\n",
        "            \"p value interaction\": p_int,\n",
        "            \"Main effects\": main_effects,\n",
        "            \"Post hoc test\": posthoc_test,\n",
        "            \"Post hoc results\": posthoc_res,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# -----------------------\n",
        "# 7) Plotting helpers\n",
        "# -----------------------\n",
        "def _p_to_stars(p):\n",
        "    if not np.isfinite(p): return \"\"\n",
        "    if p < 1e-4: return \"****\"\n",
        "    if p < 1e-3: return \"***\"\n",
        "    if p < 1e-2: return \"**\"\n",
        "    if p < 5e-2: return \"*\"\n",
        "    return \"\"\n",
        "\n",
        "def _dot_palette(hues):\n",
        "    hues = list(hues)\n",
        "    if len(hues) == 0: return {}\n",
        "    if len(hues) == 1: return {hues[0]: \"black\"}\n",
        "    if len(hues) == 2: return {hues[0]: \"white\", hues[1]: \"black\"}\n",
        "    defaults = plt.rcParams.get('axes.prop_cycle', None)\n",
        "    colors = defaults.by_key()['color'] if defaults else [\"C0\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\"]\n",
        "    return {h: colors[i % len(colors)] for i, h in enumerate(hues)}\n",
        "\n",
        "def _draw_bracket(ax, x1, x2, y, h, text):\n",
        "    ax.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=\"black\", zorder=5)\n",
        "    ax.text((x1+x2)/2, y+h, text, ha=\"center\", va=\"bottom\", fontsize=16, fontweight=\"bold\")\n",
        "\n",
        "def _plot_metric_clean(df_metric, variable, x_color_map, *, mode=\"ref\", ref_group=None, pair_list=None, return_fig=False):\n",
        "    dfm = df_metric.copy()\n",
        "    order = _order_x_groups(dfm[\"XGroup\"].dropna().unique().tolist())\n",
        "    if not order: return None\n",
        "    if (not ref_group) or (ref_group not in order):\n",
        "        ref_group = _choose_ref_group(order)\n",
        "\n",
        "    x_label_name   = _grouping_label(\"X\")\n",
        "    hue_label_name = _grouping_label(\"Hue\")\n",
        "\n",
        "    hue_levels = [h for h in dfm[\"HueGroup\"].dropna().unique().tolist()]\n",
        "    pal_dots = _dot_palette(hue_levels)\n",
        "\n",
        "    width = max(2, 1 * len(order))\n",
        "    height = 4.0\n",
        "    fig, (ax_plot, ax_text) = plt.subplots(\n",
        "        1, 2, figsize=(width/1.2, height), gridspec_kw={'width_ratios': [3, 1]}\n",
        "    )\n",
        "\n",
        "    # Bars\n",
        "    bar_palette = [x_color_map.get(g, \"tab:blue\") for g in order]\n",
        "    sns.barplot(data=dfm, x=\"XGroup\", y=\"value\", order=order, ci=None, alpha=ALPHA, ax=ax_plot, palette=bar_palette)\n",
        "\n",
        "    # Determine hue levels in a controlled order\n",
        "    raw_hues = dfm[\"HueGroup\"].dropna().unique().tolist()\n",
        "    hue_levels = _order_hue_groups(raw_hues)\n",
        "\n",
        "    # Colors for dots — your helper already maps 2 hues as {hues[0]: \"white\", hues[1]: \"black\"}\n",
        "    pal_dots = _dot_palette(hue_levels)\n",
        "\n",
        "    # Points\n",
        "    sns.stripplot(\n",
        "        data=dfm, x=\"XGroup\", y=\"value\",\n",
        "        order=order,\n",
        "        hue=\"HueGroup\",\n",
        "        hue_order=hue_levels,        # <-- enforce hue order\n",
        "        jitter=True, dodge=False, size=7,\n",
        "        edgecolor=\"black\", linewidth=1,\n",
        "        palette=pal_dots,            # <-- colors aligned to hue_order\n",
        "        ax=ax_plot, zorder=3, alpha=ALPHA\n",
        "    )\n",
        "    if ax_plot.legend_ is not None:\n",
        "        ax_plot.legend_.remove()\n",
        "\n",
        "    # Legend (right panel) in the same hue order\n",
        "    if len(hue_levels) >= 2:\n",
        "        handles = [plt.Line2D([0],[0], marker='o', linestyle='None',\n",
        "                              markerfacecolor=pal_dots[h], markeredgecolor='black', label=str(h))\n",
        "                  for h in hue_levels]\n",
        "        ax_text.legend(handles=handles, title=hue_label_name, loc=\"upper left\", bbox_to_anchor=(0, 0.6))\n",
        "\n",
        "    # Annotations\n",
        "    y_min, y_max = ax_plot.get_ylim()\n",
        "    span = (y_max - y_min) if y_max > y_min else 1.0\n",
        "    bump = 0.06 * span\n",
        "    data_max = dfm[\"value\"].max() if dfm[\"value\"].notna().any() else y_max\n",
        "\n",
        "    if mode == \"ref\" and (ref_group in order):\n",
        "        ref_vals = dfm[dfm[\"XGroup\"] == ref_group][\"value\"].dropna().to_numpy()\n",
        "        for g in order:\n",
        "            if g == ref_group: continue\n",
        "            vals = dfm[dfm[\"XGroup\"] == g][\"value\"].dropna().to_numpy()\n",
        "            if len(vals) >= 2 and len(ref_vals) >= 2:\n",
        "                try:\n",
        "                    p = float(pg.ttest(vals, ref_vals, paired=False)[\"p-val\"].values[0])\n",
        "                except Exception:\n",
        "                    p = np.nan\n",
        "                if np.isfinite(p) and p < 0.05:\n",
        "                    xloc = order.index(g)\n",
        "                    gmax = dfm[dfm[\"XGroup\"] == g][\"value\"].max()\n",
        "                    y_star = (gmax if np.isfinite(gmax) else data_max) + bump\n",
        "                    ax_plot.text(xloc, y_star, _p_to_stars(p),\n",
        "                                 ha=\"center\", va=\"bottom\", fontsize=16, fontweight=\"bold\")\n",
        "                    y_max = max(y_max, y_star + bump)\n",
        "        ax_plot.set_ylim(y_min, y_max)\n",
        "\n",
        "    elif mode == \"pairs\" and pair_list:\n",
        "        base = (dfm[\"value\"].max() if dfm[\"value\"].notna().any() else y_max) + bump\n",
        "        step = 0.12 * span\n",
        "        k = 0\n",
        "        for a,b in pair_list:\n",
        "            if (a not in order) or (b not in order):\n",
        "                continue\n",
        "            sub = dfm[dfm[\"XGroup\"].isin([a,b])].dropna(subset=[\"value\"])\n",
        "            if sub[\"XGroup\"].nunique() < 2:\n",
        "                continue\n",
        "            res = _anova_subset(sub)\n",
        "            # draw bracket ONLY if XGroup effect significant\n",
        "            if res[\"ok\"] and np.isfinite(res[\"p_x\"]) and (res[\"p_x\"] < 0.05):\n",
        "                x1 = order.index(a); x2 = order.index(b)\n",
        "                if x1 > x2: x1, x2 = x2, x1\n",
        "                y_here = base + k * step\n",
        "                _draw_bracket(ax_plot, x1, x2, y_here, 0.04 * span, _p_to_stars(res[\"p_x\"]))\n",
        "                y_max = max(y_max, y_here + 0.08 * span)\n",
        "                k += 1\n",
        "        ax_plot.set_ylim(y_min, y_max)\n",
        "\n",
        "    ax_plot.set_title(\"\")\n",
        "    ax_plot.set_xlabel(\"\")\n",
        "    ax_plot.set_ylabel(variable)\n",
        "    plt.setp(ax_plot.get_xticklabels(), rotation=45, ha='right')\n",
        "    sns.despine(ax=ax_plot)\n",
        "\n",
        "    # Right panel: stats + Hue legend\n",
        "    ax_text.axis(\"off\")\n",
        "    ax_text.text(\n",
        "        0, 1,\n",
        "        _stats_text(dfm, x_label_name, hue_label_name, mode=mode, ref_group=ref_group, pair_list=pair_list),\n",
        "        va=\"top\", ha=\"left\", fontsize=12, transform=ax_text.transAxes\n",
        "    )\n",
        "    if len(hue_levels) >= 2:\n",
        "        handles = [plt.Line2D([0],[0], marker='o', linestyle='None',\n",
        "                              markerfacecolor=pal_dots[h], markeredgecolor='black', label=str(h))\n",
        "                   for h in hue_levels]\n",
        "        ax_text.legend(handles=handles, title=hue_label_name, loc=\"upper left\", bbox_to_anchor=(0, 0.6))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig if return_fig else plt.show()\n",
        "\n",
        "# -----------------------\n",
        "# 8) Actions\n",
        "# -----------------------\n",
        "out = widgets.Output()\n",
        "\n",
        "def _get_metrics_list():\n",
        "    exclude = {\"PeakAccuracy_Day\",\"PeakAccuracy_Night\",\n",
        "               \"Win-stay_Day\",\"Win-stay_Night\",\n",
        "               \"Lose-shift_Day\",\"Lose-shift_Night\"}\n",
        "    return [m for m in long_df[\"variable\"].dropna().unique() if m not in exclude]\n",
        "metrics = _get_metrics_list()\n",
        "\n",
        "def _selected_x_and_colors():\n",
        "    sel = _selected_x()\n",
        "    color_map = {}\n",
        "    for g in sel:\n",
        "        val = x_colors[g].value.strip()\n",
        "        color_map[g] = val if val else \"tab:blue\"\n",
        "    return sel, color_map\n",
        "\n",
        "def _current_pairs():\n",
        "    return list(pairs_select.value)\n",
        "\n",
        "def _run_plots(_=None):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        sel_x, color_map = _selected_x_and_colors()\n",
        "        if len(sel_x) < 1:\n",
        "            print(\"Select at least one X group.\"); return\n",
        "\n",
        "        mode = mode_radio.value\n",
        "        if mode == \"ref\":\n",
        "            ref = ref_dropdown.value if (ref_dropdown.value in sel_x) else _choose_ref_group(sel_x)\n",
        "            print(f\"Showing X groups: {sel_x}  |  reference for stars: {ref}\")\n",
        "        else:\n",
        "            pair_list = _current_pairs()\n",
        "            if not pair_list:\n",
        "                print(f\"Showing X groups: {sel_x}  |  no pairs selected (select at least one).\"); return\n",
        "            print(f\"Showing X groups: {sel_x}  |  pairs: {pair_list}\")\n",
        "\n",
        "        exclude = {\"PeakAccuracy_Day\",\"PeakAccuracy_Night\",\n",
        "                   \"Win-stay_Day\",\"Win-stay_Night\",\n",
        "                   \"Lose-shift_Day\",\"Lose-shift_Night\"}\n",
        "        metrics = [m for m in long_df[\"variable\"].dropna().unique() if m not in exclude]\n",
        "\n",
        "        for metric in metrics:\n",
        "            subset = long_df[(long_df[\"variable\"] == metric) & (long_df[\"XGroup\"].isin(sel_x))]\n",
        "            if subset[\"value\"].dropna().empty:\n",
        "                print(f\"Skipping {metric} — no data for selected X groups.\"); continue\n",
        "            if mode == \"ref\":\n",
        "                _plot_metric_clean(\n",
        "                    subset, metric,\n",
        "                    x_color_map={g: color_map[g] for g in sel_x if g in subset['XGroup'].unique()},\n",
        "                    mode=\"ref\", ref_group=ref\n",
        "                )\n",
        "            else:\n",
        "                _plot_metric_clean(\n",
        "                    subset, metric,\n",
        "                    x_color_map={g: color_map[g] for g in sel_x if g in subset['XGroup'].unique()},\n",
        "                    mode=\"pairs\", pair_list=_current_pairs()\n",
        "                )\n",
        "\n",
        "def _save_plots(_=None):\n",
        "    with out:\n",
        "        clear_output()\n",
        "\n",
        "        sel_x, color_map = _selected_x_and_colors()\n",
        "        if len(sel_x) < 1:\n",
        "            print(\"Select at least one X group.\"); return\n",
        "\n",
        "        mode = mode_radio.value\n",
        "        ref = ref_dropdown.value if (mode == \"ref\") else None\n",
        "        pair_list = _current_pairs() if (mode == \"pairs\") else None\n",
        "        if mode == \"pairs\" and not pair_list:\n",
        "            print(\"Select at least one pair before saving.\"); return\n",
        "\n",
        "        # ---------- output folder name like strain_strainnum_task_Figures ----------\n",
        "        src_df = globals().get(\"Banditmetrics_merged\", None)\n",
        "        if src_df is None or src_df.empty:\n",
        "            src_df = bm\n",
        "\n",
        "        example = src_df.iloc[0]\n",
        "        strain_name = str(example.get(\"Gene\", example.get(\"Strain\", \"Bandit\"))).replace(\" \", \"_\")\n",
        "\n",
        "        strain_num_raw = example.get(\"Gene_ID\", example.get(\"Strain_ID\", \"Metrics\"))\n",
        "        try:\n",
        "            strain_num = f\"{int(strain_num_raw):03d}\"\n",
        "        except Exception:\n",
        "            strain_num = str(strain_num_raw).zfill(3)\n",
        "\n",
        "        task_name = str(example.get(\"Session_type\", \"Unknown\")).replace(\" \", \"_\")\n",
        "\n",
        "        out_dir = f\"{strain_name}_{strain_num}_{task_name}_Figures\"\n",
        "\n",
        "        # wipe folder so it doesn't accumulate (prevents \"double saving\" inside zip)\n",
        "        if os.path.exists(out_dir):\n",
        "            shutil.rmtree(out_dir)\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "        metrics = _get_metrics_list()\n",
        "\n",
        "        # choose ref if needed\n",
        "        if mode == \"ref\":\n",
        "            if (ref is None) or (ref not in sel_x):\n",
        "                ref = _choose_ref_group(sel_x)\n",
        "\n",
        "        # ---- build + save stats table into the SAME folder ----\n",
        "        if mode == \"ref\":\n",
        "            stats_df = build_stats_table(long_df, metrics, sel_x, mode=\"ref\", ref_group=ref)\n",
        "        else:\n",
        "            stats_df = build_stats_table(long_df, metrics, sel_x, mode=\"pairs\", pair_list=pair_list)\n",
        "\n",
        "        stats_path = f\"{out_dir}/Bandit_stats_table.xlsx\"\n",
        "        stats_df.to_excel(stats_path, index=False)\n",
        "\n",
        "        # ---- save figures ----\n",
        "        saved = 0\n",
        "        for metric in metrics:\n",
        "            subset = long_df[(long_df[\"variable\"] == metric) & (long_df[\"XGroup\"].isin(sel_x))]\n",
        "            if subset[\"value\"].dropna().empty:\n",
        "                continue\n",
        "\n",
        "            fig = _plot_metric_clean(\n",
        "                subset, metric,\n",
        "                x_color_map={g: color_map[g] for g in sel_x if g in subset[\"XGroup\"].unique()},\n",
        "                mode=mode, ref_group=ref, pair_list=pair_list, return_fig=True\n",
        "            )\n",
        "            safe = metric.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
        "            fig.savefig(f\"{out_dir}/{safe}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
        "            plt.close(fig)\n",
        "            saved += 1\n",
        "\n",
        "        if saved == 0:\n",
        "            print(\"No figures to save.\"); return\n",
        "\n",
        "        # ---- zip ONCE ----\n",
        "        zipname = f\"{out_dir}_{int(time.time())}.zip\"\n",
        "        shutil.make_archive(zipname.replace(\".zip\", \"\"), \"zip\", out_dir)\n",
        "\n",
        "        if colab_files is not None:\n",
        "            colab_files.download(zipname)\n",
        "\n",
        "        print(f\"Saved {zipname}\")\n",
        "        print(f\"Included stats table: {stats_path}\")\n",
        "try:\n",
        "    plot_btn._click_handlers.callbacks = []\n",
        "    save_btn._click_handlers.callbacks = []\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "plot_btn.on_click(_run_plots)\n",
        "save_btn.on_click(_save_plots)\n",
        "\n",
        "# -----------------------\n",
        "# 9) Assemble compact UI (two columns)\n",
        "# -----------------------\n",
        "# Toggle visibility of ref vs pairs widgets\n",
        "def _toggle_controls(*_):\n",
        "    if mode_radio.value == \"ref\":\n",
        "        ref_dropdown.layout.display = \"\"\n",
        "        pairs_select.layout.display = \"none\"\n",
        "    else:\n",
        "        ref_dropdown.layout.display = \"none\"\n",
        "        pairs_select.layout.display = \"\"\n",
        "_toggle_controls()\n",
        "mode_radio.observe(lambda _: _toggle_controls(), names=\"value\")\n",
        "\n",
        "# Two-column layout container\n",
        "row = widgets.HBox(\n",
        "    [left_col, right_col],\n",
        "    layout=widgets.Layout(\n",
        "        justify_content=\"flex-start\",   # keep columns together\n",
        "        align_items=\"flex-start\",\n",
        "        gap=\"16px\",\n",
        "        width=\"auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='margin-bottom:6px'></h3>\"),\n",
        "    row,\n",
        "    out\n",
        "], layout=widgets.Layout(width=\"auto\"))\n",
        "\n",
        "display(ui)\n",
        "\n",
        "# Auto-run once\n",
        "_run_plots()\n"
      ],
      "metadata": {
        "id": "dKKkCThnHE0H",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Grouped Demand Curves\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import sem\n",
        "from itertools import cycle\n",
        "import os\n",
        "\n",
        "def plot_group_mean_demand_with_params(group_col='XGroup', mapping_obj_name='files_to_group_both',\n",
        "                                       price_grid=None, n_grid=120, P_min=1.0, fallback_max_P=128,\n",
        "                                       annotate_x_frac=0.18):\n",
        "\n",
        "    # --- mapping ---\n",
        "    mapping = None\n",
        "    for name in (mapping_obj_name, 'files_to_group_both', 'files_to_group_x', 'files_to_group_hue'):\n",
        "        if name in globals():\n",
        "            mapping = globals()[name].copy()\n",
        "            break\n",
        "    if mapping is None:\n",
        "        _meta = metadata_df.copy()\n",
        "        _meta['filename'] = _meta['filename'].apply(lambda p: os.path.basename(str(p))) if 'filename' in _meta.columns else _meta.index.astype(str)\n",
        "        mapping = pd.DataFrame({'filename': _meta['filename'], group_col: 'ALL'})\n",
        "\n",
        "    if group_col not in mapping.columns:\n",
        "        raise ValueError(f\"mapping does not contain group column '{group_col}'\")\n",
        "\n",
        "    # filename -> Mouse_ID\n",
        "    if 'md' in globals() and 'filename' in md.columns and 'Mouse_ID' in md.columns:\n",
        "        fname_to_mouse = md.set_index('filename')['Mouse_ID'].to_dict()\n",
        "    elif 'bm' in globals() and 'filename' in bm.columns and 'Mouse_ID' in bm.columns:\n",
        "        fname_to_mouse = bm.set_index('filename')['Mouse_ID'].to_dict()\n",
        "    else:\n",
        "        raise RuntimeError(\"Cannot find filename -> Mouse_ID mapping (need `md` or `bm`).\")\n",
        "\n",
        "    # demand params table\n",
        "    if 'bm' not in globals():\n",
        "        raise RuntimeError(\"Need `bm` (PRmetrics_merged copy) with Demand_alpha_FR & Demand_beta_FR.\")\n",
        "    metrics = bm.copy()\n",
        "    alpha_col = 'Demand_alpha_FR'\n",
        "    beta_col  = 'Demand_beta_FR'\n",
        "    if alpha_col not in metrics.columns or beta_col not in metrics.columns:\n",
        "        raise RuntimeError(f\"`bm` must contain '{alpha_col}' and '{beta_col}'.\")\n",
        "\n",
        "    # normalize mapping file basenames\n",
        "    mapping['filename'] = mapping['filename'].astype(str).apply(lambda p: os.path.basename(str(p)))\n",
        "\n",
        "    # choose price grid if not provided\n",
        "    if price_grid is None:\n",
        "        maxP_candidates = []\n",
        "        max_alpha = metrics[alpha_col].dropna().max()\n",
        "        if pd.notna(max_alpha):\n",
        "            maxP_candidates.append(float(max_alpha) * 8.0)\n",
        "        if 'demand_raw_df' in globals() and 'PricePaid' in demand_raw_df.columns:\n",
        "            maxP_candidates.append(int(demand_raw_df['PricePaid'].max()))\n",
        "        maxP = int(max(maxP_candidates) if maxP_candidates else fallback_max_P)\n",
        "        maxP = max(maxP, P_min + 1)\n",
        "        price_grid = np.unique(np.logspace(np.log10(max(P_min, 1.0)), np.log10(maxP), n_grid))\n",
        "    else:\n",
        "        price_grid = np.asarray(price_grid, dtype=float)\n",
        "\n",
        "    def predict_Q(P, alpha, beta):\n",
        "        P = np.asarray(P, dtype=float)\n",
        "        return 100.0 / (1.0 + (P / alpha) ** beta)\n",
        "\n",
        "    groups = mapping[group_col].dropna().unique().tolist()\n",
        "\n",
        "    # colors from widget\n",
        "    if 'group_colors' in globals() and isinstance(group_colors, dict):\n",
        "        color_map = {g: group_colors.get(g, None) for g in groups}\n",
        "    else:\n",
        "        color_map = {g: None for g in groups}\n",
        "\n",
        "    default_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
        "\n",
        "    # ONE figure / axes for all groups\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    stat_rows = []\n",
        "    for gi, g in enumerate(groups):\n",
        "        subset = mapping.loc[mapping[group_col] == g, 'filename'].unique().tolist()\n",
        "        mice = [fname_to_mouse.get(os.path.basename(f), None) for f in subset]\n",
        "        unique_mice = sorted({m for m in mice if pd.notna(m)})\n",
        "\n",
        "        mice_with_params = []\n",
        "        for m in unique_mice:\n",
        "            row = metrics.loc[metrics['Mouse_ID'] == m]\n",
        "            if row.empty:\n",
        "                continue\n",
        "            a = row[alpha_col].values[0]\n",
        "            b = row[beta_col].values[0]\n",
        "            if pd.isna(a) or pd.isna(b):\n",
        "                continue\n",
        "            if float(a) <= 0 or float(b) <= 0:\n",
        "                continue\n",
        "            mice_with_params.append((m, float(a), float(b)))\n",
        "\n",
        "        if len(mice_with_params) == 0:\n",
        "            print(f\"[Group '{g}'] no mice with valid alpha/beta — skipped.\")\n",
        "            continue\n",
        "\n",
        "        Qs = np.vstack([predict_Q(price_grid, a, b) for (_m,a,b) in mice_with_params])\n",
        "        mean_Q = np.nanmean(Qs, axis=0)\n",
        "        sem_Q  = sem(Qs, axis=0, nan_policy='omit')\n",
        "\n",
        "        color = color_map.get(g)\n",
        "        if not color:\n",
        "            color = next(default_cycle)\n",
        "\n",
        "        alphas = np.array([a for (_m,a,b) in mice_with_params])\n",
        "        betas  = np.array([b for (_m,a,b) in mice_with_params])\n",
        "        a_mean, a_sem = alphas.mean(), sem(alphas)\n",
        "        b_mean, b_sem = betas.mean(), sem(betas)\n",
        "\n",
        "        Q_meanparams_at_alpha = predict_Q(a_mean, a_mean, b_mean)\n",
        "        ix_closest = np.argmin(np.abs(price_grid - a_mean))\n",
        "        P_meancurve = price_grid[ix_closest]\n",
        "        Q_meancurve = mean_Q[ix_closest]\n",
        "\n",
        "        print(f\"[{g}]\")\n",
        "        print(f\"  alpha_mean = {a_mean:.4f}, beta_mean = {b_mean:.4f}\")\n",
        "        print(f\"  Q_meanparams(P = alpha_mean): P = {a_mean:.4f}, Q = {Q_meanparams_at_alpha:.4f}\")\n",
        "        print(f\"  Q_meancurve (nearest alpha_mean): P = {P_meancurve:.4f}, Q = {Q_meancurve:.4f}\")\n",
        "        print()\n",
        "\n",
        "        # curve + SEM\n",
        "        ax.plot(price_grid, mean_Q, label=g, color=color, linewidth=2.8)\n",
        "        ax.fill_between(price_grid, mean_Q - sem_Q, mean_Q + sem_Q,\n",
        "                        alpha=0.22, edgecolor='none', linewidth=0, facecolor=color)\n",
        "\n",
        "        # marker & vertical line at alpha\n",
        "        x_alpha = P_meancurve\n",
        "        y_alpha = Q_meancurve\n",
        "        ax.vlines(x_alpha, 0, y_alpha, linestyle='--', linewidth=1.6,\n",
        "                  color=color, alpha=0.8)\n",
        "        ax.plot(x_alpha, y_alpha, 'o', color=color, markersize=6)\n",
        "\n",
        "        # text, alternating above/below\n",
        "        text_x = x_alpha * 1.05\n",
        "        vertical_offset = 4\n",
        "        direction = 1 if gi % 2 == 0 else -1\n",
        "        text_y = y_alpha + direction * vertical_offset\n",
        "        text = rf\"α={a_mean:.2f}, β={b_mean:.3f}\"\n",
        "        ax.text(text_x, text_y, text, color=color, fontsize=10,\n",
        "                ha='left', va='center')\n",
        "\n",
        "        stat_rows.append({\n",
        "            'Group': g,\n",
        "            'N_mice': len(mice_with_params),\n",
        "            'alpha_mean': a_mean, 'alpha_sem': a_sem,\n",
        "            'beta_mean': b_mean,  'beta_sem': b_sem\n",
        "        })\n",
        "\n",
        "    # axes / legend\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xlabel('Food Price (FR)')\n",
        "    ax.set_yticks(np.arange(0, 101, 20)); ax.set_ylim(0, 105)\n",
        "    ax.set_xlim(1, 100)\n",
        "    ticks = np.array([1,2,3,5,10,15,20,30,40,50,75,100], float)\n",
        "    ax.set_xticks(ticks); ax.set_xticklabels([str(int(x)) for x in ticks])\n",
        "    ax.set_ylabel('Consumption')\n",
        "    ax.set_title('')\n",
        "    ax.spines['left'].set_position(('data', 1))\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_linewidth(1.5)\n",
        "    ax.tick_params(axis='both', which='both', width=1.5, length=6)\n",
        "\n",
        "    ax.legend(title=\"Group\", frameon=False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    stat_df = pd.DataFrame(stat_rows).sort_values('Group').reset_index(drop=True)\n",
        "    if not stat_df.empty:\n",
        "        display(stat_df[['Group','N_mice','alpha_mean','alpha_sem','beta_mean','beta_sem']].round(3))\n",
        "    return fig, stat_df\n",
        "\n",
        "from google.colab import files as gfiles\n",
        "\n",
        "fig, stats = plot_group_mean_demand_with_params(group_col='XGroup')\n",
        "\n",
        "# save\n",
        "outdir = \"/content\"\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "fname = f\"demand_plot_{datetime.now():%Y%m%d_%H%M%S}.pdf\"\n",
        "path = os.path.join(outdir, fname)\n",
        "fig.savefig(path, format=\"pdf\", dpi=300, bbox_inches=\"tight\", transparent=True)\n",
        "gfiles.download(path)\n",
        "\n",
        "# show\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aBD2-fmtZxET",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Individual normalized demand curves fit with a (Q_0)-fixed log-logistic model**  \n",
        "Each plot shows **normalized** pellet consumption (Q) (gray points) as a function of normalized price (*P*)for a single animal. For each animal, we normalized the raw data by setting the lowest-price consumption to 100% and rescaling both axes: (q = 100/B) where (B) is consumption at the lowest price; then (P = Price X q) and (Q = Consumption X q). This forces all curves to start at (Q=100), removing trivial level differences and enabling like-for-like comparisons of elasticity across animals.\n",
        "\n",
        "The fitted demand curve (black line) was obtained by nonlinear least squares using a (Q_0)-fixed log-logistic model:![image.png](data:image/png;base64,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)\n",
        "\n",
        "**Where**  \n",
        "- alpha is the normalized price at which consumption is halved ((Q=50)),  \n",
        "- beta captures the local steepness of the decline around alpha.  \n",
        "**Each plot displays**:  \n",
        "\n",
        "- Gray points: normalized observed data,  \n",
        "- Black line: fitted demand curve,  \n",
        "- Red dashed line and red point: alpha half-max price, (Q=50),  \n",
        "- Light blue line: local slope at alpha.  \n",
        "**Interpretation**\n",
        "Because normalization fixes (Q_0=100), (Q_0) is no longer a free parameter and level differences are removed. Thus:  \n",
        "\n",
        "- alpha indexes cost sensitivity on a common scale; higher alpha indicates greater willingness to pay before consumption falls to half.  \n",
        "- beta quantifies how sharply consumption declines as (P) increases (larger beta → steeper drop).    \n",
        "\n",
        "Optionally, we also report (Pmax) (vertical dotted line when included), defined as the price where elasticity equals (-1) and response output peaks."
      ],
      "metadata": {
        "id": "pp9_eOTdpJE6"
      }
    }
  ]
}