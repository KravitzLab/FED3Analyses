{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/PsygeneAnalyses/blob/PCA_analysis/BEAM_spectral_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is a notebook for Generating spectral data from BEAM for the Psygene project.  \n",
        "<br>\n",
        "<img src=\"https://github.com/KravitzLab/KreedLabWiki/blob/main/images/ChatGPT%20Image%20Apr%2020,%202025,%2004_05_24%20PM.png?raw=true\" width=\"300\" />\n",
        "\n",
        "Updated: 04-18-25\n",
        "\n",
        "Todo:\n",
        "- normalize data per day\n",
        "- think about metrics that we could drop?\n",
        "- rename all_metrics to something task specific like beam_metrics"
      ],
      "metadata": {
        "id": "_ByYgNRzBXRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Import libraries\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "from scipy.signal import find_peaks\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "packages = {\n",
        "    \"ipywidgets\": \"ipywidgets\",\n",
        "}\n",
        "\n",
        "for name, source in packages.items():\n",
        "    if importlib.util.find_spec(name) is None:\n",
        "        print(f\"Installing {name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", source])\n",
        "\n",
        "import tempfile\n",
        "import os\n",
        "import zipfile\n",
        "import io\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from statsmodels.formula.api import ols\n",
        "import statsmodels.api as sm\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import warnings\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "from scipy.optimize import curve_fit\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import matplotlib.dates as mdates\n",
        "warnings.filterwarnings('ignore')  # this is a bit dangerous but we'll supress all warnings\n",
        "\n",
        "print(\"Packages installed.\")\n"
      ],
      "metadata": {
        "id": "Cbn8lmxKhJsY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Upload BEAM files\n",
        "\n",
        "\n",
        "import os, io, zipfile, re\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "def upload_BEAM_files(append=False):\n",
        "    \"\"\"\n",
        "    If append=False (default), this replaces global `dataframes` and `loaded_files`.\n",
        "    If append=True, new files are appended to existing state.\n",
        "    \"\"\"\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        raise ValueError(\"No files uploaded.\")\n",
        "\n",
        "    # start fresh unless appending\n",
        "    if not append:\n",
        "        globals().pop(\"dataframes\", None)\n",
        "        globals().pop(\"loaded_files\", None)\n",
        "\n",
        "    dfs_existing  = globals().get(\"dataframes\", [])\n",
        "    names_existing = globals().get(\"loaded_files\", [])\n",
        "\n",
        "    dfs_new, names_new = [], []\n",
        "\n",
        "    # continue file_id numbering if appending; otherwise start at 1\n",
        "    def _next_start():\n",
        "        try:\n",
        "            ids = [int(str(getattr(df, \"file_id\", df.get(\"file_id\", \"file_000\")))[5:]) for df in dfs_existing]\n",
        "            return (max(ids) + 1) if ids else 1\n",
        "        except Exception:\n",
        "            return 1\n",
        "\n",
        "    next_id = _next_start() if append else 1\n",
        "\n",
        "    for name, content in uploaded.items():\n",
        "        name_str = str(name)\n",
        "\n",
        "        if name_str.lower().endswith(\".zip\"):\n",
        "            with zipfile.ZipFile(io.BytesIO(content), \"r\") as z:\n",
        "                for member in z.namelist():\n",
        "                    # keep only CSV files\n",
        "                    if member.endswith(\"/\") or (not member.lower().endswith(\".csv\")):\n",
        "                        continue\n",
        "                    with z.open(member) as f:\n",
        "                        df = pd.read_csv(f)\n",
        "                    df[\"source_file\"] = f\"{os.path.basename(name_str)}::{member}\"\n",
        "                    df[\"file_id\"] = f\"file_{next_id:03d}\"\n",
        "                    dfs_new.append(df)\n",
        "                    # record the INNER CSV basename (not the ZIP name)\n",
        "                    names_new.append(os.path.basename(member))\n",
        "                    next_id += 1\n",
        "\n",
        "        elif name_str.lower().endswith(\".csv\"):\n",
        "            df = pd.read_csv(io.BytesIO(content))\n",
        "            df[\"source_file\"] = os.path.basename(name_str)\n",
        "            df[\"file_id\"] = f\"file_{next_id:03d}\"\n",
        "            dfs_new.append(df)\n",
        "            names_new.append(os.path.basename(name_str))\n",
        "            next_id += 1\n",
        "\n",
        "        else:\n",
        "            print(f\"[i] Skipping non-CSV file: {name_str}\")\n",
        "\n",
        "    if not dfs_new:\n",
        "        raise ValueError(\"No CSV files found. Upload .csv or a .zip containing CSVs.\")\n",
        "\n",
        "    # commit to globals\n",
        "    dataframes   = (dfs_existing + dfs_new) if append else dfs_new\n",
        "    loaded_files = (names_existing + names_new) if append else names_new\n",
        "    globals()[\"dataframes\"] = dataframes\n",
        "    globals()[\"loaded_files\"] = loaded_files\n",
        "\n",
        "    print(f\"Loaded {len(dfs_new)} CSV(s) from {len(uploaded)} upload(s). \"\n",
        "          f\"Current session now has {len(dataframes)} CSV(s).\")\n",
        "    print(\"Example filenames used for metadata linking:\", loaded_files[:5])\n",
        "    return dataframes, loaded_files\n",
        "\n",
        "# Use it:\n",
        "dataframes, loaded_files = upload_BEAM_files(append=False)\n",
        "\n",
        "print(type(dataframes))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tHGcTfSd82q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Upload Key\n",
        "key_df = files.upload()\n",
        "key_df = pd.read_excel(list(key_df.keys())[0])\n",
        "\n",
        "# Remove $ from columns names\n",
        "key_df.columns = key_df.columns.str.replace('$', '')\n",
        "\n",
        "keeps = [\"Genotype\", \"Gene\", \"Sex\", \"Mouse_ID\"]\n",
        "key_df = key_df[keeps]\n",
        "\n",
        "print(key_df)\n",
        "\n",
        "\n",
        "### create the download for a metafile ###\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "csv_name2 = f\"spectral_key.csv\"\n",
        "down_df2 = key_df.to_csv(csv_name2, index=False)\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(csv_name2)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(csv_name2)}</code>…\"\n",
        "        gfiles.download(csv_name2)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{csv_name2}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8mZsrwLW7AzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Average by animals\n",
        "\n",
        "dfs = dataframes.copy()\n",
        "\n",
        "# the column you want to group by\n",
        "group_col = 'Mouse_ID'\n",
        "# common time interval for BEAMs\n",
        "resample_freq = '10T'\n",
        "\n",
        "grouped_results = {}\n",
        "\n",
        "for df in dfs:\n",
        "    df = df.copy()\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "    # Extract the group label (keep for later)\n",
        "    group = df[group_col].iloc[0]\n",
        "\n",
        "    # Set datetime index\n",
        "    df = df.set_index('datetime').sort_index()\n",
        "\n",
        "    # Select only numeric columns for averaging (keep group info separate)\n",
        "    numeric_cols = df.select_dtypes(include='number').columns\n",
        "    df_numeric = df[numeric_cols]\n",
        "\n",
        "    # Resample and interpolate\n",
        "    df_numeric = df_numeric.resample(resample_freq).mean().interpolate(limit_direction='both')\n",
        "\n",
        "    # Add to grouped list\n",
        "    grouped_results.setdefault(group, []).append(df_numeric)\n",
        "\n",
        "# Now average within each group\n",
        "group_averages = {}\n",
        "for group, df_list in grouped_results.items():\n",
        "    combined = pd.concat(df_list, axis=1)\n",
        "    avg = combined.groupby(level=0, axis=1).mean()  # average numeric columns\n",
        "    group_averages[group] = avg\n",
        "\n",
        "# Check available groups\n",
        "print(\"Groups available:\", list(group_averages.keys()))\n",
        "\n",
        "# Access a group’s averaged DataFrame\n",
        "avg_df = group_averages[list(group_averages.keys())[0]]\n",
        "\n",
        "\n",
        "\n",
        "# This is code for plotting all beam activity TS\n",
        "\"\"\"\n",
        "columns_to_plot = ['activity_count']\n",
        "\n",
        "for group, avg_df in group_averages.items():\n",
        "    n_cols = len(columns_to_plot)\n",
        "    fig, axes = plt.subplots(n_cols, 1, figsize=(12, 3*n_cols), sharex=True)\n",
        "\n",
        "    # Ensure axes is always a list/array\n",
        "    if n_cols == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, col in enumerate(columns_to_plot):\n",
        "        ax = axes[i]\n",
        "        ax.plot(avg_df.index, avg_df[col], color='blue')\n",
        "        ax.set_ylabel(col)\n",
        "        ax.grid(True)\n",
        "        ax.set_title(f'{group} - {col}', fontsize=10)\n",
        "        ax.autoscale(enable=True)  # y-axis scales independently\n",
        "\n",
        "    axes[-1].set_xlabel('Time')\n",
        "    plt.suptitle(f'Averaged Time Series for Group: {group}', fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "XPoE52VETDzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title define functions for peak and valley analysis\n",
        "# peak and valley information\n",
        "\n",
        "def peak_valley_analysis(grouped_dict, column=\"activity_count\",\n",
        "                         peak_prominence=5, valley_prominence=5):\n",
        "    \"\"\"\n",
        "    grouped_dict: dict of {group_name: DataFrame}, each with datetime index\n",
        "    column: which column to analyze\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for group, df in grouped_dict.items():\n",
        "\n",
        "        # --- ensure sorted time index ---\n",
        "        df = df.sort_index()\n",
        "\n",
        "        y = df[column].values\n",
        "\n",
        "        # --- Find peaks ---\n",
        "        peak_idx, _ = find_peaks(y, prominence=peak_prominence)\n",
        "        peak_times = df.index[peak_idx]\n",
        "        peak_values = y[peak_idx]\n",
        "\n",
        "        # --- Find valleys (invert signal) ---\n",
        "        valley_idx, _ = find_peaks(-y, prominence=valley_prominence)\n",
        "        valley_times = df.index[valley_idx]\n",
        "        valley_values = y[valley_idx]\n",
        "\n",
        "        results[group] = {\n",
        "            \"peaks\": pd.DataFrame({\"time\": peak_times, \"value\": peak_values}),\n",
        "            \"valleys\": pd.DataFrame({\"time\": valley_times, \"value\": valley_values}),\n",
        "            \"n_peaks\": len(peak_idx),\n",
        "            \"n_valleys\": len(valley_idx),\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "results = peak_valley_analysis(group_averages, column=\"activity_count\")\n",
        "\n",
        "\n",
        "### Define function to plot peaks and valleys ###\n",
        "def plot_group_peaks(group_name):\n",
        "    df = group_averages[group_name]\n",
        "    df = df.sort_index()\n",
        "\n",
        "    res = results[group_name]\n",
        "\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.plot(df.index, df[\"activity_count\"], label=\"Activity\")\n",
        "\n",
        "    plt.scatter(res[\"peaks\"][\"time\"], res[\"peaks\"][\"value\"], s=30, label=\"Peaks\")\n",
        "    plt.scatter(res[\"valleys\"][\"time\"], res[\"valleys\"][\"value\"], s=30, label=\"Valleys\")\n",
        "\n",
        "    plt.title(f\"Peaks & Valleys - {group_name}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# return information\n",
        "#print(results)\n",
        "print(results['CDKL5_005_273_44'][\"peaks\"].head())\n",
        "print(results['CDKL5_005_273_44'][\"valleys\"].head())\n",
        "plot_group_peaks('CDKL5_005_273_44')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1QUqhMMpjiGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute metrics for Animals\n",
        "\n",
        "def compute_metrics(results):\n",
        "    metrics = {}\n",
        "\n",
        "    for group, res in results.items():\n",
        "\n",
        "        peaks = res[\"peaks\"]\n",
        "        valleys = res[\"valleys\"]\n",
        "\n",
        "        # --- Peak intervals ---\n",
        "        if len(peaks) > 1:\n",
        "            peak_intervals = np.diff(peaks[\"time\"]).astype(\"timedelta64[m]\").astype(int)\n",
        "        else:\n",
        "            peak_intervals = []\n",
        "\n",
        "        # --- Valley intervals ---\n",
        "        if len(valleys) > 1:\n",
        "            valley_intervals = np.diff(valleys[\"time\"]).astype(\"timedelta64[m]\").astype(int)\n",
        "        else:\n",
        "            valley_intervals = []\n",
        "\n",
        "        # --- Peak to Valley differences ---\n",
        "        if len(peaks) > 0 and len(valleys) > 0:\n",
        "            # align shortest list for pairwise difference\n",
        "            n = min(len(peaks), len(valleys))\n",
        "            pv_diff = peaks[\"value\"].iloc[:n] - valleys[\"value\"].iloc[:n]\n",
        "        else:\n",
        "            pv_diff = []\n",
        "\n",
        "        metrics[group] = {\n",
        "            # Counts\n",
        "            \"n_peaks\": res[\"n_peaks\"],\n",
        "            \"n_valleys\": res[\"n_valleys\"],\n",
        "\n",
        "            # Peak metrics\n",
        "            \"peak_mean_height\": peaks[\"value\"].mean() if len(peaks) else np.nan,\n",
        "            \"peak_max_height\": peaks[\"value\"].max() if len(peaks) else np.nan,\n",
        "            \"peak_variability\": peaks[\"value\"].std() if len(peaks) else np.nan,\n",
        "            \"peak_mean_interval_minutes\": np.mean(peak_intervals) if len(peak_intervals) else np.nan,\n",
        "            \"peak_irregularity_index\": np.std(peak_intervals) / np.mean(peak_intervals)\n",
        "                                      if len(peak_intervals) > 1 else np.nan,\n",
        "\n",
        "            # Valley metrics\n",
        "            \"valley_mean_depth\": valleys[\"value\"].mean() if len(valleys) else np.nan,\n",
        "            \"valley_min_depth\": valleys[\"value\"].min() if len(valleys) else np.nan,\n",
        "            \"valley_mean_interval_minutes\": np.mean(valley_intervals) if len(valley_intervals) else np.nan,\n",
        "\n",
        "            # Amplitude metrics\n",
        "            \"mean_peak_to_valley_difference\": pv_diff.mean() if len(pv_diff) else np.nan,\n",
        "            \"max_peak_to_valley_difference\": pv_diff.max() if len(pv_diff) else np.nan,\n",
        "        }\n",
        "\n",
        "    return pd.DataFrame(metrics).T\n",
        "\n",
        "metrics_df = compute_metrics(results)\n",
        "metrics_df\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zoLR4dQl6vup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define function for night/day metrics analysis\n",
        "\n",
        "def day_night_metrics(group_averages, peakval_results,\n",
        "                      light_start = 6, dark_start = 18,\n",
        "                      activity_col=\"activity_count\"):\n",
        "    out = {}\n",
        "\n",
        "    for animal, df in group_averages.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # Ensure datetime index\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "\n",
        "        # Separate day/night (default day = 06:00–18:00)\n",
        "        day_mask = (df.index.hour >= light_start) & (df.index.hour < dark_start)\n",
        "        night_mask = ~day_mask\n",
        "\n",
        "        day_vals = df.loc[day_mask, activity_col]\n",
        "        night_vals = df.loc[night_mask, activity_col]\n",
        "\n",
        "\n",
        "        # ----- Peak/Valley summaries -----\n",
        "        peaks = peakval_results[animal][\"peaks\"].copy()\n",
        "        valleys = peakval_results[animal][\"valleys\"].copy()\n",
        "\n",
        "        # day/night peaks\n",
        "        peaks[\"time\"] = pd.to_datetime(peaks[\"time\"])\n",
        "        valleys[\"time\"] = pd.to_datetime(valleys[\"time\"])\n",
        "\n",
        "        day_peaks = peaks[peaks[\"time\"].dt.hour.between(6, 17)]\n",
        "        night_peaks = peaks[~peaks[\"time\"].dt.hour.between(6, 17)]\n",
        "\n",
        "        day_valleys = valleys[valleys[\"time\"].dt.hour.between(6, 17)]\n",
        "        night_valleys = valleys[~valleys[\"time\"].dt.hour.between(6, 17)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # --- Day & Night Peak intervals ---\n",
        "        # Day\n",
        "        if len(day_peaks) > 1:\n",
        "            day_peak_intervals = np.diff(peaks[\"time\"]).astype(\"timedelta64[m]\").astype(int)\n",
        "        else:\n",
        "            day_peak_intervals = []\n",
        "\n",
        "        # Night\n",
        "        if len(night_peaks) > 1:\n",
        "            night_peak_intervals = np.diff(peaks[\"time\"]).astype(\"timedelta64[m]\").astype(int)\n",
        "        else:\n",
        "            night_peak_intervals = []\n",
        "\n",
        "\n",
        "\n",
        "        # --- Day & Night Valley intervals ---\n",
        "        if len(day_valleys) > 1:\n",
        "            day_valley_intervals = np.diff(valleys[\"time\"]).astype(\"timedelta64[m]\").astype(int)\n",
        "        else:\n",
        "            day_valley_intervals = []\n",
        "\n",
        "        if len(night_valleys) > 1:\n",
        "            night_valley_intervals = np.diff(valleys[\"time\"]).astype(\"timedelta64[m]\").astype(int)\n",
        "        else:\n",
        "            night_valley_intervals = []\n",
        "\n",
        "\n",
        "\n",
        "        # ----- Metrics -----\n",
        "        out[animal] = {\n",
        "            # Activity\n",
        "            \"day_mean_activity\": day_vals.mean(),\n",
        "            \"night_mean_activity\": night_vals.mean(),\n",
        "            \"day_night_ratio\": (\n",
        "                day_vals.mean() / night_vals.mean()\n",
        "                if night_vals.mean() != 0 else np.nan\n",
        "            ),\n",
        "\n",
        "            \"day_peak_variability\": day_peaks[\"value\"].std() if len(day_peaks[\"value\"]) else np.nan,\n",
        "            \"night_peak_variability\": night_peaks[\"value\"].std() if len(night_peaks[\"value\"]) else np.nan,\n",
        "            \"day_peak_mean_interval_minutes\": np.mean(day_peak_intervals) if len(day_peak_intervals) else np.nan,\n",
        "            \"night_peak_mean_interval_minutes\": np.mean(night_peak_intervals) if len(night_peak_intervals) else np.nan,\n",
        "            \"day_peak_irregularity_index\": np.std(day_peak_intervals) / np.mean(day_peak_intervals)\n",
        "                                           if len(day_peak_intervals) > 1 else np.nan,\n",
        "            \"night_peak_irregularity_index\": np.std(night_peak_intervals) / np.mean(night_peak_intervals)\n",
        "                                             if len(night_peak_intervals) > 1 else np.nan,\n",
        "            \"day_valley_mean_interval_minutes\": np.mean(day_valley_intervals) if len(day_valley_intervals) else np.nan,\n",
        "            \"night_valley_mean_interval_minutes\": np.mean(day_valley_intervals) if len(day_valley_intervals) else np.nan,\n",
        "\n",
        "            # Peak metrics\n",
        "            \"num_day_peaks\": len(day_peaks),\n",
        "            \"num_night_peaks\": len(night_peaks),\n",
        "            \"max_peak_value\": peaks[\"value\"].max(),\n",
        "            \"time_of_max_peak\": peaks.loc[peaks[\"value\"].idxmax(), \"time\"],\n",
        "\n",
        "            # Valley metrics\n",
        "            \"num_day_valleys\": len(day_valleys),\n",
        "            \"num_night_valleys\": len(night_valleys),\n",
        "            \"min_valley_value\": valleys[\"value\"].min(),\n",
        "            \"time_of_min_valley\": valleys.loc[valleys[\"value\"].idxmin(), \"time\"],\n",
        "        }\n",
        "\n",
        "    return pd.DataFrame(out).T\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "daynight_df = day_night_metrics(group_averages, results)\n",
        "daynight_df\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rcu9l6z26-5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Merge Everything\n",
        "\n",
        "combined = (\n",
        "    key_df\n",
        "    .join(daynight_df,on=\"Mouse_ID\", how=\"inner\")\n",
        "    .join(metrics_df,on=\"Mouse_ID\", how=\"inner\")\n",
        ")\n",
        "\n",
        "# set column names to lowercase\n",
        "combined.columns = combined.columns.str.lower()\n",
        "combined[\"genotype\"] = combined[\"genotype\"].str.upper()\n",
        "\n",
        "cols_to_drop = ['min_valley_value', 'valley_min_depth']\n",
        "combined = combined.drop(columns=cols_to_drop)\n",
        "\n",
        "combined = combined.infer_objects()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------- Save CSV ----------\n",
        "csv_name = f\"combined_beam_spectral.csv\"\n",
        "\n",
        "# prepare df to match format of other files\n",
        "down_df = combined.copy()\n",
        "\n",
        "\n",
        "# Define a dictionary for renaming\n",
        "rename_mapping = {'mouse_id': 'Mouse_ID',\n",
        "                  'sex': 'Sex$',\n",
        "                  'genotype': 'Genotype$',\n",
        "                  'gene': 'Gene',}\n",
        "\n",
        "# Rename the columns\n",
        "down_df.rename(columns=rename_mapping, inplace=True)\n",
        "down_df['session_type'] = 'beam_spectral'\n",
        "#down_df['Mouse_ID'] = down_df['Mouse_ID'].str.upper()\n",
        "#down_df.drop('animal_id', axis=1, inplace=True)\n",
        "#down_df.drop('strain', axis=1, inplace=True)\n",
        "\n",
        "print(down_df)\n",
        "\n",
        "down_df = down_df.to_csv(csv_name, index=False)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "# Optional download button (works in Colab)\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(csv_name)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(csv_name)}</code>…\"\n",
        "        gfiles.download(csv_name)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{csv_name}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n",
        "\n",
        "\"\"\"\n",
        "write = False\n",
        "if write == True:\n",
        "  # Prepare and format\n",
        "  output_file = \"combined_beam.csv\"\n",
        "  combined.to_csv(output_file, index=False)\n",
        "  files.download(output_file)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IEI-4tvJ83IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Visualize boxplots for all metrics\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Columns\n",
        "numeric_cols = combined.select_dtypes(include='number').columns.tolist()\n",
        "print(numeric_cols)\n",
        "numeric_cols.remove(\"valley_min_depth\")\n",
        "group_by_col = \"genotype\"  # categorical variable to compare\n",
        "facet_col = \"strain\"       # another categorical\n",
        "\n",
        "# Order\n",
        "genotype_order = [\"WT\", \"HET\", \"HOM\"]\n",
        "\n",
        "strains = combined[facet_col].unique()\n",
        "n_strains = len(strains)\n",
        "n_numeric = len(numeric_cols)\n",
        "\n",
        "# Create subplots: rows = numeric_cols, cols = strains\n",
        "fig, axes = plt.subplots(n_numeric, n_strains,\n",
        "                         figsize=(4*n_strains, 4*n_numeric), squeeze=False)\n",
        "\n",
        "for i, num_col in enumerate(numeric_cols):\n",
        "    for j, strn in enumerate(strains):\n",
        "        ax = axes[i, j]\n",
        "        sns.boxplot(\n",
        "            data = combined[combined[facet_col] == strn],\n",
        "            x = group_by_col,\n",
        "            y = num_col,\n",
        "            ax=ax,\n",
        "            order=genotype_order\n",
        "        )\n",
        "        ax.set_title(f\"{num_col} by {group_by_col}\\n{facet_col} = {strn}\")\n",
        "        ax.set_xlabel(group_by_col)\n",
        "        ax.set_ylabel(num_col)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5ubv5H2RUfJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Conduct PCA analysis for metrics based on spectral\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# Step 1: Prepare PCA data\n",
        "# -------------------------\n",
        "\n",
        "key_cols = key_df.columns.str.lower()  # Columns to drop\n",
        "pca_df = combined.drop(columns=key_cols)\n",
        "\n",
        "# Drop object, bool, datetime columns\n",
        "drops = pca_df.select_dtypes(include=['object', 'bool', 'datetime64']).columns\n",
        "pca_df = pca_df.drop(columns=drops)\n",
        "\n",
        "# Ensure index matches combined\n",
        "pca_df.index = combined.index\n",
        "\n",
        "# Select numeric features\n",
        "features = pca_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "x = StandardScaler().fit_transform(pca_df[features])\n",
        "\n",
        "# PCA transformation\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(x)\n",
        "\n",
        "# Create PCA DataFrame with aligned index\n",
        "clustering_df_pca = pd.DataFrame(\n",
        "    data=pca_result,\n",
        "    columns=['principal component 1', 'principal component 2'],\n",
        "    index=pca_df.index\n",
        ")\n",
        "\n",
        "print(\"Explained variance per component:\", pca.explained_variance_ratio_)\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Plot PCA\n",
        "# -------------------------\n",
        "\n",
        "categorical_columns = ['strain', 'genotype']\n",
        "\n",
        "for cat_col in categorical_columns:\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.title(f\"PCA colored by {cat_col}\", fontsize=16)\n",
        "    plt.xlabel(\"Principal Component 1\", fontsize=14)\n",
        "    plt.ylabel(\"Principal Component 2\", fontsize=14)\n",
        "\n",
        "    # Drop NaNs for plotting\n",
        "    targets = combined[cat_col].dropna().unique()\n",
        "    cmap = plt.cm.get_cmap('viridis', len(targets))\n",
        "    colors = [cmap(i) for i in range(len(targets))]\n",
        "\n",
        "    for i, target in enumerate(targets):\n",
        "        # Boolean mask aligned with PCA DataFrame\n",
        "        mask = (combined[cat_col] == target).reindex(clustering_df_pca.index, fill_value=False)\n",
        "\n",
        "        if mask.sum() == 0:\n",
        "            print(f\"Warning: No rows found for {cat_col}={target}\")\n",
        "            continue\n",
        "\n",
        "        plt.scatter(\n",
        "            clustering_df_pca.loc[mask, 'principal component 1'],\n",
        "            clustering_df_pca.loc[mask, 'principal component 2'],\n",
        "            color=colors[i],\n",
        "            s=50,\n",
        "            alpha=0.7,\n",
        "            edgecolors='k',\n",
        "            label=str(target)\n",
        "        )\n",
        "\n",
        "    plt.legend(title=cat_col, fontsize=9, title_fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "muZHtwyMprXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # rip a meta key from files (OLD DO NOT USE)\n",
        "\n",
        "\n",
        "### Rip Key From Existing Data ###\n",
        "# instantiate empty meta\n",
        "meta = {}   # final output: {animal_id: {meta_key: meta_value}}\n",
        "\n",
        "for df in dataframes:\n",
        "    # select only object/string columns\n",
        "    obj_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "    df['suffix'] = df['source_file'].str.extract(r'(_\\d{3}_\\d{1,}_\\d{1,})')\n",
        "\n",
        "    df[\"Gene\"] = df['source_file'].str.extract(r'(^[A-Za-z0-9]+)')\n",
        "\n",
        "    # Check if all values could be converted to numeric (i.e., no NaNs introduced)\n",
        "    numeric_col = pd.to_numeric(df['Gene'], errors='coerce')\n",
        "    is_entirely_numeric = numeric_col.notna().all()\n",
        "\n",
        "    # id is entirley numeric then use strain\n",
        "    if is_entirely_numeric:\n",
        "      #print(f\"Gene column is all numeric: defaulting to Strain for Gene\")\n",
        "      df[\"Gene\"] = df['Strain']\n",
        "\n",
        "    # Create Mouse_ID if it doesn't exist\n",
        "    if 'Mouse_ID' not in df.columns:\n",
        "        df['Mouse_ID'] = df['Gene'] + df['suffix']\n",
        "    if 'Animal_ID' not in df.columns:\n",
        "        df['Animal_ID'] = df['Mouse_ID'].str.extract(r'(\\d{1,}_\\d{1,}$)').astype('object')\n",
        "\n",
        "    # explicitly remove columns you want to skip (case-insensitive)\n",
        "    skip_cols = {\"datetime\", \"file_date\", \"source_file\", \"file_id\",\n",
        "                 \"library_version\", 'suffix'}\n",
        "    obj_cols = [c for c in obj_cols if c.lower() not in skip_cols]\n",
        "\n",
        "    # extract metadata\n",
        "    info = {}\n",
        "    for col in obj_cols:\n",
        "        vals = df[col].dropna().unique()\n",
        "        if len(vals) == 1:\n",
        "            info[col] = vals[0]\n",
        "        else:\n",
        "            info[col] = vals.tolist()\n",
        "\n",
        "    # find animal_id column (case-insensitive: 'animal_id' or 'mouse_id')\n",
        "    \"\"\"animal_id_col = next((c for c in obj_cols if c.lower() in [\"animal_id\", \"mouse_id\"]), None)\n",
        "    if animal_id_col is None:\n",
        "        raise ValueError(f\"No animal ID column found in metadata. Available columns: {obj_cols}\")\"\"\"\n",
        "    # Priority order: Animal_ID > Mouse_ID\n",
        "    candidates = {c.lower(): c for c in df.columns}\n",
        "\n",
        "    if \"animal_id\" in candidates:\n",
        "        animal_id_col = candidates[\"animal_id\"]\n",
        "    elif \"mouse_id\" in candidates:\n",
        "        animal_id_col = candidates[\"mouse_id\"]\n",
        "    else:\n",
        "        # fallback to searching object columns\n",
        "        animal_id_col = next((c for c in obj_cols if c.lower() in [\"animal_id\", \"mouse_id\"]), None)\n",
        "\n",
        "    if animal_id_col is None:\n",
        "        raise ValueError(\n",
        "            f\"No animal ID column found. Object cols={obj_cols}, All cols={df.columns.tolist()}\"\n",
        "        )\n",
        "\n",
        "    # Retrieve ID directly from the dataframe\n",
        "    id_vals = df[animal_id_col].dropna().unique()\n",
        "    if len(id_vals) == 0:\n",
        "        raise ValueError(f\"No valid values found in animal_id column '{animal_id_col}'.\")\n",
        "    animal_id = id_vals[0]\n",
        "\n",
        "\n",
        "    meta[animal_id] = info\n",
        "\n",
        "key_df = pd.DataFrame.from_dict(meta, orient='index')\n",
        "\n",
        "# Print\n",
        "print(key_df)\n",
        "\n",
        "### Check key_df for missing values and require grouped key to fill in data ###\n",
        "cols = [\"Genotype\", \"Strain\", \"Sex\"]\n",
        "cols_exist = [c in key_df.columns for c in cols]\n",
        "if not all(cols_exist) or key_df[[c for c, exists in zip(cols, cols_exist) if exists]].isnull().values.any():\n",
        "    print(f\"At least one of these columns has a null or is missing. Please upload a key to resolve\")\n",
        "    # use the uploaded group key to fill in the blanks\n",
        "    ### Import of grouped Key ###\n",
        "    # upload the key file to ensure full coverage\n",
        "    grp_key = files.upload()\n",
        "    grp_key = pd.read_excel(list(grp_key.keys())[0])\n",
        "\n",
        "    # Remove $ from columns names\n",
        "    grp_key.columns = grp_key.columns.str.replace('$', '')\n",
        "\n",
        "    keeps = [\"Genotype\", \"Gene\", \"Sex\", \"Mouse_ID\"]\n",
        "    grp_key = grp_key[keeps]\n",
        "\n",
        "    # Ensure index is aligned\n",
        "    id_col = \"Mouse_ID\"\n",
        "    key_df = key_df.set_index(id_col)\n",
        "    grp_key = grp_key.set_index(id_col)\n",
        "\n",
        "    # subset for only mice we have files for\n",
        "    grp_key = grp_key.loc[grp_key.index.intersection(key_df.index)]\n",
        "\n",
        "    # Only update missing columns and missing values\n",
        "    key_df = key_df.fillna(grp_key)\n",
        "\n",
        "\n",
        "    #key_df = grp_key.combine_first(key_df)\n",
        "    key_df = key_df.reset_index()\n",
        "\n",
        "\n",
        "    print(key_df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### create the download for a metafile ###\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "csv_name2 = f\"spectral_key.csv\"\n",
        "down_df2 = key_df.to_csv(csv_name2, index=False)\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(csv_name2)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(csv_name2)}</code>…\"\n",
        "        gfiles.download(csv_name2)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{csv_name2}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oCrRxAbEbYMR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "f2de9fec-1dc7-47f7-d5d9-b063f774a53e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Strain'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Strain'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4269846249.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_entirely_numeric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0;31m#print(f\"Gene column is all numeric: defaulting to Strain for Gene\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Gene\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Strain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Create Mouse_ID if it doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Strain'"
          ]
        }
      ]
    }
  ]
}